2019-12-03 14:35:00.270707:   [INFO  ]   -----------------------------------------------------------------------------
2019-12-03 14:35:00.270852:   [INFO  ]   Hello world!
2019-12-03 14:35:00.271117:   [INFO  ]   Start main processing!
2019-12-03 14:35:00.271314:   [START ]   Start Prepare Traning data
2019-12-03 14:35:00.271413:   [START ]   Initialize parameters Start
2019-12-03 14:35:00.271634:   [DONE  ]   Initialize parameters Done
2019-12-03 14:35:00.271729:   [START ]   Start setup FOLD
2019-12-03 14:35:00.271960:   [START ]   Setup FOLD Done
2019-12-03 14:35:00.272030:   [START ]   Start Training data
2019-12-03 14:35:00.272203:   [START ]   Start Reading KET
2019-12-03 14:35:00.273009:   [START ]   Preprocess ./dataset/cambridge/KET/1.txt Start
2019-12-03 14:35:01.798919:   [DONE  ]   Preprocess ./dataset/cambridge/KET/1.txt Done
2019-12-03 14:35:01.800302:   [START ]   Preprocess ./dataset/cambridge/KET/2.txt Start
2019-12-03 14:35:01.818846:   [DONE  ]   Preprocess ./dataset/cambridge/KET/2.txt Done
2019-12-03 14:35:01.820094:   [START ]   Preprocess ./dataset/cambridge/KET/4.txt Start
2019-12-03 14:35:01.850001:   [DONE  ]   Preprocess ./dataset/cambridge/KET/4.txt Done
2019-12-03 14:35:01.850722:   [START ]   Preprocess ./dataset/cambridge/KET/5.txt Start
2019-12-03 14:35:01.867819:   [DONE  ]   Preprocess ./dataset/cambridge/KET/5.txt Done
2019-12-03 14:35:01.868859:   [START ]   Preprocess ./dataset/cambridge/KET/6.txt Start
2019-12-03 14:35:01.883012:   [DONE  ]   Preprocess ./dataset/cambridge/KET/6.txt Done
2019-12-03 14:35:01.883915:   [START ]   Preprocess ./dataset/cambridge/KET/7.txt Start
2019-12-03 14:35:01.908396:   [DONE  ]   Preprocess ./dataset/cambridge/KET/7.txt Done
2019-12-03 14:35:01.909018:   [START ]   Preprocess ./dataset/cambridge/KET/8.txt Start
2019-12-03 14:35:01.924972:   [DONE  ]   Preprocess ./dataset/cambridge/KET/8.txt Done
2019-12-03 14:35:01.925542:   [START ]   Preprocess ./dataset/cambridge/KET/9.txt Start
2019-12-03 14:35:01.938032:   [DONE  ]   Preprocess ./dataset/cambridge/KET/9.txt Done
2019-12-03 14:35:01.938822:   [START ]   Preprocess ./dataset/cambridge/KET/10.txt Start
2019-12-03 14:35:01.963847:   [DONE  ]   Preprocess ./dataset/cambridge/KET/10.txt Done
2019-12-03 14:35:01.964508:   [START ]   Preprocess ./dataset/cambridge/KET/11.txt Start
2019-12-03 14:35:01.980055:   [DONE  ]   Preprocess ./dataset/cambridge/KET/11.txt Done
2019-12-03 14:35:01.980403:   [START ]   Preprocess ./dataset/cambridge/KET/12.txt Start
2019-12-03 14:35:01.996157:   [DONE  ]   Preprocess ./dataset/cambridge/KET/12.txt Done
2019-12-03 14:35:01.996533:   [START ]   Preprocess ./dataset/cambridge/KET/14.txt Start
2019-12-03 14:35:02.012020:   [DONE  ]   Preprocess ./dataset/cambridge/KET/14.txt Done
2019-12-03 14:35:02.012159:   [DONE  ]   Read KET Done
2019-12-03 14:35:02.012424:   [START ]   Limiting KET Start
2019-12-03 14:35:02.012543:   [RESULT]   type_token_ratio: 0.4
2019-12-03 14:35:02.012633:   [RESULT]   lexical_density: 0.4
2019-12-03 14:35:51.291287:   [INFO  ]   -----------------------------------------------------------------------------
2019-12-03 14:35:51.291468:   [INFO  ]   Hello world!
2019-12-03 14:35:51.292213:   [INFO  ]   Start main processing!
2019-12-03 14:35:51.292388:   [START ]   Start Prepare Traning data
2019-12-03 14:35:51.292612:   [START ]   Initialize parameters Start
2019-12-03 14:35:51.292746:   [DONE  ]   Initialize parameters Done
2019-12-03 14:35:51.292936:   [START ]   Start setup FOLD
2019-12-03 14:35:51.293083:   [START ]   Setup FOLD Done
2019-12-03 14:35:51.293168:   [START ]   Start Training data
2019-12-03 14:35:51.293249:   [START ]   Start Reading KET
2019-12-03 14:35:51.293466:   [START ]   Preprocess ./dataset/cambridge/KET/1.txt Start
2019-12-03 14:35:52.660019:   [DONE  ]   Preprocess ./dataset/cambridge/KET/1.txt Done
2019-12-03 14:35:52.660287:   [START ]   Preprocess ./dataset/cambridge/KET/2.txt Start
2019-12-03 14:35:52.679214:   [DONE  ]   Preprocess ./dataset/cambridge/KET/2.txt Done
2019-12-03 14:35:52.679451:   [START ]   Preprocess ./dataset/cambridge/KET/4.txt Start
2019-12-03 14:35:52.704967:   [DONE  ]   Preprocess ./dataset/cambridge/KET/4.txt Done
2019-12-03 14:35:52.705179:   [START ]   Preprocess ./dataset/cambridge/KET/5.txt Start
2019-12-03 14:35:52.720005:   [DONE  ]   Preprocess ./dataset/cambridge/KET/5.txt Done
2019-12-03 14:35:52.720209:   [START ]   Preprocess ./dataset/cambridge/KET/6.txt Start
2019-12-03 14:35:52.733285:   [DONE  ]   Preprocess ./dataset/cambridge/KET/6.txt Done
2019-12-03 14:35:52.733514:   [START ]   Preprocess ./dataset/cambridge/KET/9.txt Start
2019-12-03 14:35:52.746036:   [DONE  ]   Preprocess ./dataset/cambridge/KET/9.txt Done
2019-12-03 14:35:52.746184:   [START ]   Preprocess ./dataset/cambridge/KET/10.txt Start
2019-12-03 14:35:52.769272:   [DONE  ]   Preprocess ./dataset/cambridge/KET/10.txt Done
2019-12-03 14:35:52.769433:   [START ]   Preprocess ./dataset/cambridge/KET/11.txt Start
2019-12-03 14:35:52.782925:   [DONE  ]   Preprocess ./dataset/cambridge/KET/11.txt Done
2019-12-03 14:35:52.783069:   [START ]   Preprocess ./dataset/cambridge/KET/12.txt Start
2019-12-03 14:35:52.795434:   [DONE  ]   Preprocess ./dataset/cambridge/KET/12.txt Done
2019-12-03 14:35:52.795763:   [START ]   Preprocess ./dataset/cambridge/KET/13.txt Start
2019-12-03 14:35:52.820860:   [DONE  ]   Preprocess ./dataset/cambridge/KET/13.txt Done
2019-12-03 14:35:52.821141:   [START ]   Preprocess ./dataset/cambridge/KET/14.txt Start
2019-12-03 14:35:52.834826:   [DONE  ]   Preprocess ./dataset/cambridge/KET/14.txt Done
2019-12-03 14:35:52.835117:   [START ]   Preprocess ./dataset/cambridge/KET/15.txt Start
2019-12-03 14:35:52.846337:   [DONE  ]   Preprocess ./dataset/cambridge/KET/15.txt Done
2019-12-03 14:35:52.846421:   [DONE  ]   Read KET Done
2019-12-03 14:35:52.846638:   [START ]   Limiting KET Start
2019-12-03 14:35:52.846746:   [RESULT]   type_token_ratio: 0.4
2019-12-03 14:35:52.846827:   [RESULT]   lexical_density: 0.4
2019-12-03 14:35:52.846950:   [RESULT]   char_per_word: 5.355263157894737
2019-12-03 14:36:21.547992:   [INFO  ]   -----------------------------------------------------------------------------
2019-12-03 14:36:21.548164:   [INFO  ]   Hello world!
2019-12-03 14:36:21.549004:   [INFO  ]   Start main processing!
2019-12-03 14:36:21.549262:   [START ]   Start Prepare Traning data
2019-12-03 14:36:21.549470:   [START ]   Initialize parameters Start
2019-12-03 14:36:21.549671:   [DONE  ]   Initialize parameters Done
2019-12-03 14:36:21.549815:   [START ]   Start setup FOLD
2019-12-03 14:36:21.550146:   [START ]   Setup FOLD Done
2019-12-03 14:36:21.550306:   [START ]   Start Training data
2019-12-03 14:36:21.550439:   [START ]   Start Reading KET
2019-12-03 14:36:21.550708:   [START ]   Preprocess ./dataset/cambridge/KET/1.txt Start
2019-12-03 14:36:22.932874:   [DONE  ]   Preprocess ./dataset/cambridge/KET/1.txt Done
2019-12-03 14:36:22.933154:   [START ]   Preprocess ./dataset/cambridge/KET/2.txt Start
2019-12-03 14:36:22.951624:   [DONE  ]   Preprocess ./dataset/cambridge/KET/2.txt Done
2019-12-03 14:36:22.951829:   [START ]   Preprocess ./dataset/cambridge/KET/4.txt Start
2019-12-03 14:36:22.976069:   [DONE  ]   Preprocess ./dataset/cambridge/KET/4.txt Done
2019-12-03 14:36:22.976293:   [START ]   Preprocess ./dataset/cambridge/KET/5.txt Start
2019-12-03 14:36:22.990753:   [DONE  ]   Preprocess ./dataset/cambridge/KET/5.txt Done
2019-12-03 14:36:22.990901:   [START ]   Preprocess ./dataset/cambridge/KET/6.txt Start
2019-12-03 14:36:23.003716:   [DONE  ]   Preprocess ./dataset/cambridge/KET/6.txt Done
2019-12-03 14:36:23.003862:   [START ]   Preprocess ./dataset/cambridge/KET/7.txt Start
2019-12-03 14:36:23.026171:   [DONE  ]   Preprocess ./dataset/cambridge/KET/7.txt Done
2019-12-03 14:36:23.026316:   [START ]   Preprocess ./dataset/cambridge/KET/9.txt Start
2019-12-03 14:36:23.039390:   [DONE  ]   Preprocess ./dataset/cambridge/KET/9.txt Done
2019-12-03 14:36:23.039532:   [START ]   Preprocess ./dataset/cambridge/KET/10.txt Start
2019-12-03 14:36:23.062280:   [DONE  ]   Preprocess ./dataset/cambridge/KET/10.txt Done
2019-12-03 14:36:23.062514:   [START ]   Preprocess ./dataset/cambridge/KET/11.txt Start
2019-12-03 14:36:23.076770:   [DONE  ]   Preprocess ./dataset/cambridge/KET/11.txt Done
2019-12-03 14:36:23.076971:   [START ]   Preprocess ./dataset/cambridge/KET/13.txt Start
2019-12-03 14:36:23.100424:   [DONE  ]   Preprocess ./dataset/cambridge/KET/13.txt Done
2019-12-03 14:36:23.100628:   [START ]   Preprocess ./dataset/cambridge/KET/14.txt Start
2019-12-03 14:36:23.114132:   [DONE  ]   Preprocess ./dataset/cambridge/KET/14.txt Done
2019-12-03 14:36:23.114291:   [START ]   Preprocess ./dataset/cambridge/KET/15.txt Start
2019-12-03 14:36:23.126106:   [DONE  ]   Preprocess ./dataset/cambridge/KET/15.txt Done
2019-12-03 14:36:23.126202:   [DONE  ]   Read KET Done
2019-12-03 14:36:23.126499:   [START ]   Limiting KET Start
2019-12-03 14:36:23.126604:   [RESULT]   type_token_ratio: 0.4
2019-12-03 14:36:23.126777:   [RESULT]   lexical_density: 0.4
2019-12-03 14:36:23.126860:   [RESULT]   char_per_word: 5.355263157894737
2019-12-03 14:36:48.815259:   [INFO  ]   -----------------------------------------------------------------------------
2019-12-03 14:36:48.815430:   [INFO  ]   Hello world!
2019-12-03 14:36:48.816259:   [INFO  ]   Start main processing!
2019-12-03 14:36:48.816503:   [START ]   Start Prepare Traning data
2019-12-03 14:36:48.816696:   [START ]   Initialize parameters Start
2019-12-03 14:36:48.816973:   [DONE  ]   Initialize parameters Done
2019-12-03 14:36:48.817098:   [START ]   Start setup FOLD
2019-12-03 14:36:48.817331:   [START ]   Setup FOLD Done
2019-12-03 14:36:48.817430:   [START ]   Start Training data
2019-12-03 14:36:48.817629:   [START ]   Start Reading KET
2019-12-03 14:36:48.817874:   [START ]   Preprocess ./dataset/cambridge/KET/1.txt Start
2019-12-03 14:36:50.183420:   [DONE  ]   Preprocess ./dataset/cambridge/KET/1.txt Done
2019-12-03 14:36:50.183694:   [START ]   Preprocess ./dataset/cambridge/KET/2.txt Start
2019-12-03 14:36:50.202479:   [DONE  ]   Preprocess ./dataset/cambridge/KET/2.txt Done
2019-12-03 14:36:50.203729:   [START ]   Preprocess ./dataset/cambridge/KET/3.txt Start
2019-12-03 14:36:50.216698:   [DONE  ]   Preprocess ./dataset/cambridge/KET/3.txt Done
2019-12-03 14:36:50.216895:   [START ]   Preprocess ./dataset/cambridge/KET/4.txt Start
2019-12-03 14:36:50.239588:   [DONE  ]   Preprocess ./dataset/cambridge/KET/4.txt Done
2019-12-03 14:36:50.239783:   [START ]   Preprocess ./dataset/cambridge/KET/6.txt Start
2019-12-03 14:36:50.252445:   [DONE  ]   Preprocess ./dataset/cambridge/KET/6.txt Done
2019-12-03 14:36:50.252587:   [START ]   Preprocess ./dataset/cambridge/KET/8.txt Start
2019-12-03 14:36:50.267505:   [DONE  ]   Preprocess ./dataset/cambridge/KET/8.txt Done
2019-12-03 14:36:50.267648:   [START ]   Preprocess ./dataset/cambridge/KET/9.txt Start
2019-12-03 14:36:50.279898:   [DONE  ]   Preprocess ./dataset/cambridge/KET/9.txt Done
2019-12-03 14:36:50.280048:   [START ]   Preprocess ./dataset/cambridge/KET/10.txt Start
2019-12-03 14:36:50.303231:   [DONE  ]   Preprocess ./dataset/cambridge/KET/10.txt Done
2019-12-03 14:36:50.303408:   [START ]   Preprocess ./dataset/cambridge/KET/11.txt Start
2019-12-03 14:36:50.316771:   [DONE  ]   Preprocess ./dataset/cambridge/KET/11.txt Done
2019-12-03 14:36:50.316912:   [START ]   Preprocess ./dataset/cambridge/KET/12.txt Start
2019-12-03 14:36:50.329259:   [DONE  ]   Preprocess ./dataset/cambridge/KET/12.txt Done
2019-12-03 14:36:50.329402:   [START ]   Preprocess ./dataset/cambridge/KET/13.txt Start
2019-12-03 14:36:50.352331:   [DONE  ]   Preprocess ./dataset/cambridge/KET/13.txt Done
2019-12-03 14:36:50.352501:   [START ]   Preprocess ./dataset/cambridge/KET/14.txt Start
2019-12-03 14:36:50.365760:   [DONE  ]   Preprocess ./dataset/cambridge/KET/14.txt Done
2019-12-03 14:36:50.365848:   [DONE  ]   Read KET Done
2019-12-03 14:36:50.366119:   [START ]   Limiting KET Start
2019-12-03 14:36:50.366254:   [RESULT]   type_token_ratio: 0.4
2019-12-03 14:36:50.366358:   [RESULT]   lexical_density: 0.4
2019-12-03 14:36:50.366475:   [RESULT]   char_per_word: 5.355263157894737
2019-12-03 14:36:50.366665:   [RESULT]   Score on KET Limiting: 6.155263157894738
2019-12-03 14:36:50.366809:   [RESULT]   type_token_ratio: 0.35294117647058826
2019-12-03 14:36:50.366892:   [RESULT]   lexical_density: 0.35294117647058826
2019-12-03 14:36:50.367135:   [RESULT]   char_per_word: 4.880952380952381
2019-12-03 14:36:50.367244:   [RESULT]   Score on KET Limiting: 5.586834733893557
2019-12-03 14:36:50.367375:   [RESULT]   type_token_ratio: 0.5204081632653061
2019-12-03 14:36:50.367474:   [RESULT]   lexical_density: 0.5204081632653061
2019-12-03 14:36:50.367572:   [RESULT]   char_per_word: 4.882352941176471
2019-12-03 14:36:50.367723:   [RESULT]   Score on KET Limiting: 5.923169267707083
2019-12-03 14:36:50.367899:   [RESULT]   type_token_ratio: 0.3193717277486911
2019-12-03 14:36:50.367997:   [RESULT]   lexical_density: 0.3193717277486911
2019-12-03 14:36:50.368153:   [RESULT]   char_per_word: 5.262295081967213
2019-12-03 14:36:50.368246:   [RESULT]   Score on KET Limiting: 5.901038537464595
2019-12-03 14:36:50.368348:   [RESULT]   type_token_ratio: 0.38596491228070173
2019-12-03 14:36:50.368438:   [RESULT]   lexical_density: 0.38596491228070173
2019-12-03 14:36:50.368574:   [RESULT]   char_per_word: 4.931818181818182
2019-12-03 14:36:50.368664:   [RESULT]   Score on KET Limiting: 5.703748006379586
2019-12-03 14:36:50.368794:   [RESULT]   type_token_ratio: 0.4186046511627907
2019-12-03 14:36:50.368884:   [RESULT]   lexical_density: 0.4186046511627907
2019-12-03 14:36:50.368980:   [RESULT]   char_per_word: 4.796296296296297
2019-12-03 14:36:50.369071:   [RESULT]   Score on KET Limiting: 5.633505598621878
2019-12-03 14:36:50.369189:   [RESULT]   type_token_ratio: 0.4074074074074074
2019-12-03 14:36:50.369277:   [RESULT]   lexical_density: 0.4074074074074074
2019-12-03 14:36:50.369391:   [RESULT]   char_per_word: 5.2727272727272725
2019-12-03 14:36:50.369481:   [RESULT]   Score on KET Limiting: 6.087542087542087
2019-12-03 14:36:50.369608:   [RESULT]   type_token_ratio: 0.35175879396984927
2019-12-03 14:36:50.369699:   [RESULT]   lexical_density: 0.35175879396984927
2019-12-03 14:36:50.369815:   [RESULT]   char_per_word: 5.271428571428571
2019-12-03 14:36:50.369905:   [RESULT]   Score on KET Limiting: 5.97494615936827
2019-12-03 14:36:50.370022:   [RESULT]   type_token_ratio: 0.31932773109243695
2019-12-03 14:36:50.370112:   [RESULT]   lexical_density: 0.31932773109243695
2019-12-03 14:36:50.370225:   [RESULT]   char_per_word: 5.526315789473684
2019-12-03 14:36:50.370317:   [RESULT]   Score on KET Limiting: 6.1649712516585575
2019-12-03 14:36:50.370441:   [RESULT]   type_token_ratio: 0.3125
2019-12-03 14:36:50.370518:   [RESULT]   lexical_density: 0.3125
2019-12-03 14:36:50.370626:   [RESULT]   char_per_word: 4.685714285714286
2019-12-03 14:36:50.370703:   [RESULT]   Score on KET Limiting: 5.310714285714286
2019-12-03 14:36:50.370823:   [RESULT]   type_token_ratio: 0.2914572864321608
2019-12-03 14:36:50.370902:   [RESULT]   lexical_density: 0.2914572864321608
2019-12-03 14:36:50.371012:   [RESULT]   char_per_word: 5.155172413793103
2019-12-03 14:36:50.371091:   [RESULT]   Score on KET Limiting: 5.738086986657425
2019-12-03 14:36:50.371206:   [RESULT]   type_token_ratio: 0.41228070175438597
2019-12-03 14:36:50.371283:   [RESULT]   lexical_density: 0.41228070175438597
2019-12-03 14:36:50.371392:   [RESULT]   char_per_word: 4.957446808510638
2019-12-03 14:36:50.371471:   [RESULT]   Score on KET Limiting: 5.78200821201941
2019-12-03 14:36:50.371569:   [DONE  ]   Limiting KET Done
2019-12-03 14:36:50.371644:   [START ]   Start Reading PET
2019-12-03 14:36:50.372317:   [START ]   Preprocess ./dataset/cambridge/PET/1.txt Start
2019-12-03 14:36:50.411915:   [DONE  ]   Preprocess ./dataset/cambridge/PET/1.txt Done
2019-12-03 14:36:50.412977:   [START ]   Preprocess ./dataset/cambridge/PET/3.txt Start
2019-12-03 14:36:50.430359:   [DONE  ]   Preprocess ./dataset/cambridge/PET/3.txt Done
2019-12-03 14:36:50.431762:   [START ]   Preprocess ./dataset/cambridge/PET/4.txt Start
2019-12-03 14:36:50.476958:   [DONE  ]   Preprocess ./dataset/cambridge/PET/4.txt Done
2019-12-03 14:36:50.478658:   [START ]   Preprocess ./dataset/cambridge/PET/5.txt Start
2019-12-03 14:36:50.512176:   [DONE  ]   Preprocess ./dataset/cambridge/PET/5.txt Done
2019-12-03 14:36:50.512702:   [START ]   Preprocess ./dataset/cambridge/PET/6.txt Start
2019-12-03 14:36:50.529678:   [DONE  ]   Preprocess ./dataset/cambridge/PET/6.txt Done
2019-12-03 14:36:50.530606:   [START ]   Preprocess ./dataset/cambridge/PET/7.txt Start
2019-12-03 14:36:50.576028:   [DONE  ]   Preprocess ./dataset/cambridge/PET/7.txt Done
2019-12-03 14:36:50.576884:   [START ]   Preprocess ./dataset/cambridge/PET/8.txt Start
2019-12-03 14:36:50.603109:   [DONE  ]   Preprocess ./dataset/cambridge/PET/8.txt Done
2019-12-03 14:36:50.603675:   [START ]   Preprocess ./dataset/cambridge/PET/9.txt Start
2019-12-03 14:36:50.621760:   [DONE  ]   Preprocess ./dataset/cambridge/PET/9.txt Done
2019-12-03 14:36:50.622428:   [START ]   Preprocess ./dataset/cambridge/PET/11.txt Start
2019-12-03 14:36:50.652345:   [DONE  ]   Preprocess ./dataset/cambridge/PET/11.txt Done
2019-12-03 14:36:50.653227:   [START ]   Preprocess ./dataset/cambridge/PET/12.txt Start
2019-12-03 14:36:50.670277:   [DONE  ]   Preprocess ./dataset/cambridge/PET/12.txt Done
2019-12-03 14:36:50.670682:   [START ]   Preprocess ./dataset/cambridge/PET/13.txt Start
2019-12-03 14:36:50.711745:   [DONE  ]   Preprocess ./dataset/cambridge/PET/13.txt Done
2019-12-03 14:36:50.712754:   [START ]   Preprocess ./dataset/cambridge/PET/14.txt Start
2019-12-03 14:36:50.744269:   [DONE  ]   Preprocess ./dataset/cambridge/PET/14.txt Done
2019-12-03 14:36:50.744401:   [DONE  ]   Read PET Done
2019-12-03 14:36:50.744961:   [START ]   Limiting PET Start
2019-12-03 14:36:50.745252:   [RESULT]   type_token_ratio: 0.40878378378378377
2019-12-03 14:36:50.745473:   [RESULT]   lexical_density: 0.40878378378378377
2019-12-03 14:36:50.745707:   [RESULT]   char_per_word: 6.231404958677686
2019-12-03 14:36:50.745911:   [RESULT]   Score on PET Limiting: 7.0489725262452545
2019-12-03 14:36:50.746133:   [RESULT]   type_token_ratio: 0.423841059602649
2019-12-03 14:36:50.746327:   [RESULT]   lexical_density: 0.423841059602649
2019-12-03 14:36:50.746532:   [RESULT]   char_per_word: 6.375
2019-12-03 14:36:50.746798:   [RESULT]   Score on PET Limiting: 7.222682119205299
2019-12-03 14:36:50.747051:   [RESULT]   type_token_ratio: 0.34413965087281795
2019-12-03 14:36:50.747254:   [RESULT]   lexical_density: 0.34413965087281795
2019-12-03 14:36:50.747490:   [RESULT]   char_per_word: 5.826086956521739
2019-12-03 14:36:50.747696:   [RESULT]   Score on PET Limiting: 6.514366258267375
2019-12-03 14:36:50.747911:   [RESULT]   type_token_ratio: 0.39545454545454545
2019-12-03 14:36:50.748006:   [RESULT]   lexical_density: 0.39545454545454545
2019-12-03 14:36:50.748193:   [RESULT]   char_per_word: 5.310344827586207
2019-12-03 14:36:50.748313:   [RESULT]   Score on PET Limiting: 6.101253918495298
2019-12-03 14:36:50.748425:   [RESULT]   type_token_ratio: 0.46621621621621623
2019-12-03 14:36:50.748646:   [RESULT]   lexical_density: 0.46621621621621623
2019-12-03 14:36:50.748763:   [RESULT]   char_per_word: 5.6521739130434785
2019-12-03 14:36:50.748885:   [RESULT]   Score on PET Limiting: 6.58460634547591
2019-12-03 14:36:50.749018:   [RESULT]   type_token_ratio: 0.3906633906633907
2019-12-03 14:36:50.749179:   [RESULT]   lexical_density: 0.3906633906633907
2019-12-03 14:36:50.749311:   [RESULT]   char_per_word: 6.062893081761007
2019-12-03 14:36:50.749491:   [RESULT]   Score on PET Limiting: 6.844219863087788
2019-12-03 14:36:50.749620:   [RESULT]   type_token_ratio: 0.3177570093457944
2019-12-03 14:36:50.749720:   [RESULT]   lexical_density: 0.3177570093457944
2019-12-03 14:36:50.749882:   [RESULT]   char_per_word: 5.573529411764706
2019-12-03 14:36:50.749991:   [RESULT]   Score on PET Limiting: 6.209043430456295
2019-12-03 14:36:50.750103:   [RESULT]   type_token_ratio: 0.4090909090909091
2019-12-03 14:36:50.750197:   [RESULT]   lexical_density: 0.4090909090909091
2019-12-03 14:36:50.750372:   [RESULT]   char_per_word: 5.4603174603174605
2019-12-03 14:36:50.750477:   [RESULT]   Score on PET Limiting: 6.278499278499279
2019-12-03 14:36:50.750584:   [RESULT]   type_token_ratio: 0.4782608695652174
2019-12-03 14:36:50.750706:   [RESULT]   lexical_density: 0.4782608695652174
2019-12-03 14:36:50.750830:   [RESULT]   char_per_word: 6.113636363636363
2019-12-03 14:36:50.750923:   [RESULT]   Score on PET Limiting: 7.070158102766799
2019-12-03 14:36:50.751090:   [RESULT]   type_token_ratio: 0.39552238805970147
2019-12-03 14:36:50.751181:   [RESULT]   lexical_density: 0.39552238805970147
2019-12-03 14:36:50.751276:   [RESULT]   char_per_word: 5.90566037735849
2019-12-03 14:36:50.751440:   [RESULT]   Score on PET Limiting: 6.696705153477893
2019-12-03 14:36:50.751558:   [RESULT]   type_token_ratio: 0.3917808219178082
2019-12-03 14:36:50.751655:   [RESULT]   lexical_density: 0.3917808219178082
2019-12-03 14:36:50.751764:   [RESULT]   char_per_word: 5.916083916083916
2019-12-03 14:36:50.751857:   [RESULT]   Score on PET Limiting: 6.699645559919532
2019-12-03 14:36:50.751964:   [RESULT]   type_token_ratio: 0.39215686274509803
2019-12-03 14:36:50.752056:   [RESULT]   lexical_density: 0.39215686274509803
2019-12-03 14:36:50.752155:   [RESULT]   char_per_word: 5.9625
2019-12-03 14:36:50.752259:   [RESULT]   Score on PET Limiting: 6.746813725490197
2019-12-03 14:36:50.752342:   [DONE  ]   Limiting PET Done
2019-12-03 14:36:50.752454:   [START ]   Start Reading FCE
2019-12-03 14:36:50.753005:   [START ]   Preprocess ./dataset/cambridge/FCE/1.txt Start
2019-12-03 14:36:50.834624:   [DONE  ]   Preprocess ./dataset/cambridge/FCE/1.txt Done
2019-12-03 14:36:50.836131:   [START ]   Preprocess ./dataset/cambridge/FCE/2.txt Start
2019-12-03 14:36:50.900542:   [DONE  ]   Preprocess ./dataset/cambridge/FCE/2.txt Done
2019-12-03 14:36:50.902292:   [START ]   Preprocess ./dataset/cambridge/FCE/3.txt Start
2019-12-03 14:36:50.972097:   [DONE  ]   Preprocess ./dataset/cambridge/FCE/3.txt Done
2019-12-03 14:36:50.973738:   [START ]   Preprocess ./dataset/cambridge/FCE/4.txt Start
2019-12-03 14:36:51.053500:   [DONE  ]   Preprocess ./dataset/cambridge/FCE/4.txt Done
2019-12-03 14:36:51.055221:   [START ]   Preprocess ./dataset/cambridge/FCE/5.txt Start
2019-12-03 14:36:51.126913:   [DONE  ]   Preprocess ./dataset/cambridge/FCE/5.txt Done
2019-12-03 14:36:51.128521:   [START ]   Preprocess ./dataset/cambridge/FCE/6.txt Start
2019-12-03 14:36:51.212896:   [DONE  ]   Preprocess ./dataset/cambridge/FCE/6.txt Done
2019-12-03 14:36:51.214059:   [START ]   Preprocess ./dataset/cambridge/FCE/8.txt Start
2019-12-03 14:36:51.292441:   [DONE  ]   Preprocess ./dataset/cambridge/FCE/8.txt Done
2019-12-03 14:36:51.293541:   [START ]   Preprocess ./dataset/cambridge/FCE/10.txt Start
2019-12-03 14:36:51.366640:   [DONE  ]   Preprocess ./dataset/cambridge/FCE/10.txt Done
2019-12-03 14:36:51.367738:   [START ]   Preprocess ./dataset/cambridge/FCE/11.txt Start
2019-12-03 14:36:51.446252:   [DONE  ]   Preprocess ./dataset/cambridge/FCE/11.txt Done
2019-12-03 14:36:51.447251:   [START ]   Preprocess ./dataset/cambridge/FCE/12.txt Start
2019-12-03 14:36:51.523547:   [DONE  ]   Preprocess ./dataset/cambridge/FCE/12.txt Done
2019-12-03 14:36:51.524588:   [START ]   Preprocess ./dataset/cambridge/FCE/13.txt Start
2019-12-03 14:36:51.597700:   [DONE  ]   Preprocess ./dataset/cambridge/FCE/13.txt Done
2019-12-03 14:36:51.598362:   [START ]   Preprocess ./dataset/cambridge/FCE/14.txt Start
2019-12-03 14:36:51.670135:   [DONE  ]   Preprocess ./dataset/cambridge/FCE/14.txt Done
2019-12-03 14:36:51.670277:   [DONE  ]   Read FCE Done
2019-12-03 14:36:51.671393:   [START ]   Limiting FCE Start
2019-12-03 14:36:51.671763:   [RESULT]   type_token_ratio: 0.34206896551724136
2019-12-03 14:36:51.671912:   [RESULT]   lexical_density: 0.34206896551724136
2019-12-03 14:36:51.672076:   [RESULT]   char_per_word: 5.919354838709677
2019-12-03 14:36:51.672187:   [RESULT]   Score on FCE Limiting: 6.60349276974416
2019-12-03 14:36:51.672408:   [RESULT]   type_token_ratio: 0.3769911504424779
2019-12-03 14:36:51.672575:   [RESULT]   lexical_density: 0.3769911504424779
2019-12-03 14:36:51.672703:   [RESULT]   char_per_word: 5.962441314553991
2019-12-03 14:36:51.672807:   [RESULT]   Score on FCE Limiting: 6.716423615438947
2019-12-03 14:36:51.672954:   [RESULT]   type_token_ratio: 0.36482084690553745
2019-12-03 14:36:51.673059:   [RESULT]   lexical_density: 0.36482084690553745
2019-12-03 14:36:51.673185:   [RESULT]   char_per_word: 6.401785714285714
2019-12-03 14:36:51.673349:   [RESULT]   Score on FCE Limiting: 7.13142740809679
2019-12-03 14:36:51.673611:   [RESULT]   type_token_ratio: 0.3472429210134128
2019-12-03 14:36:51.673718:   [RESULT]   lexical_density: 0.3472429210134128
2019-12-03 14:36:51.673941:   [RESULT]   char_per_word: 5.896995708154506
2019-12-03 14:36:51.674043:   [RESULT]   Score on FCE Limiting: 6.5914815501813315
2019-12-03 14:36:51.674192:   [RESULT]   type_token_ratio: 0.3643533123028391
2019-12-03 14:36:51.674294:   [RESULT]   lexical_density: 0.3643533123028391
2019-12-03 14:36:51.674419:   [RESULT]   char_per_word: 6.008658008658009
2019-12-03 14:36:51.674517:   [RESULT]   Score on FCE Limiting: 6.737364633263687
2019-12-03 14:36:51.674688:   [RESULT]   type_token_ratio: 0.31420765027322406
2019-12-03 14:36:51.674787:   [RESULT]   lexical_density: 0.31420765027322406
2019-12-03 14:36:51.674910:   [RESULT]   char_per_word: 5.773913043478261
2019-12-03 14:36:51.675009:   [RESULT]   Score on FCE Limiting: 6.40232834402471
2019-12-03 14:36:51.675154:   [RESULT]   type_token_ratio: 0.3258064516129032
2019-12-03 14:36:51.675253:   [RESULT]   lexical_density: 0.3258064516129032
2019-12-03 14:36:51.675436:   [RESULT]   char_per_word: 6.01980198019802
2019-12-03 14:36:51.675551:   [RESULT]   Score on FCE Limiting: 6.671414883423827
2019-12-03 14:36:51.675727:   [RESULT]   type_token_ratio: 0.3584
2019-12-03 14:36:51.675832:   [RESULT]   lexical_density: 0.3584
2019-12-03 14:36:51.675964:   [RESULT]   char_per_word: 5.834821428571429
2019-12-03 14:36:51.676066:   [RESULT]   Score on FCE Limiting: 6.551621428571428
2019-12-03 14:36:51.676275:   [RESULT]   type_token_ratio: 0.3603174603174603
2019-12-03 14:36:51.676392:   [RESULT]   lexical_density: 0.3603174603174603
2019-12-03 14:36:51.676522:   [RESULT]   char_per_word: 5.872246696035242
2019-12-03 14:36:51.676625:   [RESULT]   Score on FCE Limiting: 6.592881616670162
2019-12-03 14:36:51.676789:   [RESULT]   type_token_ratio: 0.363905325443787
2019-12-03 14:36:51.676894:   [RESULT]   lexical_density: 0.363905325443787
2019-12-03 14:36:51.677027:   [RESULT]   char_per_word: 6.2317073170731705
2019-12-03 14:36:51.677128:   [RESULT]   Score on FCE Limiting: 6.959517967960745
2019-12-03 14:36:51.677278:   [RESULT]   type_token_ratio: 0.36316695352839934
2019-12-03 14:36:51.677380:   [RESULT]   lexical_density: 0.36316695352839934
2019-12-03 14:36:51.677514:   [RESULT]   char_per_word: 6.312796208530806
2019-12-03 14:36:51.677612:   [RESULT]   Score on FCE Limiting: 7.039130115587604
2019-12-03 14:36:51.677760:   [RESULT]   type_token_ratio: 0.32685512367491165
2019-12-03 14:36:51.677858:   [RESULT]   lexical_density: 0.32685512367491165
2019-12-03 14:36:51.677987:   [RESULT]   char_per_word: 6.005405405405406
2019-12-03 14:36:51.678085:   [RESULT]   Score on FCE Limiting: 6.659115652755228
2019-12-03 14:36:51.678196:   [DONE  ]   Limiting FCE Done
2019-12-03 14:36:51.678272:   [START ]   Start Reading CAE
2019-12-03 14:36:51.679145:   [START ]   Preprocess ./dataset/cambridge/CAE/4.txt Start
2019-12-03 14:36:51.811475:   [DONE  ]   Preprocess ./dataset/cambridge/CAE/4.txt Done
2019-12-03 14:36:51.812997:   [START ]   Preprocess ./dataset/cambridge/CAE/5.txt Start
2019-12-03 14:36:51.896386:   [DONE  ]   Preprocess ./dataset/cambridge/CAE/5.txt Done
2019-12-03 14:36:51.897917:   [START ]   Preprocess ./dataset/cambridge/CAE/6.txt Start
2019-12-03 14:36:51.986888:   [DONE  ]   Preprocess ./dataset/cambridge/CAE/6.txt Done
2019-12-03 14:36:51.987852:   [START ]   Preprocess ./dataset/cambridge/CAE/7.txt Start
2019-12-03 14:36:52.083649:   [DONE  ]   Preprocess ./dataset/cambridge/CAE/7.txt Done
2019-12-03 14:36:52.084518:   [START ]   Preprocess ./dataset/cambridge/CAE/8.txt Start
2019-12-03 14:36:52.188510:   [DONE  ]   Preprocess ./dataset/cambridge/CAE/8.txt Done
2019-12-03 14:37:17.452213:   [INFO  ]   -----------------------------------------------------------------------------
2019-12-03 14:37:17.452385:   [INFO  ]   Hello world!
2019-12-03 14:37:17.453093:   [INFO  ]   Start main processing!
2019-12-03 14:37:17.453329:   [START ]   Start Prepare Traning data
2019-12-03 14:37:17.453532:   [START ]   Initialize parameters Start
2019-12-03 14:37:17.453781:   [DONE  ]   Initialize parameters Done
2019-12-03 14:37:17.453905:   [START ]   Start setup FOLD
2019-12-03 14:37:17.454066:   [START ]   Setup FOLD Done
2019-12-03 14:37:17.454143:   [START ]   Start Training data
2019-12-03 14:37:17.454383:   [START ]   Start Reading KET
2019-12-03 14:37:17.454538:   [START ]   Preprocess ./dataset/cambridge/KET/1.txt Start
2019-12-03 14:37:18.851743:   [DONE  ]   Preprocess ./dataset/cambridge/KET/1.txt Done
2019-12-03 14:37:18.852006:   [START ]   Preprocess ./dataset/cambridge/KET/2.txt Start
2019-12-03 14:37:18.875694:   [DONE  ]   Preprocess ./dataset/cambridge/KET/2.txt Done
2019-12-03 14:37:18.875979:   [START ]   Preprocess ./dataset/cambridge/KET/3.txt Start
2019-12-03 14:37:18.895349:   [DONE  ]   Preprocess ./dataset/cambridge/KET/3.txt Done
2019-12-03 14:37:18.895609:   [START ]   Preprocess ./dataset/cambridge/KET/4.txt Start
2019-12-03 14:37:18.922965:   [DONE  ]   Preprocess ./dataset/cambridge/KET/4.txt Done
2019-12-03 14:37:18.923360:   [START ]   Preprocess ./dataset/cambridge/KET/5.txt Start
2019-12-03 14:37:18.939275:   [DONE  ]   Preprocess ./dataset/cambridge/KET/5.txt Done
2019-12-03 14:37:18.939489:   [START ]   Preprocess ./dataset/cambridge/KET/7.txt Start
2019-12-03 14:37:18.969467:   [DONE  ]   Preprocess ./dataset/cambridge/KET/7.txt Done
2019-12-03 14:37:18.969667:   [START ]   Preprocess ./dataset/cambridge/KET/8.txt Start
2019-12-03 14:37:18.988457:   [DONE  ]   Preprocess ./dataset/cambridge/KET/8.txt Done
2019-12-03 14:37:18.988671:   [START ]   Preprocess ./dataset/cambridge/KET/9.txt Start
2019-12-03 14:37:19.003086:   [DONE  ]   Preprocess ./dataset/cambridge/KET/9.txt Done
2019-12-03 14:37:19.003279:   [START ]   Preprocess ./dataset/cambridge/KET/10.txt Start
2019-12-03 14:37:19.028153:   [DONE  ]   Preprocess ./dataset/cambridge/KET/10.txt Done
2019-12-03 14:37:19.028352:   [START ]   Preprocess ./dataset/cambridge/KET/11.txt Start
2019-12-03 14:37:19.043370:   [DONE  ]   Preprocess ./dataset/cambridge/KET/11.txt Done
2019-12-03 14:37:19.043595:   [START ]   Preprocess ./dataset/cambridge/KET/13.txt Start
2019-12-03 14:37:19.068830:   [DONE  ]   Preprocess ./dataset/cambridge/KET/13.txt Done
2019-12-03 14:37:19.069063:   [START ]   Preprocess ./dataset/cambridge/KET/14.txt Start
2019-12-03 14:37:19.084186:   [DONE  ]   Preprocess ./dataset/cambridge/KET/14.txt Done
2019-12-03 14:37:19.084314:   [DONE  ]   Read KET Done
2019-12-03 14:37:19.084716:   [START ]   Limiting KET Start
2019-12-03 14:37:19.084984:   [RESULT]   type_token_ratio: 0.4
2019-12-03 14:37:19.085250:   [RESULT]   lexical_density: 0.4
2019-12-03 14:37:19.085468:   [RESULT]   char_per_word: 5.355263157894737
2019-12-03 14:37:19.085670:   [RESULT]   Score on KET Limiting: 6.155263157894738
2019-12-03 14:37:19.085891:   [RESULT]   type_token_ratio: 0.35294117647058826
2019-12-03 14:37:19.086093:   [RESULT]   lexical_density: 0.35294117647058826
2019-12-03 14:37:19.086208:   [RESULT]   char_per_word: 4.880952380952381
2019-12-03 14:37:19.086390:   [RESULT]   Score on KET Limiting: 5.586834733893557
2019-12-03 14:37:19.086594:   [RESULT]   type_token_ratio: 0.5204081632653061
2019-12-03 14:37:19.086875:   [RESULT]   lexical_density: 0.5204081632653061
2019-12-03 14:37:19.087021:   [RESULT]   char_per_word: 4.882352941176471
2019-12-03 14:37:19.087194:   [RESULT]   Score on KET Limiting: 5.923169267707083
2019-12-03 14:37:19.087322:   [RESULT]   type_token_ratio: 0.3193717277486911
2019-12-03 14:37:19.087570:   [RESULT]   lexical_density: 0.3193717277486911
2019-12-03 14:37:19.087687:   [RESULT]   char_per_word: 5.262295081967213
2019-12-03 14:37:19.087791:   [RESULT]   Score on KET Limiting: 5.901038537464595
2019-12-03 14:37:19.087952:   [RESULT]   type_token_ratio: 0.42016806722689076
2019-12-03 14:37:19.088047:   [RESULT]   lexical_density: 0.42016806722689076
2019-12-03 14:37:19.088170:   [RESULT]   char_per_word: 5.18
2019-12-03 14:37:19.088269:   [RESULT]   Score on KET Limiting: 6.020336134453782
2019-12-03 14:37:19.088397:   [RESULT]   type_token_ratio: 0.38860103626943004
2019-12-03 14:37:19.088492:   [RESULT]   lexical_density: 0.38860103626943004
2019-12-03 14:37:19.088670:   [RESULT]   char_per_word: 5.56
2019-12-03 14:37:19.088767:   [RESULT]   Score on KET Limiting: 6.337202072538859
2019-12-03 14:37:19.088936:   [RESULT]   type_token_ratio: 0.4186046511627907
2019-12-03 14:37:19.089030:   [RESULT]   lexical_density: 0.4186046511627907
2019-12-03 14:37:19.089176:   [RESULT]   char_per_word: 4.796296296296297
2019-12-03 14:37:19.089260:   [RESULT]   Score on KET Limiting: 5.633505598621878
2019-12-03 14:37:19.089361:   [RESULT]   type_token_ratio: 0.4074074074074074
2019-12-03 14:37:19.089441:   [RESULT]   lexical_density: 0.4074074074074074
2019-12-03 14:37:19.089565:   [RESULT]   char_per_word: 5.2727272727272725
2019-12-03 14:37:19.089645:   [RESULT]   Score on KET Limiting: 6.087542087542087
2019-12-03 14:37:19.089789:   [RESULT]   type_token_ratio: 0.35175879396984927
2019-12-03 14:37:19.089868:   [RESULT]   lexical_density: 0.35175879396984927
2019-12-03 14:37:19.090014:   [RESULT]   char_per_word: 5.271428571428571
2019-12-03 14:37:19.090095:   [RESULT]   Score on KET Limiting: 5.97494615936827
2019-12-03 14:37:19.090183:   [RESULT]   type_token_ratio: 0.31932773109243695
2019-12-03 14:37:19.090280:   [RESULT]   lexical_density: 0.31932773109243695
2019-12-03 14:37:19.090360:   [RESULT]   char_per_word: 5.526315789473684
2019-12-03 14:37:19.090462:   [RESULT]   Score on KET Limiting: 6.1649712516585575
2019-12-03 14:37:19.090629:   [RESULT]   type_token_ratio: 0.2914572864321608
2019-12-03 14:37:19.090709:   [RESULT]   lexical_density: 0.2914572864321608
2019-12-03 14:37:19.090817:   [RESULT]   char_per_word: 5.155172413793103
2019-12-03 14:37:19.090896:   [RESULT]   Score on KET Limiting: 5.738086986657425
2019-12-03 14:37:19.091009:   [RESULT]   type_token_ratio: 0.41228070175438597
2019-12-03 14:37:19.091087:   [RESULT]   lexical_density: 0.41228070175438597
2019-12-03 14:37:19.091196:   [RESULT]   char_per_word: 4.957446808510638
2019-12-03 14:37:19.091275:   [RESULT]   Score on KET Limiting: 5.78200821201941
2019-12-03 14:37:19.091375:   [DONE  ]   Limiting KET Done
2019-12-03 14:37:19.091450:   [START ]   Start Reading PET
2019-12-03 14:37:19.091602:   [START ]   Preprocess ./dataset/cambridge/PET/1.txt Start
2019-12-03 14:37:19.136385:   [DONE  ]   Preprocess ./dataset/cambridge/PET/1.txt Done
2019-12-03 14:37:19.136703:   [START ]   Preprocess ./dataset/cambridge/PET/2.txt Start
2019-12-03 14:37:19.163047:   [DONE  ]   Preprocess ./dataset/cambridge/PET/2.txt Done
2019-12-03 14:37:19.163272:   [START ]   Preprocess ./dataset/cambridge/PET/3.txt Start
2019-12-03 14:37:19.182144:   [DONE  ]   Preprocess ./dataset/cambridge/PET/3.txt Done
2019-12-03 14:37:19.182553:   [START ]   Preprocess ./dataset/cambridge/PET/4.txt Start
2019-12-03 14:37:19.233401:   [DONE  ]   Preprocess ./dataset/cambridge/PET/4.txt Done
2019-12-03 14:37:19.233610:   [START ]   Preprocess ./dataset/cambridge/PET/6.txt Start
2019-12-03 14:37:19.258102:   [DONE  ]   Preprocess ./dataset/cambridge/PET/6.txt Done
2019-12-03 14:37:19.258306:   [START ]   Preprocess ./dataset/cambridge/PET/8.txt Start
2019-12-03 14:37:19.286176:   [DONE  ]   Preprocess ./dataset/cambridge/PET/8.txt Done
2019-12-03 14:37:19.286378:   [START ]   Preprocess ./dataset/cambridge/PET/9.txt Start
2019-12-03 14:37:19.306109:   [DONE  ]   Preprocess ./dataset/cambridge/PET/9.txt Done
2019-12-03 14:37:19.306537:   [START ]   Preprocess ./dataset/cambridge/PET/10.txt Start
2019-12-03 14:37:19.363797:   [DONE  ]   Preprocess ./dataset/cambridge/PET/10.txt Done
2019-12-03 14:37:19.364023:   [START ]   Preprocess ./dataset/cambridge/PET/11.txt Start
2019-12-03 14:37:19.389754:   [DONE  ]   Preprocess ./dataset/cambridge/PET/11.txt Done
2019-12-03 14:37:19.390188:   [START ]   Preprocess ./dataset/cambridge/PET/12.txt Start
2019-12-03 14:37:19.407045:   [DONE  ]   Preprocess ./dataset/cambridge/PET/12.txt Done
2019-12-03 14:37:19.407286:   [START ]   Preprocess ./dataset/cambridge/PET/13.txt Start
2019-12-03 14:37:19.460893:   [DONE  ]   Preprocess ./dataset/cambridge/PET/13.txt Done
2019-12-03 14:37:19.461526:   [START ]   Preprocess ./dataset/cambridge/PET/15.txt Start
2019-12-03 14:37:19.485952:   [DONE  ]   Preprocess ./dataset/cambridge/PET/15.txt Done
2019-12-03 14:37:19.486086:   [DONE  ]   Read PET Done
2019-12-03 14:37:19.486336:   [START ]   Limiting PET Start
2019-12-03 14:37:19.486452:   [RESULT]   type_token_ratio: 0.40878378378378377
2019-12-03 14:37:19.486538:   [RESULT]   lexical_density: 0.40878378378378377
2019-12-03 14:37:19.486630:   [RESULT]   char_per_word: 6.231404958677686
2019-12-03 14:37:19.486785:   [RESULT]   Score on PET Limiting: 7.0489725262452545
2019-12-03 14:37:19.486876:   [RESULT]   type_token_ratio: 0.40096618357487923
2019-12-03 14:37:19.486953:   [RESULT]   lexical_density: 0.40096618357487923
2019-12-03 14:37:19.487122:   [RESULT]   char_per_word: 5.674698795180723
2019-12-03 14:37:19.487214:   [RESULT]   Score on PET Limiting: 6.476631162330482
2019-12-03 14:37:19.487415:   [RESULT]   type_token_ratio: 0.423841059602649
2019-12-03 14:37:19.487504:   [RESULT]   lexical_density: 0.423841059602649
2019-12-03 14:37:19.487664:   [RESULT]   char_per_word: 6.375
2019-12-03 14:37:19.487824:   [RESULT]   Score on PET Limiting: 7.222682119205299
2019-12-03 14:37:19.487936:   [RESULT]   type_token_ratio: 0.34413965087281795
2019-12-03 14:37:19.488030:   [RESULT]   lexical_density: 0.34413965087281795
2019-12-03 14:37:19.488130:   [RESULT]   char_per_word: 5.826086956521739
2019-12-03 14:37:19.488295:   [RESULT]   Score on PET Limiting: 6.514366258267375
2019-12-03 14:37:19.488469:   [RESULT]   type_token_ratio: 0.46621621621621623
2019-12-03 14:37:19.488619:   [RESULT]   lexical_density: 0.46621621621621623
2019-12-03 14:37:19.488789:   [RESULT]   char_per_word: 5.6521739130434785
2019-12-03 14:37:19.489034:   [RESULT]   Score on PET Limiting: 6.58460634547591
2019-12-03 14:37:19.489252:   [RESULT]   type_token_ratio: 0.3177570093457944
2019-12-03 14:37:19.489375:   [RESULT]   lexical_density: 0.3177570093457944
2019-12-03 14:37:19.489551:   [RESULT]   char_per_word: 5.573529411764706
2019-12-03 14:37:19.489655:   [RESULT]   Score on PET Limiting: 6.209043430456295
2019-12-03 14:37:19.489919:   [RESULT]   type_token_ratio: 0.4090909090909091
2019-12-03 14:37:19.490012:   [RESULT]   lexical_density: 0.4090909090909091
2019-12-03 14:37:19.490099:   [RESULT]   char_per_word: 5.4603174603174605
2019-12-03 14:37:19.490227:   [RESULT]   Score on PET Limiting: 6.278499278499279
2019-12-03 14:37:19.490345:   [RESULT]   type_token_ratio: 0.38913043478260867
2019-12-03 14:37:19.490448:   [RESULT]   lexical_density: 0.38913043478260867
2019-12-03 14:37:19.490555:   [RESULT]   char_per_word: 6.150837988826815
2019-12-03 14:37:19.490667:   [RESULT]   Score on PET Limiting: 6.929098858392033
2019-12-03 14:37:19.490758:   [RESULT]   type_token_ratio: 0.4782608695652174
2019-12-03 14:37:19.490854:   [RESULT]   lexical_density: 0.4782608695652174
2019-12-03 14:37:19.490939:   [RESULT]   char_per_word: 6.113636363636363
2019-12-03 14:37:19.491036:   [RESULT]   Score on PET Limiting: 7.070158102766799
2019-12-03 14:37:19.491120:   [RESULT]   type_token_ratio: 0.39552238805970147
2019-12-03 14:37:19.491214:   [RESULT]   lexical_density: 0.39552238805970147
2019-12-03 14:37:19.491293:   [RESULT]   char_per_word: 5.90566037735849
2019-12-03 14:37:19.491392:   [RESULT]   Score on PET Limiting: 6.696705153477893
2019-12-03 14:37:19.491539:   [RESULT]   type_token_ratio: 0.3917808219178082
2019-12-03 14:37:19.491619:   [RESULT]   lexical_density: 0.3917808219178082
2019-12-03 14:37:19.491781:   [RESULT]   char_per_word: 5.916083916083916
2019-12-03 14:37:19.491926:   [RESULT]   Score on PET Limiting: 6.699645559919532
2019-12-03 14:37:19.492017:   [RESULT]   type_token_ratio: 0.37748344370860926
2019-12-03 14:37:19.492109:   [RESULT]   lexical_density: 0.37748344370860926
2019-12-03 14:37:19.492191:   [RESULT]   char_per_word: 5.9298245614035086
2019-12-03 14:37:19.492290:   [RESULT]   Score on PET Limiting: 6.684791448820727
2019-12-03 14:37:19.492360:   [DONE  ]   Limiting PET Done
2019-12-03 14:37:19.492443:   [START ]   Start Reading FCE
2019-12-03 14:37:19.492561:   [START ]   Preprocess ./dataset/cambridge/FCE/1.txt Start
2019-12-03 14:37:19.585805:   [DONE  ]   Preprocess ./dataset/cambridge/FCE/1.txt Done
2019-12-03 14:37:19.586015:   [START ]   Preprocess ./dataset/cambridge/FCE/2.txt Start
2019-12-03 14:37:19.659981:   [DONE  ]   Preprocess ./dataset/cambridge/FCE/2.txt Done
2019-12-03 14:37:19.660264:   [START ]   Preprocess ./dataset/cambridge/FCE/4.txt Start
2019-12-03 14:37:19.746416:   [DONE  ]   Preprocess ./dataset/cambridge/FCE/4.txt Done
2019-12-03 14:37:19.746645:   [START ]   Preprocess ./dataset/cambridge/FCE/6.txt Start
2019-12-03 14:37:19.839605:   [DONE  ]   Preprocess ./dataset/cambridge/FCE/6.txt Done
2019-12-03 14:37:19.840637:   [START ]   Preprocess ./dataset/cambridge/FCE/7.txt Start
2019-12-03 14:37:19.940313:   [DONE  ]   Preprocess ./dataset/cambridge/FCE/7.txt Done
2019-12-03 14:37:19.940525:   [START ]   Preprocess ./dataset/cambridge/FCE/8.txt Start
2019-12-03 14:37:20.021419:   [DONE  ]   Preprocess ./dataset/cambridge/FCE/8.txt Done
2019-12-03 14:37:20.021766:   [START ]   Preprocess ./dataset/cambridge/FCE/10.txt Start
2019-12-03 14:37:20.124920:   [DONE  ]   Preprocess ./dataset/cambridge/FCE/10.txt Done
2019-12-03 14:37:20.125222:   [START ]   Preprocess ./dataset/cambridge/FCE/11.txt Start
2019-12-03 14:37:20.225786:   [DONE  ]   Preprocess ./dataset/cambridge/FCE/11.txt Done
2019-12-03 14:37:20.226027:   [START ]   Preprocess ./dataset/cambridge/FCE/12.txt Start
2019-12-03 14:37:20.321538:   [DONE  ]   Preprocess ./dataset/cambridge/FCE/12.txt Done
2019-12-03 14:37:20.321749:   [START ]   Preprocess ./dataset/cambridge/FCE/13.txt Start
2019-12-03 14:37:20.404542:   [DONE  ]   Preprocess ./dataset/cambridge/FCE/13.txt Done
2019-12-03 14:37:20.404746:   [START ]   Preprocess ./dataset/cambridge/FCE/14.txt Start
2019-12-03 14:37:20.485911:   [DONE  ]   Preprocess ./dataset/cambridge/FCE/14.txt Done
2019-12-03 14:37:20.486357:   [START ]   Preprocess ./dataset/cambridge/FCE/15.txt Start
2019-12-03 14:37:20.584801:   [DONE  ]   Preprocess ./dataset/cambridge/FCE/15.txt Done
2019-12-03 14:37:20.584947:   [DONE  ]   Read FCE Done
2019-12-03 14:37:20.585545:   [START ]   Limiting FCE Start
2019-12-03 14:37:20.585698:   [RESULT]   type_token_ratio: 0.34206896551724136
2019-12-03 14:37:20.585794:   [RESULT]   lexical_density: 0.34206896551724136
2019-12-03 14:37:20.585905:   [RESULT]   char_per_word: 5.919354838709677
2019-12-03 14:37:20.585993:   [RESULT]   Score on FCE Limiting: 6.60349276974416
2019-12-03 14:37:20.586118:   [RESULT]   type_token_ratio: 0.3769911504424779
2019-12-03 14:37:20.586202:   [RESULT]   lexical_density: 0.3769911504424779
2019-12-03 14:37:20.586305:   [RESULT]   char_per_word: 5.962441314553991
2019-12-03 14:37:20.586434:   [RESULT]   Score on FCE Limiting: 6.716423615438947
2019-12-03 14:37:20.586583:   [RESULT]   type_token_ratio: 0.3472429210134128
2019-12-03 14:37:20.586687:   [RESULT]   lexical_density: 0.3472429210134128
2019-12-03 14:37:20.586815:   [RESULT]   char_per_word: 5.896995708154506
2019-12-03 14:37:20.586919:   [RESULT]   Score on FCE Limiting: 6.5914815501813315
2019-12-03 14:37:20.587070:   [RESULT]   type_token_ratio: 0.31420765027322406
2019-12-03 14:37:20.587170:   [RESULT]   lexical_density: 0.31420765027322406
2019-12-03 14:37:20.587331:   [RESULT]   char_per_word: 5.773913043478261
2019-12-03 14:37:20.587578:   [RESULT]   Score on FCE Limiting: 6.40232834402471
2019-12-03 14:37:20.587722:   [RESULT]   type_token_ratio: 0.29839883551673946
2019-12-03 14:37:20.587811:   [RESULT]   lexical_density: 0.29839883551673946
2019-12-03 14:37:20.588024:   [RESULT]   char_per_word: 5.590243902439024
2019-12-03 14:37:20.588128:   [RESULT]   Score on FCE Limiting: 6.187041573472502
2019-12-03 14:37:20.588264:   [RESULT]   type_token_ratio: 0.3258064516129032
2019-12-03 14:37:20.588359:   [RESULT]   lexical_density: 0.3258064516129032
2019-12-03 14:37:20.588485:   [RESULT]   char_per_word: 6.01980198019802
2019-12-03 14:37:20.588574:   [RESULT]   Score on FCE Limiting: 6.671414883423827
2019-12-03 14:37:20.588809:   [RESULT]   type_token_ratio: 0.3584
2019-12-03 14:37:20.588911:   [RESULT]   lexical_density: 0.3584
2019-12-03 14:37:20.589050:   [RESULT]   char_per_word: 5.834821428571429
2019-12-03 14:37:20.589142:   [RESULT]   Score on FCE Limiting: 6.551621428571428
2019-12-03 14:37:20.589335:   [RESULT]   type_token_ratio: 0.3603174603174603
2019-12-03 14:37:20.589433:   [RESULT]   lexical_density: 0.3603174603174603
2019-12-03 14:37:20.589636:   [RESULT]   char_per_word: 5.872246696035242
2019-12-03 14:37:20.589757:   [RESULT]   Score on FCE Limiting: 6.592881616670162
2019-12-03 14:37:20.590051:   [RESULT]   type_token_ratio: 0.363905325443787
2019-12-03 14:37:20.590377:   [RESULT]   lexical_density: 0.363905325443787
2019-12-03 14:37:20.590807:   [RESULT]   char_per_word: 6.2317073170731705
2019-12-03 14:37:20.591230:   [RESULT]   Score on FCE Limiting: 6.959517967960745
2019-12-03 14:37:20.591599:   [RESULT]   type_token_ratio: 0.36316695352839934
2019-12-03 14:37:20.591757:   [RESULT]   lexical_density: 0.36316695352839934
2019-12-03 14:37:20.591936:   [RESULT]   char_per_word: 6.312796208530806
2019-12-03 14:37:20.592034:   [RESULT]   Score on FCE Limiting: 7.039130115587604
2019-12-03 14:37:20.592202:   [RESULT]   type_token_ratio: 0.32685512367491165
2019-12-03 14:37:20.592287:   [RESULT]   lexical_density: 0.32685512367491165
2019-12-03 14:37:20.592448:   [RESULT]   char_per_word: 6.005405405405406
2019-12-03 14:37:20.592566:   [RESULT]   Score on FCE Limiting: 6.659115652755228
2019-12-03 14:37:20.592727:   [RESULT]   type_token_ratio: 0.3848396501457726
2019-12-03 14:37:20.592813:   [RESULT]   lexical_density: 0.3848396501457726
2019-12-03 14:37:20.593020:   [RESULT]   char_per_word: 6.318181818181818
2019-12-03 14:37:20.593127:   [RESULT]   Score on FCE Limiting: 7.087861118473363
2019-12-03 14:37:20.593262:   [DONE  ]   Limiting FCE Done
2019-12-03 14:37:20.593358:   [START ]   Start Reading CAE
2019-12-03 14:37:20.594159:   [START ]   Preprocess ./dataset/cambridge/CAE/1.txt Start
2019-12-03 14:37:20.673516:   [DONE  ]   Preprocess ./dataset/cambridge/CAE/1.txt Done
2019-12-03 14:37:20.675204:   [START ]   Preprocess ./dataset/cambridge/CAE/2.txt Start
2019-12-03 14:37:20.744563:   [DONE  ]   Preprocess ./dataset/cambridge/CAE/2.txt Done
2019-12-03 14:37:20.745828:   [START ]   Preprocess ./dataset/cambridge/CAE/3.txt Start
2019-12-03 14:37:20.871093:   [DONE  ]   Preprocess ./dataset/cambridge/CAE/3.txt Done
2019-12-03 14:37:20.871380:   [START ]   Preprocess ./dataset/cambridge/CAE/4.txt Start
2019-12-03 14:37:21.021700:   [DONE  ]   Preprocess ./dataset/cambridge/CAE/4.txt Done
2019-12-03 14:37:21.021894:   [START ]   Preprocess ./dataset/cambridge/CAE/5.txt Start
2019-12-03 14:37:21.108418:   [DONE  ]   Preprocess ./dataset/cambridge/CAE/5.txt Done
2019-12-03 14:37:21.108675:   [START ]   Preprocess ./dataset/cambridge/CAE/6.txt Start
2019-12-03 14:37:21.209154:   [DONE  ]   Preprocess ./dataset/cambridge/CAE/6.txt Done
2019-12-03 14:37:21.209378:   [START ]   Preprocess ./dataset/cambridge/CAE/7.txt Start
2019-12-03 14:37:21.320493:   [DONE  ]   Preprocess ./dataset/cambridge/CAE/7.txt Done
2019-12-03 14:37:21.320796:   [START ]   Preprocess ./dataset/cambridge/CAE/8.txt Start
2019-12-03 14:37:21.431820:   [DONE  ]   Preprocess ./dataset/cambridge/CAE/8.txt Done
2019-12-03 14:37:21.433044:   [START ]   Preprocess ./dataset/cambridge/CAE/11.txt Start
2019-12-03 14:37:21.504265:   [DONE  ]   Preprocess ./dataset/cambridge/CAE/11.txt Done
2019-12-03 14:37:21.505116:   [START ]   Preprocess ./dataset/cambridge/CAE/12.txt Start
2019-12-03 14:37:21.625030:   [DONE  ]   Preprocess ./dataset/cambridge/CAE/12.txt Done
2019-12-03 14:37:21.626635:   [START ]   Preprocess ./dataset/cambridge/CAE/14.txt Start
2019-12-03 14:37:21.702478:   [DONE  ]   Preprocess ./dataset/cambridge/CAE/14.txt Done
2019-12-03 14:37:21.703470:   [START ]   Preprocess ./dataset/cambridge/CAE/15.txt Start
2019-12-03 14:37:21.770649:   [DONE  ]   Preprocess ./dataset/cambridge/CAE/15.txt Done
2019-12-03 14:37:21.770797:   [DONE  ]   Read CAE Done
2019-12-03 14:37:21.772261:   [START ]   Limiting CAE Start
2019-12-03 14:37:21.772745:   [RESULT]   type_token_ratio: 0.47285464098073554
2019-12-03 14:37:21.772901:   [RESULT]   lexical_density: 0.47285464098073554
2019-12-03 14:37:21.773161:   [RESULT]   char_per_word: 6.233333333333333
2019-12-03 14:37:21.773318:   [RESULT]   Score on CAE Limiting: 7.179042615294805
2019-12-03 14:37:21.773468:   [RESULT]   type_token_ratio: 0.40040241448692154
2019-12-03 14:37:21.773593:   [RESULT]   lexical_density: 0.40040241448692154
2019-12-03 14:37:21.773730:   [RESULT]   char_per_word: 6.582914572864322
2019-12-03 14:37:21.773887:   [RESULT]   Score on CAE Limiting: 7.3837194018381656
2019-12-03 14:37:21.774056:   [RESULT]   type_token_ratio: 0.3070362473347548
2019-12-03 14:37:21.774185:   [RESULT]   lexical_density: 0.3070362473347548
2019-12-03 14:37:21.774320:   [RESULT]   char_per_word: 5.90625
2019-12-03 14:37:21.774410:   [RESULT]   Score on CAE Limiting: 6.520322494669509
2019-12-03 14:37:21.774721:   [RESULT]   type_token_ratio: 0.38316151202749144
2019-12-03 14:37:21.774854:   [RESULT]   lexical_density: 0.38316151202749144
2019-12-03 14:37:21.775047:   [RESULT]   char_per_word: 6.692825112107624
2019-12-03 14:37:21.775230:   [RESULT]   Score on CAE Limiting: 7.459148136162607
2019-12-03 14:37:21.775388:   [RESULT]   type_token_ratio: 0.3851963746223565
2019-12-03 14:37:21.775481:   [RESULT]   lexical_density: 0.3851963746223565
2019-12-03 14:37:21.775594:   [RESULT]   char_per_word: 7.070588235294117
2019-12-03 14:37:21.775755:   [RESULT]   Score on CAE Limiting: 7.840980984538831
2019-12-03 14:37:21.775904:   [RESULT]   type_token_ratio: 0.3471502590673575
2019-12-03 14:37:21.775999:   [RESULT]   lexical_density: 0.3471502590673575
2019-12-03 14:37:21.776251:   [RESULT]   char_per_word: 6.634328358208955
2019-12-03 14:37:21.776425:   [RESULT]   Score on CAE Limiting: 7.328628876343671
2019-12-03 14:37:21.776573:   [RESULT]   type_token_ratio: 0.28782707622298065
2019-12-03 14:37:21.776667:   [RESULT]   lexical_density: 0.28782707622298065
2019-12-03 14:37:21.776832:   [RESULT]   char_per_word: 6.529644268774703
2019-12-03 14:37:21.776933:   [RESULT]   Score on CAE Limiting: 7.105298421220665
2019-12-03 14:37:21.777103:   [RESULT]   type_token_ratio: 0.39934711643090315
2019-12-03 14:37:21.777204:   [RESULT]   lexical_density: 0.39934711643090315
2019-12-03 14:37:21.777352:   [RESULT]   char_per_word: 6.4959128065395095
2019-12-03 14:37:21.777525:   [RESULT]   Score on CAE Limiting: 7.294607039401315
2019-12-03 14:37:21.777652:   [RESULT]   type_token_ratio: 0.43853820598006643
2019-12-03 14:37:21.777740:   [RESULT]   lexical_density: 0.43853820598006643
2019-12-03 14:37:21.777950:   [RESULT]   char_per_word: 6.371212121212121
2019-12-03 14:37:21.778052:   [RESULT]   Score on CAE Limiting: 7.2482885331722535
2019-12-03 14:37:21.778220:   [RESULT]   type_token_ratio: 0.3727175080558539
2019-12-03 14:37:21.778319:   [RESULT]   lexical_density: 0.3727175080558539
2019-12-03 14:37:21.778475:   [RESULT]   char_per_word: 6.296829971181556
2019-12-03 14:37:21.778644:   [RESULT]   Score on CAE Limiting: 7.042264987293264
2019-12-03 14:37:21.778760:   [RESULT]   type_token_ratio: 0.36879432624113473
2019-12-03 14:37:21.778846:   [RESULT]   lexical_density: 0.36879432624113473
2019-12-03 14:37:21.779029:   [RESULT]   char_per_word: 6.586538461538462
2019-12-03 14:37:21.779115:   [RESULT]   Score on CAE Limiting: 7.324127114020731
2019-12-03 14:37:21.779262:   [RESULT]   type_token_ratio: 0.36278195488721804
2019-12-03 14:37:21.779352:   [RESULT]   lexical_density: 0.36278195488721804
2019-12-03 14:37:21.779521:   [RESULT]   char_per_word: 6.27979274611399
2019-12-03 14:37:21.779617:   [RESULT]   Score on CAE Limiting: 7.005356655888426
2019-12-03 14:37:21.779708:   [DONE  ]   Limiting CAE Done
2019-12-03 14:37:21.779787:   [START ]   Start Reading CPE
2019-12-03 14:37:21.780812:   [START ]   Preprocess ./dataset/cambridge/CPE/1.txt Start
2019-12-03 14:37:21.848538:   [DONE  ]   Preprocess ./dataset/cambridge/CPE/1.txt Done
2019-12-03 14:37:21.849913:   [START ]   Preprocess ./dataset/cambridge/CPE/2.txt Start
2019-12-03 14:37:21.958965:   [DONE  ]   Preprocess ./dataset/cambridge/CPE/2.txt Done
2019-12-03 14:37:21.959992:   [START ]   Preprocess ./dataset/cambridge/CPE/3.txt Start
2019-12-03 14:37:22.100229:   [DONE  ]   Preprocess ./dataset/cambridge/CPE/3.txt Done
2019-12-03 14:37:22.101151:   [START ]   Preprocess ./dataset/cambridge/CPE/4.txt Start
2019-12-03 14:37:22.170230:   [DONE  ]   Preprocess ./dataset/cambridge/CPE/4.txt Done
2019-12-03 14:37:22.171746:   [START ]   Preprocess ./dataset/cambridge/CPE/5.txt Start
2019-12-03 14:37:22.226682:   [DONE  ]   Preprocess ./dataset/cambridge/CPE/5.txt Done
2019-12-03 14:37:22.228120:   [START ]   Preprocess ./dataset/cambridge/CPE/6.txt Start
2019-12-03 14:37:22.351458:   [DONE  ]   Preprocess ./dataset/cambridge/CPE/6.txt Done
2019-12-03 14:37:22.352829:   [START ]   Preprocess ./dataset/cambridge/CPE/7.txt Start
2019-12-03 14:37:22.471566:   [DONE  ]   Preprocess ./dataset/cambridge/CPE/7.txt Done
2019-12-03 14:37:22.472663:   [START ]   Preprocess ./dataset/cambridge/CPE/8.txt Start
2019-12-03 14:37:22.569585:   [DONE  ]   Preprocess ./dataset/cambridge/CPE/8.txt Done
2019-12-03 14:37:22.570656:   [START ]   Preprocess ./dataset/cambridge/CPE/9.txt Start
2019-12-03 14:37:22.627816:   [DONE  ]   Preprocess ./dataset/cambridge/CPE/9.txt Done
2019-12-03 14:37:22.628851:   [START ]   Preprocess ./dataset/cambridge/CPE/11.txt Start
2019-12-03 14:37:22.736828:   [DONE  ]   Preprocess ./dataset/cambridge/CPE/11.txt Done
2019-12-03 14:37:22.737735:   [START ]   Preprocess ./dataset/cambridge/CPE/13.txt Start
2019-12-03 14:37:22.792762:   [DONE  ]   Preprocess ./dataset/cambridge/CPE/13.txt Done
2019-12-03 14:37:22.793257:   [START ]   Preprocess ./dataset/cambridge/CPE/14.txt Start
2019-12-03 14:37:22.911115:   [DONE  ]   Preprocess ./dataset/cambridge/CPE/14.txt Done
2019-12-03 14:37:22.911263:   [DONE  ]   Read CPE Done
2019-12-03 14:37:22.912549:   [START ]   Limiting CPE Start
2019-12-03 14:37:22.912820:   [RESULT]   type_token_ratio: 0.42857142857142855
2019-12-03 14:37:22.912991:   [RESULT]   lexical_density: 0.42857142857142855
2019-12-03 14:37:22.913156:   [RESULT]   char_per_word: 6.173913043478261
2019-12-03 14:37:22.913284:   [RESULT]   Score on CPE Limiting: 7.031055900621118
2019-12-03 14:37:22.913514:   [RESULT]   type_token_ratio: 0.4033018867924528
2019-12-03 14:37:22.913648:   [RESULT]   lexical_density: 0.4033018867924528
2019-12-03 14:37:22.913848:   [RESULT]   char_per_word: 6.637426900584796
2019-12-03 14:37:22.913979:   [RESULT]   Score on CPE Limiting: 7.444030674169701
2019-12-03 14:37:22.914294:   [RESULT]   type_token_ratio: 0.3888888888888889
2019-12-03 14:37:22.914464:   [RESULT]   lexical_density: 0.3888888888888889
2019-12-03 14:37:22.914654:   [RESULT]   char_per_word: 6.466165413533835
2019-12-03 14:37:22.914861:   [RESULT]   Score on CPE Limiting: 7.2439431913116135
2019-12-03 14:37:22.915023:   [RESULT]   type_token_ratio: 0.39679715302491103
2019-12-03 14:37:22.915199:   [RESULT]   lexical_density: 0.39679715302491103
2019-12-03 14:37:22.915417:   [RESULT]   char_per_word: 7.044843049327354
2019-12-03 14:37:22.915545:   [RESULT]   Score on CPE Limiting: 7.8384373553771765
2019-12-03 14:37:22.915802:   [RESULT]   type_token_ratio: 0.4330357142857143
2019-12-03 14:37:22.916100:   [RESULT]   lexical_density: 0.4330357142857143
2019-12-03 14:37:22.916289:   [RESULT]   char_per_word: 6.283505154639175
2019-12-03 14:37:22.916484:   [RESULT]   Score on CPE Limiting: 7.149576583210604
2019-12-03 14:37:22.916666:   [RESULT]   type_token_ratio: 0.36129032258064514
2019-12-03 14:37:22.916784:   [RESULT]   lexical_density: 0.36129032258064514
2019-12-03 14:37:22.917020:   [RESULT]   char_per_word: 6.854166666666667
2019-12-03 14:37:22.917155:   [RESULT]   Score on CPE Limiting: 7.576747311827957
2019-12-03 14:37:22.917422:   [RESULT]   type_token_ratio: 0.3824152542372881
2019-12-03 14:37:22.917552:   [RESULT]   lexical_density: 0.3824152542372881
2019-12-03 14:37:22.917752:   [RESULT]   char_per_word: 5.966759002770083
2019-12-03 14:37:22.917943:   [RESULT]   Score on CPE Limiting: 6.7315895112446595
2019-12-03 14:37:22.918100:   [RESULT]   type_token_ratio: 0.27471116816431324
2019-12-03 14:37:22.918202:   [RESULT]   lexical_density: 0.27471116816431324
2019-12-03 14:37:22.918344:   [RESULT]   char_per_word: 6.242990654205608
2019-12-03 14:37:22.918445:   [RESULT]   Score on CPE Limiting: 6.792412990534235
2019-12-03 14:37:22.918587:   [RESULT]   type_token_ratio: 0.4012875536480687
2019-12-03 14:37:22.918685:   [RESULT]   lexical_density: 0.4012875536480687
2019-12-03 14:37:22.918933:   [RESULT]   char_per_word: 6.106951871657754
2019-12-03 14:37:22.919117:   [RESULT]   Score on CPE Limiting: 6.909526978953892
2019-12-03 14:37:22.919291:   [RESULT]   type_token_ratio: 0.38571428571428573
2019-12-03 14:37:22.919426:   [RESULT]   lexical_density: 0.38571428571428573
2019-12-03 14:37:22.919632:   [RESULT]   char_per_word: 6.64957264957265
2019-12-03 14:37:22.919758:   [RESULT]   Score on CPE Limiting: 7.421001221001222
2019-12-03 14:37:22.919908:   [RESULT]   type_token_ratio: 0.4451901565995526
2019-12-03 14:37:22.920011:   [RESULT]   lexical_density: 0.4451901565995526
2019-12-03 14:37:22.920153:   [RESULT]   char_per_word: 7.015075376884422
2019-12-03 14:37:22.920330:   [RESULT]   Score on CPE Limiting: 7.905455690083526
2019-12-03 14:37:22.920495:   [RESULT]   type_token_ratio: 0.35353535353535354
2019-12-03 14:37:22.920606:   [RESULT]   lexical_density: 0.35353535353535354
2019-12-03 14:37:22.920754:   [RESULT]   char_per_word: 6.095238095238095
2019-12-03 14:37:22.920861:   [RESULT]   Score on CPE Limiting: 6.8023088023088025
2019-12-03 14:37:22.920953:   [DONE  ]   Limiting CPE Done
2019-12-03 14:37:22.921040:   [DONE  ]   Traning data Done
2019-12-03 14:37:22.921346:   [START ]   Test Start
2019-12-03 14:37:22.921535:   [START ]   Reading Test ./dataset/cambridge/KET/12.txt Start
2019-12-03 14:37:22.921679:   [START ]   Preprocess ./dataset/cambridge/KET/12.txt Start
2019-12-03 14:37:22.935322:   [DONE  ]   Preprocess ./dataset/cambridge/KET/12.txt Done
2019-12-03 14:37:22.935418:   [DONE  ]   Reading Test ./dataset/cambridge/KET/12.txt Done
2019-12-03 14:37:22.935725:   [START ]   Scoring KET Start
2019-12-03 14:37:53.728382:   [INFO  ]   -----------------------------------------------------------------------------
2019-12-03 14:37:53.728558:   [INFO  ]   Hello world!
2019-12-03 14:37:53.729418:   [INFO  ]   Start main processing!
2019-12-03 14:37:53.729804:   [START ]   Start Prepare Traning data
2019-12-03 14:37:53.730093:   [START ]   Initialize parameters Start
2019-12-03 14:37:53.730201:   [DONE  ]   Initialize parameters Done
2019-12-03 14:37:53.730384:   [START ]   Start setup FOLD
2019-12-03 14:37:53.730501:   [START ]   Setup FOLD Done
2019-12-03 14:37:53.730572:   [START ]   Start Training data
2019-12-03 14:37:53.730647:   [START ]   Start Reading KET
2019-12-03 14:37:53.730881:   [START ]   Preprocess ./dataset/cambridge/KET/1.txt Start
2019-12-03 14:37:55.102217:   [DONE  ]   Preprocess ./dataset/cambridge/KET/1.txt Done
2019-12-03 14:37:55.102490:   [START ]   Preprocess ./dataset/cambridge/KET/2.txt Start
2019-12-03 14:37:55.120753:   [DONE  ]   Preprocess ./dataset/cambridge/KET/2.txt Done
2019-12-03 14:37:55.120970:   [START ]   Preprocess ./dataset/cambridge/KET/3.txt Start
2019-12-03 14:37:55.133583:   [DONE  ]   Preprocess ./dataset/cambridge/KET/3.txt Done
2019-12-03 14:37:55.133781:   [START ]   Preprocess ./dataset/cambridge/KET/5.txt Start
2019-12-03 14:37:55.148319:   [DONE  ]   Preprocess ./dataset/cambridge/KET/5.txt Done
2019-12-03 14:37:55.148552:   [START ]   Preprocess ./dataset/cambridge/KET/6.txt Start
2019-12-03 14:37:55.161658:   [DONE  ]   Preprocess ./dataset/cambridge/KET/6.txt Done
2019-12-03 14:37:55.161854:   [START ]   Preprocess ./dataset/cambridge/KET/7.txt Start
2019-12-03 14:37:55.184157:   [DONE  ]   Preprocess ./dataset/cambridge/KET/7.txt Done
2019-12-03 14:37:55.184297:   [START ]   Preprocess ./dataset/cambridge/KET/9.txt Start
2019-12-03 14:37:55.196691:   [DONE  ]   Preprocess ./dataset/cambridge/KET/9.txt Done
2019-12-03 14:37:55.196830:   [START ]   Preprocess ./dataset/cambridge/KET/10.txt Start
2019-12-03 14:37:55.220116:   [DONE  ]   Preprocess ./dataset/cambridge/KET/10.txt Done
2019-12-03 14:37:55.220283:   [START ]   Preprocess ./dataset/cambridge/KET/11.txt Start
2019-12-03 14:37:55.234086:   [DONE  ]   Preprocess ./dataset/cambridge/KET/11.txt Done
2019-12-03 14:37:55.234233:   [START ]   Preprocess ./dataset/cambridge/KET/12.txt Start
2019-12-03 14:37:55.246460:   [DONE  ]   Preprocess ./dataset/cambridge/KET/12.txt Done
2019-12-03 14:37:55.246596:   [START ]   Preprocess ./dataset/cambridge/KET/13.txt Start
2019-12-03 14:37:55.270461:   [DONE  ]   Preprocess ./dataset/cambridge/KET/13.txt Done
2019-12-03 14:37:55.270732:   [START ]   Preprocess ./dataset/cambridge/KET/14.txt Start
2019-12-03 14:37:55.284753:   [DONE  ]   Preprocess ./dataset/cambridge/KET/14.txt Done
2019-12-03 14:37:55.284849:   [DONE  ]   Read KET Done
2019-12-03 14:37:55.285148:   [START ]   Limiting KET Start
2019-12-03 14:37:55.285256:   [RESULT]   type_token_ratio: 0.4
2019-12-03 14:37:55.285430:   [RESULT]   lexical_density: 0.4
2019-12-03 14:37:55.285608:   [RESULT]   char_per_word: 5.355263157894737
2019-12-03 14:37:55.285703:   [RESULT]   Score on KET Limiting: 6.155263157894738
2019-12-03 14:37:55.285826:   [RESULT]   type_token_ratio: 0.35294117647058826
2019-12-03 14:37:55.285936:   [RESULT]   lexical_density: 0.35294117647058826
2019-12-03 14:37:55.286050:   [RESULT]   char_per_word: 4.880952380952381
2019-12-03 14:37:55.286160:   [RESULT]   Score on KET Limiting: 5.586834733893557
2019-12-03 14:37:55.286288:   [RESULT]   type_token_ratio: 0.5204081632653061
2019-12-03 14:37:55.286477:   [RESULT]   lexical_density: 0.5204081632653061
2019-12-03 14:37:55.286657:   [RESULT]   char_per_word: 4.882352941176471
2019-12-03 14:37:55.286735:   [RESULT]   Score on KET Limiting: 5.923169267707083
2019-12-03 14:37:55.286921:   [RESULT]   type_token_ratio: 0.42016806722689076
2019-12-03 14:37:55.287004:   [RESULT]   lexical_density: 0.42016806722689076
2019-12-03 14:37:55.287083:   [RESULT]   char_per_word: 5.18
2019-12-03 14:37:55.287211:   [RESULT]   Score on KET Limiting: 6.020336134453782
2019-12-03 14:37:55.287292:   [RESULT]   type_token_ratio: 0.38596491228070173
2019-12-03 14:37:55.287382:   [RESULT]   lexical_density: 0.38596491228070173
2019-12-03 14:37:55.287510:   [RESULT]   char_per_word: 4.931818181818182
2019-12-03 14:37:55.287653:   [RESULT]   Score on KET Limiting: 5.703748006379586
2019-12-03 14:37:55.287813:   [RESULT]   type_token_ratio: 0.38860103626943004
2019-12-03 14:37:55.287922:   [RESULT]   lexical_density: 0.38860103626943004
2019-12-03 14:37:55.288014:   [RESULT]   char_per_word: 5.56
2019-12-03 14:37:55.288235:   [RESULT]   Score on KET Limiting: 6.337202072538859
2019-12-03 14:37:55.288313:   [RESULT]   type_token_ratio: 0.4074074074074074
2019-12-03 14:37:55.288382:   [RESULT]   lexical_density: 0.4074074074074074
2019-12-03 14:37:55.288544:   [RESULT]   char_per_word: 5.2727272727272725
2019-12-03 14:37:55.288629:   [RESULT]   Score on KET Limiting: 6.087542087542087
2019-12-03 14:37:55.288776:   [RESULT]   type_token_ratio: 0.35175879396984927
2019-12-03 14:37:55.288847:   [RESULT]   lexical_density: 0.35175879396984927
2019-12-03 14:37:55.289036:   [RESULT]   char_per_word: 5.271428571428571
2019-12-03 14:37:55.289111:   [RESULT]   Score on KET Limiting: 5.97494615936827
2019-12-03 14:37:55.289273:   [RESULT]   type_token_ratio: 0.31932773109243695
2019-12-03 14:37:55.289373:   [RESULT]   lexical_density: 0.31932773109243695
2019-12-03 14:37:55.289450:   [RESULT]   char_per_word: 5.526315789473684
2019-12-03 14:37:55.289534:   [RESULT]   Score on KET Limiting: 6.1649712516585575
2019-12-03 14:37:55.289618:   [RESULT]   type_token_ratio: 0.3125
2019-12-03 14:37:55.289716:   [RESULT]   lexical_density: 0.3125
2019-12-03 14:37:55.289793:   [RESULT]   char_per_word: 4.685714285714286
2019-12-03 14:37:55.289897:   [RESULT]   Score on KET Limiting: 5.310714285714286
2019-12-03 14:37:55.289986:   [RESULT]   type_token_ratio: 0.2914572864321608
2019-12-03 14:37:55.290082:   [RESULT]   lexical_density: 0.2914572864321608
2019-12-03 14:37:55.290162:   [RESULT]   char_per_word: 5.155172413793103
2019-12-03 14:37:55.290264:   [RESULT]   Score on KET Limiting: 5.738086986657425
2019-12-03 14:37:55.290347:   [RESULT]   type_token_ratio: 0.41228070175438597
2019-12-03 14:37:55.290446:   [RESULT]   lexical_density: 0.41228070175438597
2019-12-03 14:37:55.290523:   [RESULT]   char_per_word: 4.957446808510638
2019-12-03 14:37:55.290628:   [RESULT]   Score on KET Limiting: 5.78200821201941
2019-12-03 14:37:55.290697:   [DONE  ]   Limiting KET Done
2019-12-03 14:37:55.290809:   [START ]   Start Reading PET
2019-12-03 14:37:55.290925:   [START ]   Preprocess ./dataset/cambridge/PET/2.txt Start
2019-12-03 14:37:55.315582:   [DONE  ]   Preprocess ./dataset/cambridge/PET/2.txt Done
2019-12-03 14:37:55.315789:   [START ]   Preprocess ./dataset/cambridge/PET/4.txt Start
2019-12-03 14:37:55.360999:   [DONE  ]   Preprocess ./dataset/cambridge/PET/4.txt Done
2019-12-03 14:37:55.361219:   [START ]   Preprocess ./dataset/cambridge/PET/5.txt Start
2019-12-03 14:37:55.386320:   [DONE  ]   Preprocess ./dataset/cambridge/PET/5.txt Done
2019-12-03 14:37:55.386469:   [START ]   Preprocess ./dataset/cambridge/PET/6.txt Start
2019-12-03 14:37:55.403542:   [DONE  ]   Preprocess ./dataset/cambridge/PET/6.txt Done
2019-12-03 14:37:55.403692:   [START ]   Preprocess ./dataset/cambridge/PET/7.txt Start
2019-12-03 14:37:55.449335:   [DONE  ]   Preprocess ./dataset/cambridge/PET/7.txt Done
2019-12-03 14:37:55.449496:   [START ]   Preprocess ./dataset/cambridge/PET/8.txt Start
2019-12-03 14:37:55.475116:   [DONE  ]   Preprocess ./dataset/cambridge/PET/8.txt Done
2019-12-03 14:37:55.475356:   [START ]   Preprocess ./dataset/cambridge/PET/9.txt Start
2019-12-03 14:37:55.493471:   [DONE  ]   Preprocess ./dataset/cambridge/PET/9.txt Done
2019-12-03 14:37:55.493628:   [START ]   Preprocess ./dataset/cambridge/PET/10.txt Start
2019-12-03 14:37:55.544740:   [DONE  ]   Preprocess ./dataset/cambridge/PET/10.txt Done
2019-12-03 14:37:55.544932:   [START ]   Preprocess ./dataset/cambridge/PET/11.txt Start
2019-12-03 14:37:55.576082:   [DONE  ]   Preprocess ./dataset/cambridge/PET/11.txt Done
2019-12-03 14:37:55.576281:   [START ]   Preprocess ./dataset/cambridge/PET/12.txt Start
2019-12-03 14:37:55.591894:   [DONE  ]   Preprocess ./dataset/cambridge/PET/12.txt Done
2019-12-03 14:37:55.592051:   [START ]   Preprocess ./dataset/cambridge/PET/14.txt Start
2019-12-03 14:37:55.615501:   [DONE  ]   Preprocess ./dataset/cambridge/PET/14.txt Done
2019-12-03 14:37:55.615658:   [START ]   Preprocess ./dataset/cambridge/PET/15.txt Start
2019-12-03 14:37:55.633398:   [DONE  ]   Preprocess ./dataset/cambridge/PET/15.txt Done
2019-12-03 14:37:55.633496:   [DONE  ]   Read PET Done
2019-12-03 14:37:55.633874:   [START ]   Limiting PET Start
2019-12-03 14:37:55.633984:   [RESULT]   type_token_ratio: 0.40096618357487923
2019-12-03 14:37:55.634153:   [RESULT]   lexical_density: 0.40096618357487923
2019-12-03 14:37:55.634360:   [RESULT]   char_per_word: 5.674698795180723
2019-12-03 14:37:55.634484:   [RESULT]   Score on PET Limiting: 6.476631162330482
2019-12-03 14:37:55.634678:   [RESULT]   type_token_ratio: 0.34413965087281795
2019-12-03 14:37:55.634830:   [RESULT]   lexical_density: 0.34413965087281795
2019-12-03 14:37:55.634924:   [RESULT]   char_per_word: 5.826086956521739
2019-12-03 14:37:55.635110:   [RESULT]   Score on PET Limiting: 6.514366258267375
2019-12-03 14:37:55.635218:   [RESULT]   type_token_ratio: 0.39545454545454545
2019-12-03 14:37:55.635400:   [RESULT]   lexical_density: 0.39545454545454545
2019-12-03 14:37:55.635492:   [RESULT]   char_per_word: 5.310344827586207
2019-12-03 14:37:55.635569:   [RESULT]   Score on PET Limiting: 6.101253918495298
2019-12-03 14:37:55.635832:   [RESULT]   type_token_ratio: 0.46621621621621623
2019-12-03 14:37:55.636030:   [RESULT]   lexical_density: 0.46621621621621623
2019-12-03 14:37:55.636115:   [RESULT]   char_per_word: 5.6521739130434785
2019-12-03 14:37:55.636286:   [RESULT]   Score on PET Limiting: 6.58460634547591
2019-12-03 14:37:55.636439:   [RESULT]   type_token_ratio: 0.3906633906633907
2019-12-03 14:37:55.636516:   [RESULT]   lexical_density: 0.3906633906633907
2019-12-03 14:37:55.636723:   [RESULT]   char_per_word: 6.062893081761007
2019-12-03 14:37:55.636801:   [RESULT]   Score on PET Limiting: 6.844219863087788
2019-12-03 14:37:55.637002:   [RESULT]   type_token_ratio: 0.3177570093457944
2019-12-03 14:37:55.637186:   [RESULT]   lexical_density: 0.3177570093457944
2019-12-03 14:37:55.637262:   [RESULT]   char_per_word: 5.573529411764706
2019-12-03 14:37:55.637415:   [RESULT]   Score on PET Limiting: 6.209043430456295
2019-12-03 14:37:55.637498:   [RESULT]   type_token_ratio: 0.4090909090909091
2019-12-03 14:37:55.637594:   [RESULT]   lexical_density: 0.4090909090909091
2019-12-03 14:37:55.637674:   [RESULT]   char_per_word: 5.4603174603174605
2019-12-03 14:37:55.637774:   [RESULT]   Score on PET Limiting: 6.278499278499279
2019-12-03 14:37:55.637890:   [RESULT]   type_token_ratio: 0.38913043478260867
2019-12-03 14:37:55.637968:   [RESULT]   lexical_density: 0.38913043478260867
2019-12-03 14:37:55.638057:   [RESULT]   char_per_word: 6.150837988826815
2019-12-03 14:37:55.638181:   [RESULT]   Score on PET Limiting: 6.929098858392033
2019-12-03 14:37:55.638262:   [RESULT]   type_token_ratio: 0.4782608695652174
2019-12-03 14:37:55.638333:   [RESULT]   lexical_density: 0.4782608695652174
2019-12-03 14:37:55.638509:   [RESULT]   char_per_word: 6.113636363636363
2019-12-03 14:37:55.638581:   [RESULT]   Score on PET Limiting: 7.070158102766799
2019-12-03 14:37:55.638719:   [RESULT]   type_token_ratio: 0.39552238805970147
2019-12-03 14:37:55.638862:   [RESULT]   lexical_density: 0.39552238805970147
2019-12-03 14:37:55.638936:   [RESULT]   char_per_word: 5.90566037735849
2019-12-03 14:37:55.639103:   [RESULT]   Score on PET Limiting: 6.696705153477893
2019-12-03 14:37:55.639187:   [RESULT]   type_token_ratio: 0.39215686274509803
2019-12-03 14:37:55.639313:   [RESULT]   lexical_density: 0.39215686274509803
2019-12-03 14:37:55.639389:   [RESULT]   char_per_word: 5.9625
2019-12-03 14:37:55.639518:   [RESULT]   Score on PET Limiting: 6.746813725490197
2019-12-03 14:37:55.639597:   [RESULT]   type_token_ratio: 0.37748344370860926
2019-12-03 14:37:55.639724:   [RESULT]   lexical_density: 0.37748344370860926
2019-12-03 14:37:55.639798:   [RESULT]   char_per_word: 5.9298245614035086
2019-12-03 14:37:55.639963:   [RESULT]   Score on PET Limiting: 6.684791448820727
2019-12-03 14:37:55.640027:   [DONE  ]   Limiting PET Done
2019-12-03 14:37:55.640188:   [START ]   Start Reading FCE
2019-12-03 14:37:55.640300:   [START ]   Preprocess ./dataset/cambridge/FCE/1.txt Start
2019-12-03 14:37:55.722667:   [DONE  ]   Preprocess ./dataset/cambridge/FCE/1.txt Done
2019-12-03 14:37:55.722878:   [START ]   Preprocess ./dataset/cambridge/FCE/2.txt Start
2019-12-03 14:37:55.787583:   [DONE  ]   Preprocess ./dataset/cambridge/FCE/2.txt Done
2019-12-03 14:37:55.787795:   [START ]   Preprocess ./dataset/cambridge/FCE/3.txt Start
2019-12-03 14:37:55.866839:   [DONE  ]   Preprocess ./dataset/cambridge/FCE/3.txt Done
2019-12-03 14:37:55.867049:   [START ]   Preprocess ./dataset/cambridge/FCE/4.txt Start
2019-12-03 14:37:55.951772:   [DONE  ]   Preprocess ./dataset/cambridge/FCE/4.txt Done
2019-12-03 14:37:55.951971:   [START ]   Preprocess ./dataset/cambridge/FCE/5.txt Start
2019-12-03 14:37:56.025768:   [DONE  ]   Preprocess ./dataset/cambridge/FCE/5.txt Done
2019-12-03 14:37:56.025986:   [START ]   Preprocess ./dataset/cambridge/FCE/6.txt Start
2019-12-03 14:37:56.119016:   [DONE  ]   Preprocess ./dataset/cambridge/FCE/6.txt Done
2019-12-03 14:37:56.119236:   [START ]   Preprocess ./dataset/cambridge/FCE/7.txt Start
2019-12-03 14:37:56.199359:   [DONE  ]   Preprocess ./dataset/cambridge/FCE/7.txt Done
2019-12-03 14:37:56.200282:   [START ]   Preprocess ./dataset/cambridge/FCE/9.txt Start
2019-12-03 14:37:56.275997:   [DONE  ]   Preprocess ./dataset/cambridge/FCE/9.txt Done
2019-12-03 14:37:56.276211:   [START ]   Preprocess ./dataset/cambridge/FCE/10.txt Start
2019-12-03 14:37:56.354295:   [DONE  ]   Preprocess ./dataset/cambridge/FCE/10.txt Done
2019-12-03 14:37:56.354504:   [START ]   Preprocess ./dataset/cambridge/FCE/12.txt Start
2019-12-03 14:37:56.439163:   [DONE  ]   Preprocess ./dataset/cambridge/FCE/12.txt Done
2019-12-03 14:37:56.439379:   [START ]   Preprocess ./dataset/cambridge/FCE/13.txt Start
2019-12-03 14:37:56.514492:   [DONE  ]   Preprocess ./dataset/cambridge/FCE/13.txt Done
2019-12-03 14:37:56.514713:   [START ]   Preprocess ./dataset/cambridge/FCE/14.txt Start
2019-12-03 14:37:56.580862:   [DONE  ]   Preprocess ./dataset/cambridge/FCE/14.txt Done
2019-12-03 14:37:56.581005:   [DONE  ]   Read FCE Done
2019-12-03 14:37:56.581988:   [START ]   Limiting FCE Start
2019-12-03 14:37:56.582235:   [RESULT]   type_token_ratio: 0.34206896551724136
2019-12-03 14:37:56.582442:   [RESULT]   lexical_density: 0.34206896551724136
2019-12-03 14:37:56.582589:   [RESULT]   char_per_word: 5.919354838709677
2019-12-03 14:37:56.582698:   [RESULT]   Score on FCE Limiting: 6.60349276974416
2019-12-03 14:37:56.582993:   [RESULT]   type_token_ratio: 0.3769911504424779
2019-12-03 14:37:56.583086:   [RESULT]   lexical_density: 0.3769911504424779
2019-12-03 14:37:56.583335:   [RESULT]   char_per_word: 5.962441314553991
2019-12-03 14:37:56.583429:   [RESULT]   Score on FCE Limiting: 6.716423615438947
2019-12-03 14:37:56.583610:   [RESULT]   type_token_ratio: 0.36482084690553745
2019-12-03 14:37:56.583755:   [RESULT]   lexical_density: 0.36482084690553745
2019-12-03 14:37:56.583873:   [RESULT]   char_per_word: 6.401785714285714
2019-12-03 14:37:56.584047:   [RESULT]   Score on FCE Limiting: 7.13142740809679
2019-12-03 14:37:56.584167:   [RESULT]   type_token_ratio: 0.3472429210134128
2019-12-03 14:37:56.584302:   [RESULT]   lexical_density: 0.3472429210134128
2019-12-03 14:37:56.584540:   [RESULT]   char_per_word: 5.896995708154506
2019-12-03 14:37:56.584652:   [RESULT]   Score on FCE Limiting: 6.5914815501813315
2019-12-03 14:37:56.584825:   [RESULT]   type_token_ratio: 0.3643533123028391
2019-12-03 14:37:56.584916:   [RESULT]   lexical_density: 0.3643533123028391
2019-12-03 14:37:56.585018:   [RESULT]   char_per_word: 6.008658008658009
2019-12-03 14:37:56.585105:   [RESULT]   Score on FCE Limiting: 6.737364633263687
2019-12-03 14:37:56.585226:   [RESULT]   type_token_ratio: 0.31420765027322406
2019-12-03 14:37:56.585310:   [RESULT]   lexical_density: 0.31420765027322406
2019-12-03 14:37:56.585415:   [RESULT]   char_per_word: 5.773913043478261
2019-12-03 14:37:56.585495:   [RESULT]   Score on FCE Limiting: 6.40232834402471
2019-12-03 14:37:56.585743:   [RESULT]   type_token_ratio: 0.29839883551673946
2019-12-03 14:37:56.585911:   [RESULT]   lexical_density: 0.29839883551673946
2019-12-03 14:37:56.586013:   [RESULT]   char_per_word: 5.590243902439024
2019-12-03 14:37:56.586096:   [RESULT]   Score on FCE Limiting: 6.187041573472502
2019-12-03 14:37:56.586212:   [RESULT]   type_token_ratio: 0.38860971524288107
2019-12-03 14:37:56.586293:   [RESULT]   lexical_density: 0.38860971524288107
2019-12-03 14:37:56.586394:   [RESULT]   char_per_word: 6.120689655172414
2019-12-03 14:37:56.586474:   [RESULT]   Score on FCE Limiting: 6.897909085658175
2019-12-03 14:37:56.586593:   [RESULT]   type_token_ratio: 0.3584
2019-12-03 14:37:56.586746:   [RESULT]   lexical_density: 0.3584
2019-12-03 14:37:56.586845:   [RESULT]   char_per_word: 5.834821428571429
2019-12-03 14:37:56.586926:   [RESULT]   Score on FCE Limiting: 6.551621428571428
2019-12-03 14:37:56.587043:   [RESULT]   type_token_ratio: 0.363905325443787
2019-12-03 14:37:56.587124:   [RESULT]   lexical_density: 0.363905325443787
2019-12-03 14:37:56.587226:   [RESULT]   char_per_word: 6.2317073170731705
2019-12-03 14:37:56.587306:   [RESULT]   Score on FCE Limiting: 6.959517967960745
2019-12-03 14:37:56.587460:   [RESULT]   type_token_ratio: 0.36316695352839934
2019-12-03 14:37:56.587612:   [RESULT]   lexical_density: 0.36316695352839934
2019-12-03 14:37:56.587709:   [RESULT]   char_per_word: 6.312796208530806
2019-12-03 14:37:56.587833:   [RESULT]   Score on FCE Limiting: 7.039130115587604
2019-12-03 14:37:56.587941:   [RESULT]   type_token_ratio: 0.32685512367491165
2019-12-03 14:37:56.588019:   [RESULT]   lexical_density: 0.32685512367491165
2019-12-03 14:37:56.588177:   [RESULT]   char_per_word: 6.005405405405406
2019-12-03 14:37:56.588254:   [RESULT]   Score on FCE Limiting: 6.659115652755228
2019-12-03 14:37:56.588382:   [DONE  ]   Limiting FCE Done
2019-12-03 14:37:56.588468:   [START ]   Start Reading CAE
2019-12-03 14:37:56.588590:   [START ]   Preprocess ./dataset/cambridge/CAE/2.txt Start
2019-12-03 14:37:56.645708:   [DONE  ]   Preprocess ./dataset/cambridge/CAE/2.txt Done
2019-12-03 14:37:56.645897:   [START ]   Preprocess ./dataset/cambridge/CAE/3.txt Start
2019-12-03 14:37:56.753099:   [DONE  ]   Preprocess ./dataset/cambridge/CAE/3.txt Done
2019-12-03 14:37:56.753318:   [START ]   Preprocess ./dataset/cambridge/CAE/5.txt Start
2019-12-03 14:37:56.831471:   [DONE  ]   Preprocess ./dataset/cambridge/CAE/5.txt Done
2019-12-03 14:37:56.831685:   [START ]   Preprocess ./dataset/cambridge/CAE/7.txt Start
2019-12-03 14:37:56.936616:   [DONE  ]   Preprocess ./dataset/cambridge/CAE/7.txt Done
2019-12-03 14:37:56.936840:   [START ]   Preprocess ./dataset/cambridge/CAE/8.txt Start
2019-12-03 14:37:57.041083:   [DONE  ]   Preprocess ./dataset/cambridge/CAE/8.txt Done
2019-12-03 14:37:57.041305:   [START ]   Preprocess ./dataset/cambridge/CAE/9.txt Start
2019-12-03 14:37:57.153948:   [DONE  ]   Preprocess ./dataset/cambridge/CAE/9.txt Done
2019-12-03 14:37:57.154999:   [START ]   Preprocess ./dataset/cambridge/CAE/10.txt Start
2019-12-03 14:37:57.322590:   [DONE  ]   Preprocess ./dataset/cambridge/CAE/10.txt Done
2019-12-03 14:37:57.322841:   [START ]   Preprocess ./dataset/cambridge/CAE/11.txt Start
2019-12-03 14:37:57.397789:   [DONE  ]   Preprocess ./dataset/cambridge/CAE/11.txt Done
2019-12-03 14:37:57.398002:   [START ]   Preprocess ./dataset/cambridge/CAE/12.txt Start
2019-12-03 14:37:57.513736:   [DONE  ]   Preprocess ./dataset/cambridge/CAE/12.txt Done
2019-12-03 14:37:57.514629:   [START ]   Preprocess ./dataset/cambridge/CAE/13.txt Start
2019-12-03 14:37:57.620151:   [DONE  ]   Preprocess ./dataset/cambridge/CAE/13.txt Done
2019-12-03 14:37:57.620364:   [START ]   Preprocess ./dataset/cambridge/CAE/14.txt Start
2019-12-03 14:37:57.692265:   [DONE  ]   Preprocess ./dataset/cambridge/CAE/14.txt Done
2019-12-03 14:37:57.692496:   [START ]   Preprocess ./dataset/cambridge/CAE/15.txt Start
2019-12-03 14:37:57.761867:   [DONE  ]   Preprocess ./dataset/cambridge/CAE/15.txt Done
2019-12-03 14:37:57.762025:   [DONE  ]   Read CAE Done
2019-12-03 14:37:57.763036:   [START ]   Limiting CAE Start
2019-12-03 14:37:57.763256:   [RESULT]   type_token_ratio: 0.40040241448692154
2019-12-03 14:37:57.763395:   [RESULT]   lexical_density: 0.40040241448692154
2019-12-03 14:37:57.763545:   [RESULT]   char_per_word: 6.582914572864322
2019-12-03 14:37:57.763652:   [RESULT]   Score on CAE Limiting: 7.3837194018381656
2019-12-03 14:37:57.763914:   [RESULT]   type_token_ratio: 0.3070362473347548
2019-12-03 14:37:57.764095:   [RESULT]   lexical_density: 0.3070362473347548
2019-12-03 14:37:57.764223:   [RESULT]   char_per_word: 5.90625
2019-12-03 14:37:57.764318:   [RESULT]   Score on CAE Limiting: 6.520322494669509
2019-12-03 14:37:57.764665:   [RESULT]   type_token_ratio: 0.3851963746223565
2019-12-03 14:37:57.764770:   [RESULT]   lexical_density: 0.3851963746223565
2019-12-03 14:37:57.764978:   [RESULT]   char_per_word: 7.070588235294117
2019-12-03 14:37:57.765082:   [RESULT]   Score on CAE Limiting: 7.840980984538831
2019-12-03 14:37:57.765241:   [RESULT]   type_token_ratio: 0.28782707622298065
2019-12-03 14:37:57.765351:   [RESULT]   lexical_density: 0.28782707622298065
2019-12-03 14:37:57.765466:   [RESULT]   char_per_word: 6.529644268774703
2019-12-03 14:37:57.765803:   [RESULT]   Score on CAE Limiting: 7.105298421220665
2019-12-03 14:37:57.765977:   [RESULT]   type_token_ratio: 0.39934711643090315
2019-12-03 14:37:57.766107:   [RESULT]   lexical_density: 0.39934711643090315
2019-12-03 14:37:57.766246:   [RESULT]   char_per_word: 6.4959128065395095
2019-12-03 14:37:57.766347:   [RESULT]   Score on CAE Limiting: 7.294607039401315
2019-12-03 14:37:57.766640:   [RESULT]   type_token_ratio: 0.3162743091095189
2019-12-03 14:37:57.766974:   [RESULT]   lexical_density: 0.3162743091095189
2019-12-03 14:37:57.767256:   [RESULT]   char_per_word: 6.090614886731392
2019-12-03 14:37:57.767471:   [RESULT]   Score on CAE Limiting: 6.72316350495043
2019-12-03 14:37:57.767835:   [RESULT]   type_token_ratio: 0.36108821104699096
2019-12-03 14:37:57.768067:   [RESULT]   lexical_density: 0.36108821104699096
2019-12-03 14:37:57.768381:   [RESULT]   char_per_word: 5.970319634703197
2019-12-03 14:37:57.768621:   [RESULT]   Score on CAE Limiting: 6.692496056797179
2019-12-03 14:37:57.768854:   [RESULT]   type_token_ratio: 0.43853820598006643
2019-12-03 14:37:57.769132:   [RESULT]   lexical_density: 0.43853820598006643
2019-12-03 14:37:57.769392:   [RESULT]   char_per_word: 6.371212121212121
2019-12-03 14:37:57.769596:   [RESULT]   Score on CAE Limiting: 7.2482885331722535
2019-12-03 14:37:57.769945:   [RESULT]   type_token_ratio: 0.3727175080558539
2019-12-03 14:37:57.770119:   [RESULT]   lexical_density: 0.3727175080558539
2019-12-03 14:37:57.770341:   [RESULT]   char_per_word: 6.296829971181556
2019-12-03 14:37:57.770505:   [RESULT]   Score on CAE Limiting: 7.042264987293264
2019-12-03 14:37:57.770738:   [RESULT]   type_token_ratio: 0.3079625292740047
2019-12-03 14:37:57.770969:   [RESULT]   lexical_density: 0.3079625292740047
2019-12-03 14:37:57.771168:   [RESULT]   char_per_word: 6.064638783269962
2019-12-03 14:37:57.771325:   [RESULT]   Score on CAE Limiting: 6.680563841817972
2019-12-03 14:37:57.771525:   [RESULT]   type_token_ratio: 0.36879432624113473
2019-12-03 14:37:57.771677:   [RESULT]   lexical_density: 0.36879432624113473
2019-12-03 14:37:57.771864:   [RESULT]   char_per_word: 6.586538461538462
2019-12-03 14:37:57.772014:   [RESULT]   Score on CAE Limiting: 7.324127114020731
2019-12-03 14:37:57.772220:   [RESULT]   type_token_ratio: 0.36278195488721804
2019-12-03 14:37:57.772448:   [RESULT]   lexical_density: 0.36278195488721804
2019-12-03 14:37:57.772631:   [RESULT]   char_per_word: 6.27979274611399
2019-12-03 14:37:57.772782:   [RESULT]   Score on CAE Limiting: 7.005356655888426
2019-12-03 14:37:57.772913:   [DONE  ]   Limiting CAE Done
2019-12-03 14:37:57.773145:   [START ]   Start Reading CPE
2019-12-03 14:37:57.773410:   [START ]   Preprocess ./dataset/cambridge/CPE/1.txt Start
2019-12-03 14:37:57.833332:   [DONE  ]   Preprocess ./dataset/cambridge/CPE/1.txt Done
2019-12-03 14:37:57.833552:   [START ]   Preprocess ./dataset/cambridge/CPE/2.txt Start
2019-12-03 14:37:57.938740:   [DONE  ]   Preprocess ./dataset/cambridge/CPE/2.txt Done
2019-12-03 14:37:57.938952:   [START ]   Preprocess ./dataset/cambridge/CPE/3.txt Start
2019-12-03 14:37:58.059502:   [DONE  ]   Preprocess ./dataset/cambridge/CPE/3.txt Done
2019-12-03 14:37:58.059719:   [START ]   Preprocess ./dataset/cambridge/CPE/5.txt Start
2019-12-03 14:37:58.111911:   [DONE  ]   Preprocess ./dataset/cambridge/CPE/5.txt Done
2019-12-03 14:37:58.112131:   [START ]   Preprocess ./dataset/cambridge/CPE/8.txt Start
2019-12-03 14:37:58.209376:   [DONE  ]   Preprocess ./dataset/cambridge/CPE/8.txt Done
2019-12-03 14:37:58.209593:   [START ]   Preprocess ./dataset/cambridge/CPE/9.txt Start
2019-12-03 14:37:58.263489:   [DONE  ]   Preprocess ./dataset/cambridge/CPE/9.txt Done
2019-12-03 14:37:58.264374:   [START ]   Preprocess ./dataset/cambridge/CPE/10.txt Start
2019-12-03 14:37:58.370759:   [DONE  ]   Preprocess ./dataset/cambridge/CPE/10.txt Done
2019-12-03 14:37:58.370979:   [START ]   Preprocess ./dataset/cambridge/CPE/11.txt Start
2019-12-03 14:37:58.483125:   [DONE  ]   Preprocess ./dataset/cambridge/CPE/11.txt Done
2019-12-03 14:37:58.483999:   [START ]   Preprocess ./dataset/cambridge/CPE/12.txt Start
2019-12-03 14:37:58.585783:   [DONE  ]   Preprocess ./dataset/cambridge/CPE/12.txt Done
2019-12-03 14:37:58.586001:   [START ]   Preprocess ./dataset/cambridge/CPE/13.txt Start
2019-12-03 14:37:58.639290:   [DONE  ]   Preprocess ./dataset/cambridge/CPE/13.txt Done
2019-12-03 14:37:58.639495:   [START ]   Preprocess ./dataset/cambridge/CPE/14.txt Start
2019-12-03 14:37:58.749249:   [DONE  ]   Preprocess ./dataset/cambridge/CPE/14.txt Done
2019-12-03 14:37:58.750624:   [START ]   Preprocess ./dataset/cambridge/CPE/15.txt Start
2019-12-03 14:37:58.848638:   [DONE  ]   Preprocess ./dataset/cambridge/CPE/15.txt Done
2019-12-03 14:37:58.848784:   [DONE  ]   Read CPE Done
2019-12-03 14:37:58.850504:   [START ]   Limiting CPE Start
2019-12-03 14:37:58.850853:   [RESULT]   type_token_ratio: 0.42857142857142855
2019-12-03 14:37:58.850966:   [RESULT]   lexical_density: 0.42857142857142855
2019-12-03 14:37:58.851121:   [RESULT]   char_per_word: 6.173913043478261
2019-12-03 14:37:58.851231:   [RESULT]   Score on CPE Limiting: 7.031055900621118
2019-12-03 14:37:58.851412:   [RESULT]   type_token_ratio: 0.4033018867924528
2019-12-03 14:37:58.851527:   [RESULT]   lexical_density: 0.4033018867924528
2019-12-03 14:37:58.851789:   [RESULT]   char_per_word: 6.637426900584796
2019-12-03 14:37:58.852096:   [RESULT]   Score on CPE Limiting: 7.444030674169701
2019-12-03 14:37:58.852477:   [RESULT]   type_token_ratio: 0.3888888888888889
2019-12-03 14:37:58.852703:   [RESULT]   lexical_density: 0.3888888888888889
2019-12-03 14:37:58.853009:   [RESULT]   char_per_word: 6.466165413533835
2019-12-03 14:37:58.853229:   [RESULT]   Score on CPE Limiting: 7.2439431913116135
2019-12-03 14:37:58.853476:   [RESULT]   type_token_ratio: 0.4330357142857143
2019-12-03 14:37:58.853674:   [RESULT]   lexical_density: 0.4330357142857143
2019-12-03 14:37:58.853916:   [RESULT]   char_per_word: 6.283505154639175
2019-12-03 14:37:58.854148:   [RESULT]   Score on CPE Limiting: 7.149576583210604
2019-12-03 14:37:58.854400:   [RESULT]   type_token_ratio: 0.27471116816431324
2019-12-03 14:37:58.854603:   [RESULT]   lexical_density: 0.27471116816431324
2019-12-03 14:37:58.854849:   [RESULT]   char_per_word: 6.242990654205608
2019-12-03 14:37:58.855049:   [RESULT]   Score on CPE Limiting: 6.792412990534235
2019-12-03 14:37:58.855304:   [RESULT]   type_token_ratio: 0.4012875536480687
2019-12-03 14:37:58.855489:   [RESULT]   lexical_density: 0.4012875536480687
2019-12-03 14:37:58.855673:   [RESULT]   char_per_word: 6.106951871657754
2019-12-03 14:37:58.855896:   [RESULT]   Score on CPE Limiting: 6.909526978953892
2019-12-03 14:37:58.856142:   [RESULT]   type_token_ratio: 0.43367935409457903
2019-12-03 14:37:58.856309:   [RESULT]   lexical_density: 0.43367935409457903
2019-12-03 14:37:58.856538:   [RESULT]   char_per_word: 6.473404255319149
2019-12-03 14:37:58.856705:   [RESULT]   Score on CPE Limiting: 7.340762963508308
2019-12-03 14:37:58.856960:   [RESULT]   type_token_ratio: 0.38571428571428573
2019-12-03 14:37:58.857130:   [RESULT]   lexical_density: 0.38571428571428573
2019-12-03 14:37:58.857353:   [RESULT]   char_per_word: 6.64957264957265
2019-12-03 14:37:58.857593:   [RESULT]   Score on CPE Limiting: 7.421001221001222
2019-12-03 14:37:58.857844:   [RESULT]   type_token_ratio: 0.35527809307604996
2019-12-03 14:37:58.858019:   [RESULT]   lexical_density: 0.35527809307604996
2019-12-03 14:37:58.858234:   [RESULT]   char_per_word: 7.156549520766773
2019-12-03 14:37:58.858398:   [RESULT]   Score on CPE Limiting: 7.867105706918873
2019-12-03 14:37:58.858597:   [RESULT]   type_token_ratio: 0.4451901565995526
2019-12-03 14:37:58.858801:   [RESULT]   lexical_density: 0.4451901565995526
2019-12-03 14:37:58.859049:   [RESULT]   char_per_word: 7.015075376884422
2019-12-03 14:37:58.859254:   [RESULT]   Score on CPE Limiting: 7.905455690083526
2019-12-03 14:37:58.859487:   [RESULT]   type_token_ratio: 0.35353535353535354
2019-12-03 14:37:58.859649:   [RESULT]   lexical_density: 0.35353535353535354
2019-12-03 14:37:58.859822:   [RESULT]   char_per_word: 6.095238095238095
2019-12-03 14:37:58.859954:   [RESULT]   Score on CPE Limiting: 6.8023088023088025
2019-12-03 14:37:58.860230:   [RESULT]   type_token_ratio: 0.33623910336239105
2019-12-03 14:37:58.860382:   [RESULT]   lexical_density: 0.33623910336239105
2019-12-03 14:37:58.860609:   [RESULT]   char_per_word: 6.555555555555555
2019-12-03 14:37:58.860824:   [RESULT]   Score on CPE Limiting: 7.228033762280338
2019-12-03 14:37:58.860936:   [DONE  ]   Limiting CPE Done
2019-12-03 14:37:58.861045:   [DONE  ]   Traning data Done
2019-12-03 14:37:58.861149:   [START ]   Test Start
2019-12-03 14:37:58.861258:   [START ]   Reading Test ./dataset/cambridge/KET/8.txt Start
2019-12-03 14:37:58.861423:   [START ]   Preprocess ./dataset/cambridge/KET/8.txt Start
2019-12-03 14:37:58.878594:   [DONE  ]   Preprocess ./dataset/cambridge/KET/8.txt Done
2019-12-03 14:37:58.878688:   [DONE  ]   Reading Test ./dataset/cambridge/KET/8.txt Done
2019-12-03 14:37:58.879144:   [START ]   Scoring KET Start
2019-12-03 14:42:55.568278:   [INFO  ]   -----------------------------------------------------------------------------
2019-12-03 14:42:55.568458:   [INFO  ]   Hello world!
2019-12-03 14:42:55.569412:   [INFO  ]   Start main processing!
2019-12-03 14:42:55.569948:   [START ]   Start Prepare Traning data
2019-12-03 14:42:55.570166:   [START ]   Initialize parameters Start
2019-12-03 14:42:55.570308:   [DONE  ]   ----------Initialize parameters Done----------
2019-12-03 14:42:55.570577:   [START ]   Start setup FOLD
2019-12-03 14:42:55.570786:   [START ]   Setup FOLD Done
2019-12-03 14:42:55.570922:   [START ]   Start Training data
2019-12-03 14:42:55.571077:   [START ]   Start Reading KET
2019-12-03 14:42:55.571233:   [START ]   Preprocess ./dataset/cambridge/KET/1.txt Start
2019-12-03 14:42:56.951928:   [DONE  ]   ----------Preprocess ./dataset/cambridge/KET/1.txt Done----------
2019-12-03 14:42:56.952300:   [START ]   Preprocess ./dataset/cambridge/KET/3.txt Start
2019-12-03 14:42:56.970435:   [DONE  ]   ----------Preprocess ./dataset/cambridge/KET/3.txt Done----------
2019-12-03 14:42:56.970691:   [START ]   Preprocess ./dataset/cambridge/KET/4.txt Start
2019-12-03 14:42:56.995926:   [DONE  ]   ----------Preprocess ./dataset/cambridge/KET/4.txt Done----------
2019-12-03 14:42:56.996069:   [START ]   Preprocess ./dataset/cambridge/KET/5.txt Start
2019-12-03 14:42:57.017948:   [DONE  ]   ----------Preprocess ./dataset/cambridge/KET/5.txt Done----------
2019-12-03 14:42:57.018160:   [START ]   Preprocess ./dataset/cambridge/KET/6.txt Start
2019-12-03 14:42:57.031404:   [DONE  ]   ----------Preprocess ./dataset/cambridge/KET/6.txt Done----------
2019-12-03 14:42:57.031556:   [START ]   Preprocess ./dataset/cambridge/KET/7.txt Start
2019-12-03 14:42:57.053767:   [DONE  ]   ----------Preprocess ./dataset/cambridge/KET/7.txt Done----------
2019-12-03 14:42:57.053925:   [START ]   Preprocess ./dataset/cambridge/KET/8.txt Start
2019-12-03 14:42:57.068428:   [DONE  ]   ----------Preprocess ./dataset/cambridge/KET/8.txt Done----------
2019-12-03 14:42:57.068587:   [START ]   Preprocess ./dataset/cambridge/KET/11.txt Start
2019-12-03 14:42:57.082142:   [DONE  ]   ----------Preprocess ./dataset/cambridge/KET/11.txt Done----------
2019-12-03 14:42:57.082301:   [START ]   Preprocess ./dataset/cambridge/KET/12.txt Start
2019-12-03 14:42:57.094599:   [DONE  ]   ----------Preprocess ./dataset/cambridge/KET/12.txt Done----------
2019-12-03 14:42:57.094742:   [START ]   Preprocess ./dataset/cambridge/KET/13.txt Start
2019-12-03 14:42:57.117310:   [DONE  ]   ----------Preprocess ./dataset/cambridge/KET/13.txt Done----------
2019-12-03 14:42:57.117459:   [START ]   Preprocess ./dataset/cambridge/KET/14.txt Start
2019-12-03 14:42:57.131051:   [DONE  ]   ----------Preprocess ./dataset/cambridge/KET/14.txt Done----------
2019-12-03 14:42:57.131260:   [START ]   Preprocess ./dataset/cambridge/KET/15.txt Start
2019-12-03 14:42:57.143831:   [DONE  ]   ----------Preprocess ./dataset/cambridge/KET/15.txt Done----------
2019-12-03 14:42:57.143970:   [DONE  ]   ----------Read KET Done----------
2019-12-03 14:42:57.144348:   [START ]   Limiting KET Start
2019-12-03 14:43:33.877714:   [INFO  ]   -----------------------------------------------------------------------------
2019-12-03 14:43:33.877975:   [INFO  ]   Hello world!
2019-12-03 14:43:33.878589:   [INFO  ]   Start main processing!
2019-12-03 14:43:33.878678:   [START ]   Start Prepare Traning data
2019-12-03 14:43:33.878984:   [START ]   Initialize parameters Start
2019-12-03 14:43:33.879103:   [DONE  ]   ----------Initialize parameters Done----------
2019-12-03 14:43:33.879284:   [START ]   Start setup FOLD
2019-12-03 14:43:33.879442:   [START ]   Setup FOLD Done
2019-12-03 14:43:33.879515:   [START ]   Start Training data
2019-12-03 14:43:33.879668:   [START ]   Start Reading KET
2019-12-03 14:43:33.879844:   [START ]   Preprocess ./dataset/cambridge/KET/1.txt Start
2019-12-03 14:43:35.282992:   [DONE  ]   ----------Preprocess ./dataset/cambridge/KET/1.txt Done----------
2019-12-03 14:43:35.283278:   [START ]   Preprocess ./dataset/cambridge/KET/2.txt Start
2019-12-03 14:43:35.306396:   [DONE  ]   ----------Preprocess ./dataset/cambridge/KET/2.txt Done----------
2019-12-03 14:43:35.306652:   [START ]   Preprocess ./dataset/cambridge/KET/3.txt Start
2019-12-03 14:43:35.321080:   [DONE  ]   ----------Preprocess ./dataset/cambridge/KET/3.txt Done----------
2019-12-03 14:43:35.321308:   [START ]   Preprocess ./dataset/cambridge/KET/4.txt Start
2019-12-03 14:43:35.342957:   [DONE  ]   ----------Preprocess ./dataset/cambridge/KET/4.txt Done----------
2019-12-03 14:43:35.343123:   [START ]   Preprocess ./dataset/cambridge/KET/5.txt Start
2019-12-03 14:43:35.356515:   [DONE  ]   ----------Preprocess ./dataset/cambridge/KET/5.txt Done----------
2019-12-03 14:43:35.356659:   [START ]   Preprocess ./dataset/cambridge/KET/7.txt Start
2019-12-03 14:43:35.379329:   [DONE  ]   ----------Preprocess ./dataset/cambridge/KET/7.txt Done----------
2019-12-03 14:43:35.379474:   [START ]   Preprocess ./dataset/cambridge/KET/8.txt Start
2019-12-03 14:43:35.393697:   [DONE  ]   ----------Preprocess ./dataset/cambridge/KET/8.txt Done----------
2019-12-03 14:43:35.393834:   [START ]   Preprocess ./dataset/cambridge/KET/9.txt Start
2019-12-03 14:43:35.407198:   [DONE  ]   ----------Preprocess ./dataset/cambridge/KET/9.txt Done----------
2019-12-03 14:43:35.407344:   [START ]   Preprocess ./dataset/cambridge/KET/10.txt Start
2019-12-03 14:43:35.429741:   [DONE  ]   ----------Preprocess ./dataset/cambridge/KET/10.txt Done----------
2019-12-03 14:43:35.429923:   [START ]   Preprocess ./dataset/cambridge/KET/11.txt Start
2019-12-03 14:43:35.443697:   [DONE  ]   ----------Preprocess ./dataset/cambridge/KET/11.txt Done----------
2019-12-03 14:43:35.443853:   [START ]   Preprocess ./dataset/cambridge/KET/12.txt Start
2019-12-03 14:43:35.456513:   [DONE  ]   ----------Preprocess ./dataset/cambridge/KET/12.txt Done----------
2019-12-03 14:43:35.456693:   [START ]   Preprocess ./dataset/cambridge/KET/13.txt Start
2019-12-03 14:43:35.479494:   [DONE  ]   ----------Preprocess ./dataset/cambridge/KET/13.txt Done----------
2019-12-03 14:43:35.479586:   [DONE  ]   ----------Read KET Done----------
2019-12-03 14:43:35.479875:   [START ]   Limiting KET Start
2019-12-03 14:43:35.479984:   [RESULT]   type_token_ratio: 0.4
2019-12-03 14:43:35.480125:   [RESULT]   lexical_density: 0.4
2019-12-03 14:43:35.480252:   [RESULT]   char_per_word: 5.355263157894737
2019-12-03 14:43:35.480367:   [RESULT]   Score on KET Limiting: 6.155263157894738
2019-12-03 14:43:35.480651:   [RESULT]   type_token_ratio: 0.35294117647058826
2019-12-03 14:43:35.480744:   [RESULT]   lexical_density: 0.35294117647058826
2019-12-03 14:43:35.480867:   [RESULT]   char_per_word: 4.880952380952381
2019-12-03 14:43:35.480983:   [RESULT]   Score on KET Limiting: 5.586834733893557
2019-12-03 14:43:35.481106:   [RESULT]   type_token_ratio: 0.5204081632653061
2019-12-03 14:43:35.481218:   [RESULT]   lexical_density: 0.5204081632653061
2019-12-03 14:43:35.481299:   [RESULT]   char_per_word: 4.882352941176471
2019-12-03 14:43:35.481374:   [RESULT]   Score on KET Limiting: 5.923169267707083
2019-12-03 14:43:35.481602:   [RESULT]   type_token_ratio: 0.3193717277486911
2019-12-03 14:43:35.481832:   [RESULT]   lexical_density: 0.3193717277486911
2019-12-03 14:43:35.481917:   [RESULT]   char_per_word: 5.262295081967213
2019-12-03 14:43:35.482094:   [RESULT]   Score on KET Limiting: 5.901038537464595
2019-12-03 14:43:35.482178:   [RESULT]   type_token_ratio: 0.42016806722689076
2019-12-03 14:43:35.482252:   [RESULT]   lexical_density: 0.42016806722689076
2019-12-03 14:43:35.482376:   [RESULT]   char_per_word: 5.18
2019-12-03 14:43:35.482521:   [RESULT]   Score on KET Limiting: 6.020336134453782
2019-12-03 14:43:35.482630:   [RESULT]   type_token_ratio: 0.38860103626943004
2019-12-03 14:43:35.482758:   [RESULT]   lexical_density: 0.38860103626943004
2019-12-03 14:43:35.482837:   [RESULT]   char_per_word: 5.56
2019-12-03 14:43:35.482910:   [RESULT]   Score on KET Limiting: 6.337202072538859
2019-12-03 14:43:35.483036:   [RESULT]   type_token_ratio: 0.4186046511627907
2019-12-03 14:43:35.483108:   [RESULT]   lexical_density: 0.4186046511627907
2019-12-03 14:43:35.483186:   [RESULT]   char_per_word: 4.796296296296297
2019-12-03 14:43:35.483259:   [RESULT]   Score on KET Limiting: 5.633505598621878
2019-12-03 14:43:35.483435:   [RESULT]   type_token_ratio: 0.4074074074074074
2019-12-03 14:43:35.483580:   [RESULT]   lexical_density: 0.4074074074074074
2019-12-03 14:43:35.483656:   [RESULT]   char_per_word: 5.2727272727272725
2019-12-03 14:43:35.483797:   [RESULT]   Score on KET Limiting: 6.087542087542087
2019-12-03 14:43:35.483885:   [RESULT]   type_token_ratio: 0.35175879396984927
2019-12-03 14:43:35.483980:   [RESULT]   lexical_density: 0.35175879396984927
2019-12-03 14:43:35.484061:   [RESULT]   char_per_word: 5.271428571428571
2019-12-03 14:43:35.484162:   [RESULT]   Score on KET Limiting: 5.97494615936827
2019-12-03 14:43:35.484245:   [RESULT]   type_token_ratio: 0.31932773109243695
2019-12-03 14:43:35.484346:   [RESULT]   lexical_density: 0.31932773109243695
2019-12-03 14:43:35.484423:   [RESULT]   char_per_word: 5.526315789473684
2019-12-03 14:43:35.484527:   [RESULT]   Score on KET Limiting: 6.1649712516585575
2019-12-03 14:43:35.484609:   [RESULT]   type_token_ratio: 0.3125
2019-12-03 14:43:35.484708:   [RESULT]   lexical_density: 0.3125
2019-12-03 14:43:35.484784:   [RESULT]   char_per_word: 4.685714285714286
2019-12-03 14:43:35.484888:   [RESULT]   Score on KET Limiting: 5.310714285714286
2019-12-03 14:43:35.484975:   [RESULT]   type_token_ratio: 0.2914572864321608
2019-12-03 14:43:35.485074:   [RESULT]   lexical_density: 0.2914572864321608
2019-12-03 14:43:35.485154:   [RESULT]   char_per_word: 5.155172413793103
2019-12-03 14:43:35.485307:   [RESULT]   Score on KET Limiting: 5.738086986657425
2019-12-03 14:43:35.485409:   [DONE  ]   ----------Limiting KET Done----------
2019-12-03 14:43:35.485481:   [START ]   Start Reading PET
2019-12-03 14:43:35.485624:   [START ]   Preprocess ./dataset/cambridge/PET/1.txt Start
2019-12-03 14:43:35.524968:   [DONE  ]   ----------Preprocess ./dataset/cambridge/PET/1.txt Done----------
2019-12-03 14:43:35.525175:   [START ]   Preprocess ./dataset/cambridge/PET/2.txt Start
2019-12-03 14:43:35.549502:   [DONE  ]   ----------Preprocess ./dataset/cambridge/PET/2.txt Done----------
2019-12-03 14:43:35.549933:   [START ]   Preprocess ./dataset/cambridge/PET/3.txt Start
2019-12-03 14:43:35.567783:   [DONE  ]   ----------Preprocess ./dataset/cambridge/PET/3.txt Done----------
2019-12-03 14:43:35.568000:   [START ]   Preprocess ./dataset/cambridge/PET/5.txt Start
2019-12-03 14:43:35.593006:   [DONE  ]   ----------Preprocess ./dataset/cambridge/PET/5.txt Done----------
2019-12-03 14:43:35.593156:   [START ]   Preprocess ./dataset/cambridge/PET/7.txt Start
2019-12-03 14:43:35.644286:   [DONE  ]   ----------Preprocess ./dataset/cambridge/PET/7.txt Done----------
2019-12-03 14:43:35.644474:   [START ]   Preprocess ./dataset/cambridge/PET/8.txt Start
2019-12-03 14:43:35.680108:   [DONE  ]   ----------Preprocess ./dataset/cambridge/PET/8.txt Done----------
2019-12-03 14:43:35.680313:   [START ]   Preprocess ./dataset/cambridge/PET/9.txt Start
2019-12-03 14:43:35.697806:   [DONE  ]   ----------Preprocess ./dataset/cambridge/PET/9.txt Done----------
2019-12-03 14:43:35.697954:   [START ]   Preprocess ./dataset/cambridge/PET/10.txt Start
2019-12-03 14:43:35.748774:   [DONE  ]   ----------Preprocess ./dataset/cambridge/PET/10.txt Done----------
2019-12-03 14:43:35.748959:   [START ]   Preprocess ./dataset/cambridge/PET/11.txt Start
2019-12-03 14:43:35.774688:   [DONE  ]   ----------Preprocess ./dataset/cambridge/PET/11.txt Done----------
2019-12-03 14:43:35.774888:   [START ]   Preprocess ./dataset/cambridge/PET/12.txt Start
2019-12-03 14:43:35.790235:   [DONE  ]   ----------Preprocess ./dataset/cambridge/PET/12.txt Done----------
2019-12-03 14:43:35.790379:   [START ]   Preprocess ./dataset/cambridge/PET/14.txt Start
2019-12-03 14:43:35.822114:   [DONE  ]   ----------Preprocess ./dataset/cambridge/PET/14.txt Done----------
2019-12-03 14:43:35.822285:   [START ]   Preprocess ./dataset/cambridge/PET/15.txt Start
2019-12-03 14:43:35.839530:   [DONE  ]   ----------Preprocess ./dataset/cambridge/PET/15.txt Done----------
2019-12-03 14:43:35.839630:   [DONE  ]   ----------Read PET Done----------
2019-12-03 14:43:35.840152:   [START ]   Limiting PET Start
2019-12-03 14:43:35.840463:   [RESULT]   type_token_ratio: 0.40878378378378377
2019-12-03 14:43:35.840625:   [RESULT]   lexical_density: 0.40878378378378377
2019-12-03 14:43:35.840755:   [RESULT]   char_per_word: 6.231404958677686
2019-12-03 14:43:35.840875:   [RESULT]   Score on PET Limiting: 7.0489725262452545
2019-12-03 14:43:35.841001:   [RESULT]   type_token_ratio: 0.40096618357487923
2019-12-03 14:43:35.841334:   [RESULT]   lexical_density: 0.40096618357487923
2019-12-03 14:43:35.841429:   [RESULT]   char_per_word: 5.674698795180723
2019-12-03 14:43:35.841681:   [RESULT]   Score on PET Limiting: 6.476631162330482
2019-12-03 14:43:35.841799:   [RESULT]   type_token_ratio: 0.423841059602649
2019-12-03 14:43:35.841931:   [RESULT]   lexical_density: 0.423841059602649
2019-12-03 14:43:35.842052:   [RESULT]   char_per_word: 6.375
2019-12-03 14:43:35.842164:   [RESULT]   Score on PET Limiting: 7.222682119205299
2019-12-03 14:43:35.842256:   [RESULT]   type_token_ratio: 0.39545454545454545
2019-12-03 14:43:35.842333:   [RESULT]   lexical_density: 0.39545454545454545
2019-12-03 14:43:35.842421:   [RESULT]   char_per_word: 5.310344827586207
2019-12-03 14:43:35.842498:   [RESULT]   Score on PET Limiting: 6.101253918495298
2019-12-03 14:43:35.842754:   [RESULT]   type_token_ratio: 0.3906633906633907
2019-12-03 14:43:35.842850:   [RESULT]   lexical_density: 0.3906633906633907
2019-12-03 14:43:35.842948:   [RESULT]   char_per_word: 6.062893081761007
2019-12-03 14:43:35.843072:   [RESULT]   Score on PET Limiting: 6.844219863087788
2019-12-03 14:43:35.843162:   [RESULT]   type_token_ratio: 0.3177570093457944
2019-12-03 14:43:35.843236:   [RESULT]   lexical_density: 0.3177570093457944
2019-12-03 14:43:35.843315:   [RESULT]   char_per_word: 5.573529411764706
2019-12-03 14:43:35.843443:   [RESULT]   Score on PET Limiting: 6.209043430456295
2019-12-03 14:43:35.843526:   [RESULT]   type_token_ratio: 0.4090909090909091
2019-12-03 14:43:35.843695:   [RESULT]   lexical_density: 0.4090909090909091
2019-12-03 14:43:35.843777:   [RESULT]   char_per_word: 5.4603174603174605
2019-12-03 14:43:35.843851:   [RESULT]   Score on PET Limiting: 6.278499278499279
2019-12-03 14:43:35.844054:   [RESULT]   type_token_ratio: 0.38913043478260867
2019-12-03 14:43:35.844133:   [RESULT]   lexical_density: 0.38913043478260867
2019-12-03 14:43:35.844289:   [RESULT]   char_per_word: 6.150837988826815
2019-12-03 14:43:35.844366:   [RESULT]   Score on PET Limiting: 6.929098858392033
2019-12-03 14:43:35.844543:   [RESULT]   type_token_ratio: 0.4782608695652174
2019-12-03 14:43:35.844706:   [RESULT]   lexical_density: 0.4782608695652174
2019-12-03 14:43:35.844788:   [RESULT]   char_per_word: 6.113636363636363
2019-12-03 14:43:35.844923:   [RESULT]   Score on PET Limiting: 7.070158102766799
2019-12-03 14:43:35.845005:   [RESULT]   type_token_ratio: 0.39552238805970147
2019-12-03 14:43:35.845137:   [RESULT]   lexical_density: 0.39552238805970147
2019-12-03 14:43:35.845214:   [RESULT]   char_per_word: 5.90566037735849
2019-12-03 14:43:35.845317:   [RESULT]   Score on PET Limiting: 6.696705153477893
2019-12-03 14:43:35.845404:   [RESULT]   type_token_ratio: 0.39215686274509803
2019-12-03 14:43:35.845501:   [RESULT]   lexical_density: 0.39215686274509803
2019-12-03 14:43:35.845582:   [RESULT]   char_per_word: 5.9625
2019-12-03 14:43:35.845719:   [RESULT]   Score on PET Limiting: 6.746813725490197
2019-12-03 14:43:35.845800:   [RESULT]   type_token_ratio: 0.37748344370860926
2019-12-03 14:43:35.845933:   [RESULT]   lexical_density: 0.37748344370860926
2019-12-03 14:43:35.846010:   [RESULT]   char_per_word: 5.9298245614035086
2019-12-03 14:43:35.846144:   [RESULT]   Score on PET Limiting: 6.684791448820727
2019-12-03 14:43:35.846211:   [DONE  ]   ----------Limiting PET Done----------
2019-12-03 14:43:35.846441:   [START ]   Start Reading FCE
2019-12-03 14:43:35.846549:   [START ]   Preprocess ./dataset/cambridge/FCE/1.txt Start
2019-12-03 14:43:35.928231:   [DONE  ]   ----------Preprocess ./dataset/cambridge/FCE/1.txt Done----------
2019-12-03 14:43:35.928430:   [START ]   Preprocess ./dataset/cambridge/FCE/2.txt Start
2019-12-03 14:43:36.000398:   [DONE  ]   ----------Preprocess ./dataset/cambridge/FCE/2.txt Done----------
2019-12-03 14:43:36.000604:   [START ]   Preprocess ./dataset/cambridge/FCE/3.txt Start
2019-12-03 14:43:36.071789:   [DONE  ]   ----------Preprocess ./dataset/cambridge/FCE/3.txt Done----------
2019-12-03 14:43:36.072008:   [START ]   Preprocess ./dataset/cambridge/FCE/4.txt Start
2019-12-03 14:43:36.146807:   [DONE  ]   ----------Preprocess ./dataset/cambridge/FCE/4.txt Done----------
2019-12-03 14:43:36.147014:   [START ]   Preprocess ./dataset/cambridge/FCE/5.txt Start
2019-12-03 14:43:36.218872:   [DONE  ]   ----------Preprocess ./dataset/cambridge/FCE/5.txt Done----------
2019-12-03 14:43:36.219079:   [START ]   Preprocess ./dataset/cambridge/FCE/7.txt Start
2019-12-03 14:43:36.297056:   [DONE  ]   ----------Preprocess ./dataset/cambridge/FCE/7.txt Done----------
2019-12-03 14:43:36.297266:   [START ]   Preprocess ./dataset/cambridge/FCE/8.txt Start
2019-12-03 14:43:36.375601:   [DONE  ]   ----------Preprocess ./dataset/cambridge/FCE/8.txt Done----------
2019-12-03 14:43:36.375816:   [START ]   Preprocess ./dataset/cambridge/FCE/9.txt Start
2019-12-03 14:43:36.452366:   [DONE  ]   ----------Preprocess ./dataset/cambridge/FCE/9.txt Done----------
2019-12-03 14:43:36.452562:   [START ]   Preprocess ./dataset/cambridge/FCE/10.txt Start
2019-12-03 14:43:36.524622:   [DONE  ]   ----------Preprocess ./dataset/cambridge/FCE/10.txt Done----------
2019-12-03 14:43:36.524828:   [START ]   Preprocess ./dataset/cambridge/FCE/11.txt Start
2019-12-03 14:43:36.598314:   [DONE  ]   ----------Preprocess ./dataset/cambridge/FCE/11.txt Done----------
2019-12-03 14:43:36.598522:   [START ]   Preprocess ./dataset/cambridge/FCE/13.txt Start
2019-12-03 14:43:36.676452:   [DONE  ]   ----------Preprocess ./dataset/cambridge/FCE/13.txt Done----------
2019-12-03 14:43:36.676672:   [START ]   Preprocess ./dataset/cambridge/FCE/14.txt Start
2019-12-03 14:43:36.744341:   [DONE  ]   ----------Preprocess ./dataset/cambridge/FCE/14.txt Done----------
2019-12-03 14:43:36.744481:   [DONE  ]   ----------Read FCE Done----------
2019-12-03 14:43:36.745378:   [START ]   Limiting FCE Start
2019-12-03 14:43:36.745606:   [RESULT]   type_token_ratio: 0.34206896551724136
2019-12-03 14:43:36.745748:   [RESULT]   lexical_density: 0.34206896551724136
2019-12-03 14:43:36.745909:   [RESULT]   char_per_word: 5.919354838709677
2019-12-03 14:43:36.746039:   [RESULT]   Score on FCE Limiting: 6.60349276974416
2019-12-03 14:43:36.746195:   [RESULT]   type_token_ratio: 0.3769911504424779
2019-12-03 14:43:36.746315:   [RESULT]   lexical_density: 0.3769911504424779
2019-12-03 14:43:36.746429:   [RESULT]   char_per_word: 5.962441314553991
2019-12-03 14:43:36.746521:   [RESULT]   Score on FCE Limiting: 6.716423615438947
2019-12-03 14:43:36.746655:   [RESULT]   type_token_ratio: 0.36482084690553745
2019-12-03 14:43:36.746747:   [RESULT]   lexical_density: 0.36482084690553745
2019-12-03 14:43:36.746862:   [RESULT]   char_per_word: 6.401785714285714
2019-12-03 14:43:36.746951:   [RESULT]   Score on FCE Limiting: 7.13142740809679
2019-12-03 14:43:36.747262:   [RESULT]   type_token_ratio: 0.3472429210134128
2019-12-03 14:43:36.747417:   [RESULT]   lexical_density: 0.3472429210134128
2019-12-03 14:43:36.747533:   [RESULT]   char_per_word: 5.896995708154506
2019-12-03 14:43:36.747626:   [RESULT]   Score on FCE Limiting: 6.5914815501813315
2019-12-03 14:43:36.747758:   [RESULT]   type_token_ratio: 0.3643533123028391
2019-12-03 14:43:36.747850:   [RESULT]   lexical_density: 0.3643533123028391
2019-12-03 14:43:36.747961:   [RESULT]   char_per_word: 6.008658008658009
2019-12-03 14:43:36.748113:   [RESULT]   Score on FCE Limiting: 6.737364633263687
2019-12-03 14:43:36.748259:   [RESULT]   type_token_ratio: 0.29839883551673946
2019-12-03 14:43:36.748431:   [RESULT]   lexical_density: 0.29839883551673946
2019-12-03 14:43:36.748544:   [RESULT]   char_per_word: 5.590243902439024
2019-12-03 14:43:36.748641:   [RESULT]   Score on FCE Limiting: 6.187041573472502
2019-12-03 14:43:36.748773:   [RESULT]   type_token_ratio: 0.3258064516129032
2019-12-03 14:43:36.748905:   [RESULT]   lexical_density: 0.3258064516129032
2019-12-03 14:43:36.749050:   [RESULT]   char_per_word: 6.01980198019802
2019-12-03 14:43:36.749162:   [RESULT]   Score on FCE Limiting: 6.671414883423827
2019-12-03 14:43:36.749293:   [RESULT]   type_token_ratio: 0.38860971524288107
2019-12-03 14:43:36.749457:   [RESULT]   lexical_density: 0.38860971524288107
2019-12-03 14:43:36.749570:   [RESULT]   char_per_word: 6.120689655172414
2019-12-03 14:43:36.749660:   [RESULT]   Score on FCE Limiting: 6.897909085658175
2019-12-03 14:43:36.749836:   [RESULT]   type_token_ratio: 0.3584
2019-12-03 14:43:36.749926:   [RESULT]   lexical_density: 0.3584
2019-12-03 14:43:36.750109:   [RESULT]   char_per_word: 5.834821428571429
2019-12-03 14:43:36.750213:   [RESULT]   Score on FCE Limiting: 6.551621428571428
2019-12-03 14:43:36.750401:   [RESULT]   type_token_ratio: 0.3603174603174603
2019-12-03 14:43:36.750571:   [RESULT]   lexical_density: 0.3603174603174603
2019-12-03 14:43:36.750688:   [RESULT]   char_per_word: 5.872246696035242
2019-12-03 14:43:36.750781:   [RESULT]   Score on FCE Limiting: 6.592881616670162
2019-12-03 14:43:36.750903:   [RESULT]   type_token_ratio: 0.36316695352839934
2019-12-03 14:43:36.750995:   [RESULT]   lexical_density: 0.36316695352839934
2019-12-03 14:43:36.751107:   [RESULT]   char_per_word: 6.312796208530806
2019-12-03 14:43:36.751194:   [RESULT]   Score on FCE Limiting: 7.039130115587604
2019-12-03 14:43:36.751346:   [RESULT]   type_token_ratio: 0.32685512367491165
2019-12-03 14:43:36.751510:   [RESULT]   lexical_density: 0.32685512367491165
2019-12-03 14:43:36.751616:   [RESULT]   char_per_word: 6.005405405405406
2019-12-03 14:43:36.751703:   [RESULT]   Score on FCE Limiting: 6.659115652755228
2019-12-03 14:43:36.751780:   [DONE  ]   ----------Limiting FCE Done----------
2019-12-03 14:43:36.751892:   [START ]   Start Reading CAE
2019-12-03 14:43:36.752024:   [START ]   Preprocess ./dataset/cambridge/CAE/2.txt Start
2019-12-03 14:43:36.808912:   [DONE  ]   ----------Preprocess ./dataset/cambridge/CAE/2.txt Done----------
2019-12-03 14:43:36.809120:   [START ]   Preprocess ./dataset/cambridge/CAE/3.txt Start
2019-12-03 14:43:36.917672:   [DONE  ]   ----------Preprocess ./dataset/cambridge/CAE/3.txt Done----------
2019-12-03 14:43:36.918116:   [START ]   Preprocess ./dataset/cambridge/CAE/4.txt Start
2019-12-03 14:43:37.054677:   [DONE  ]   ----------Preprocess ./dataset/cambridge/CAE/4.txt Done----------
2019-12-03 14:43:37.054904:   [START ]   Preprocess ./dataset/cambridge/CAE/5.txt Start
2019-12-03 14:43:37.135340:   [DONE  ]   ----------Preprocess ./dataset/cambridge/CAE/5.txt Done----------
2019-12-03 14:43:37.135564:   [START ]   Preprocess ./dataset/cambridge/CAE/6.txt Start
2019-12-03 14:43:37.225384:   [DONE  ]   ----------Preprocess ./dataset/cambridge/CAE/6.txt Done----------
2019-12-03 14:43:37.225598:   [START ]   Preprocess ./dataset/cambridge/CAE/7.txt Start
2019-12-03 14:43:37.328254:   [DONE  ]   ----------Preprocess ./dataset/cambridge/CAE/7.txt Done----------
2019-12-03 14:43:37.328472:   [START ]   Preprocess ./dataset/cambridge/CAE/8.txt Start
2019-12-03 14:43:37.435098:   [DONE  ]   ----------Preprocess ./dataset/cambridge/CAE/8.txt Done----------
2019-12-03 14:43:37.435312:   [START ]   Preprocess ./dataset/cambridge/CAE/9.txt Start
2019-12-03 14:43:37.549761:   [DONE  ]   ----------Preprocess ./dataset/cambridge/CAE/9.txt Done----------
2019-12-03 14:43:37.549978:   [START ]   Preprocess ./dataset/cambridge/CAE/10.txt Start
2019-12-03 14:43:37.692612:   [DONE  ]   ----------Preprocess ./dataset/cambridge/CAE/10.txt Done----------
2019-12-03 14:43:37.692828:   [START ]   Preprocess ./dataset/cambridge/CAE/12.txt Start
2019-12-03 14:43:37.801120:   [DONE  ]   ----------Preprocess ./dataset/cambridge/CAE/12.txt Done----------
2019-12-03 14:43:37.801337:   [START ]   Preprocess ./dataset/cambridge/CAE/13.txt Start
2019-12-03 14:43:37.900045:   [DONE  ]   ----------Preprocess ./dataset/cambridge/CAE/13.txt Done----------
2019-12-03 14:43:37.900250:   [START ]   Preprocess ./dataset/cambridge/CAE/15.txt Start
2019-12-03 14:43:37.963972:   [DONE  ]   ----------Preprocess ./dataset/cambridge/CAE/15.txt Done----------
2019-12-03 14:43:37.964161:   [DONE  ]   ----------Read CAE Done----------
2019-12-03 14:43:37.965233:   [START ]   Limiting CAE Start
2019-12-03 14:43:37.965465:   [RESULT]   type_token_ratio: 0.40040241448692154
2019-12-03 14:43:37.965668:   [RESULT]   lexical_density: 0.40040241448692154
2019-12-03 14:43:37.965835:   [RESULT]   char_per_word: 6.582914572864322
2019-12-03 14:43:37.965989:   [RESULT]   Score on CAE Limiting: 7.3837194018381656
2019-12-03 14:43:37.966168:   [RESULT]   type_token_ratio: 0.3070362473347548
2019-12-03 14:43:37.966261:   [RESULT]   lexical_density: 0.3070362473347548
2019-12-03 14:43:37.966497:   [RESULT]   char_per_word: 5.90625
2019-12-03 14:43:37.966597:   [RESULT]   Score on CAE Limiting: 6.520322494669509
2019-12-03 14:43:37.966781:   [RESULT]   type_token_ratio: 0.38316151202749144
2019-12-03 14:43:37.966903:   [RESULT]   lexical_density: 0.38316151202749144
2019-12-03 14:43:37.967042:   [RESULT]   char_per_word: 6.692825112107624
2019-12-03 14:43:37.967136:   [RESULT]   Score on CAE Limiting: 7.459148136162607
2019-12-03 14:43:37.967262:   [RESULT]   type_token_ratio: 0.3851963746223565
2019-12-03 14:43:37.967349:   [RESULT]   lexical_density: 0.3851963746223565
2019-12-03 14:43:37.967460:   [RESULT]   char_per_word: 7.070588235294117
2019-12-03 14:43:37.967560:   [RESULT]   Score on CAE Limiting: 7.840980984538831
2019-12-03 14:43:37.967690:   [RESULT]   type_token_ratio: 0.3471502590673575
2019-12-03 14:43:37.967851:   [RESULT]   lexical_density: 0.3471502590673575
2019-12-03 14:43:37.967961:   [RESULT]   char_per_word: 6.634328358208955
2019-12-03 14:43:37.968050:   [RESULT]   Score on CAE Limiting: 7.328628876343671
2019-12-03 14:43:37.968185:   [RESULT]   type_token_ratio: 0.28782707622298065
2019-12-03 14:43:37.968275:   [RESULT]   lexical_density: 0.28782707622298065
2019-12-03 14:43:37.968556:   [RESULT]   char_per_word: 6.529644268774703
2019-12-03 14:43:37.968653:   [RESULT]   Score on CAE Limiting: 7.105298421220665
2019-12-03 14:43:37.968833:   [RESULT]   type_token_ratio: 0.39934711643090315
2019-12-03 14:43:37.969017:   [RESULT]   lexical_density: 0.39934711643090315
2019-12-03 14:43:37.969143:   [RESULT]   char_per_word: 6.4959128065395095
2019-12-03 14:43:37.969256:   [RESULT]   Score on CAE Limiting: 7.294607039401315
2019-12-03 14:43:37.969399:   [RESULT]   type_token_ratio: 0.3162743091095189
2019-12-03 14:43:37.969491:   [RESULT]   lexical_density: 0.3162743091095189
2019-12-03 14:43:37.969607:   [RESULT]   char_per_word: 6.090614886731392
2019-12-03 14:43:37.969707:   [RESULT]   Score on CAE Limiting: 6.72316350495043
2019-12-03 14:43:37.969939:   [RESULT]   type_token_ratio: 0.36108821104699096
2019-12-03 14:43:37.970109:   [RESULT]   lexical_density: 0.36108821104699096
2019-12-03 14:43:37.970241:   [RESULT]   char_per_word: 5.970319634703197
2019-12-03 14:43:37.970366:   [RESULT]   Score on CAE Limiting: 6.692496056797179
2019-12-03 14:43:37.970609:   [RESULT]   type_token_ratio: 0.3727175080558539
2019-12-03 14:43:37.970705:   [RESULT]   lexical_density: 0.3727175080558539
2019-12-03 14:43:37.970881:   [RESULT]   char_per_word: 6.296829971181556
2019-12-03 14:43:37.970971:   [RESULT]   Score on CAE Limiting: 7.042264987293264
2019-12-03 14:43:37.971117:   [RESULT]   type_token_ratio: 0.3079625292740047
2019-12-03 14:43:37.971274:   [RESULT]   lexical_density: 0.3079625292740047
2019-12-03 14:43:37.971381:   [RESULT]   char_per_word: 6.064638783269962
2019-12-03 14:43:37.971507:   [RESULT]   Score on CAE Limiting: 6.680563841817972
2019-12-03 14:43:37.971660:   [RESULT]   type_token_ratio: 0.36278195488721804
2019-12-03 14:43:37.971893:   [RESULT]   lexical_density: 0.36278195488721804
2019-12-03 14:43:37.972009:   [RESULT]   char_per_word: 6.27979274611399
2019-12-03 14:43:37.972172:   [RESULT]   Score on CAE Limiting: 7.005356655888426
2019-12-03 14:43:37.972268:   [DONE  ]   ----------Limiting CAE Done----------
2019-12-03 14:43:37.972424:   [START ]   Start Reading CPE
2019-12-03 14:43:37.972610:   [START ]   Preprocess ./dataset/cambridge/CPE/1.txt Start
2019-12-03 14:43:38.031776:   [DONE  ]   ----------Preprocess ./dataset/cambridge/CPE/1.txt Done----------
2019-12-03 14:43:38.032070:   [START ]   Preprocess ./dataset/cambridge/CPE/2.txt Start
2019-12-03 14:43:38.134209:   [DONE  ]   ----------Preprocess ./dataset/cambridge/CPE/2.txt Done----------
2019-12-03 14:43:38.134429:   [START ]   Preprocess ./dataset/cambridge/CPE/3.txt Start
2019-12-03 14:43:38.253485:   [DONE  ]   ----------Preprocess ./dataset/cambridge/CPE/3.txt Done----------
2019-12-03 14:43:38.253702:   [START ]   Preprocess ./dataset/cambridge/CPE/5.txt Start
2019-12-03 14:43:38.307203:   [DONE  ]   ----------Preprocess ./dataset/cambridge/CPE/5.txt Done----------
2019-12-03 14:43:38.307422:   [START ]   Preprocess ./dataset/cambridge/CPE/6.txt Start
2019-12-03 14:43:38.421816:   [DONE  ]   ----------Preprocess ./dataset/cambridge/CPE/6.txt Done----------
2019-12-03 14:43:38.422185:   [START ]   Preprocess ./dataset/cambridge/CPE/7.txt Start
2019-12-03 14:43:38.531180:   [DONE  ]   ----------Preprocess ./dataset/cambridge/CPE/7.txt Done----------
2019-12-03 14:43:38.531384:   [START ]   Preprocess ./dataset/cambridge/CPE/9.txt Start
2019-12-03 14:43:38.588013:   [DONE  ]   ----------Preprocess ./dataset/cambridge/CPE/9.txt Done----------
2019-12-03 14:43:38.588225:   [START ]   Preprocess ./dataset/cambridge/CPE/11.txt Start
2019-12-03 14:43:38.693935:   [DONE  ]   ----------Preprocess ./dataset/cambridge/CPE/11.txt Done----------
2019-12-03 14:43:38.694153:   [START ]   Preprocess ./dataset/cambridge/CPE/12.txt Start
2019-12-03 14:43:38.795374:   [DONE  ]   ----------Preprocess ./dataset/cambridge/CPE/12.txt Done----------
2019-12-03 14:43:38.795577:   [START ]   Preprocess ./dataset/cambridge/CPE/13.txt Start
2019-12-03 14:43:38.847808:   [DONE  ]   ----------Preprocess ./dataset/cambridge/CPE/13.txt Done----------
2019-12-03 14:43:38.848009:   [START ]   Preprocess ./dataset/cambridge/CPE/14.txt Start
2019-12-03 14:43:38.953835:   [DONE  ]   ----------Preprocess ./dataset/cambridge/CPE/14.txt Done----------
2019-12-03 14:43:38.954055:   [START ]   Preprocess ./dataset/cambridge/CPE/15.txt Start
2019-12-03 14:43:39.046202:   [DONE  ]   ----------Preprocess ./dataset/cambridge/CPE/15.txt Done----------
2019-12-03 14:43:39.046353:   [DONE  ]   ----------Read CPE Done----------
2019-12-03 14:43:39.047498:   [START ]   Limiting CPE Start
2019-12-03 14:43:39.047718:   [RESULT]   type_token_ratio: 0.42857142857142855
2019-12-03 14:43:39.047852:   [RESULT]   lexical_density: 0.42857142857142855
2019-12-03 14:43:39.047993:   [RESULT]   char_per_word: 6.173913043478261
2019-12-03 14:43:39.048085:   [RESULT]   Score on CPE Limiting: 7.031055900621118
2019-12-03 14:43:39.048252:   [RESULT]   type_token_ratio: 0.4033018867924528
2019-12-03 14:43:39.048365:   [RESULT]   lexical_density: 0.4033018867924528
2019-12-03 14:43:39.048488:   [RESULT]   char_per_word: 6.637426900584796
2019-12-03 14:43:39.048579:   [RESULT]   Score on CPE Limiting: 7.444030674169701
2019-12-03 14:43:39.048735:   [RESULT]   type_token_ratio: 0.3888888888888889
2019-12-03 14:43:39.048827:   [RESULT]   lexical_density: 0.3888888888888889
2019-12-03 14:43:39.048992:   [RESULT]   char_per_word: 6.466165413533835
2019-12-03 14:43:39.049084:   [RESULT]   Score on CPE Limiting: 7.2439431913116135
2019-12-03 14:43:39.049266:   [RESULT]   type_token_ratio: 0.4330357142857143
2019-12-03 14:43:39.049388:   [RESULT]   lexical_density: 0.4330357142857143
2019-12-03 14:43:39.049550:   [RESULT]   char_per_word: 6.283505154639175
2019-12-03 14:43:39.049672:   [RESULT]   Score on CPE Limiting: 7.149576583210604
2019-12-03 14:43:39.049933:   [RESULT]   type_token_ratio: 0.36129032258064514
2019-12-03 14:43:39.050051:   [RESULT]   lexical_density: 0.36129032258064514
2019-12-03 14:43:39.050283:   [RESULT]   char_per_word: 6.854166666666667
2019-12-03 14:43:39.050385:   [RESULT]   Score on CPE Limiting: 7.576747311827957
2019-12-03 14:43:39.050577:   [RESULT]   type_token_ratio: 0.3824152542372881
2019-12-03 14:43:39.050772:   [RESULT]   lexical_density: 0.3824152542372881
2019-12-03 14:43:39.050899:   [RESULT]   char_per_word: 5.966759002770083
2019-12-03 14:43:39.050990:   [RESULT]   Score on CPE Limiting: 6.7315895112446595
2019-12-03 14:43:39.051098:   [RESULT]   type_token_ratio: 0.4012875536480687
2019-12-03 14:43:39.051195:   [RESULT]   lexical_density: 0.4012875536480687
2019-12-03 14:43:39.051313:   [RESULT]   char_per_word: 6.106951871657754
2019-12-03 14:43:39.051411:   [RESULT]   Score on CPE Limiting: 6.909526978953892
2019-12-03 14:43:39.051569:   [RESULT]   type_token_ratio: 0.38571428571428573
2019-12-03 14:43:39.051664:   [RESULT]   lexical_density: 0.38571428571428573
2019-12-03 14:43:39.051785:   [RESULT]   char_per_word: 6.64957264957265
2019-12-03 14:43:39.051873:   [RESULT]   Score on CPE Limiting: 7.421001221001222
2019-12-03 14:43:39.052035:   [RESULT]   type_token_ratio: 0.35527809307604996
2019-12-03 14:43:39.052124:   [RESULT]   lexical_density: 0.35527809307604996
2019-12-03 14:43:39.052257:   [RESULT]   char_per_word: 7.156549520766773
2019-12-03 14:43:39.052344:   [RESULT]   Score on CPE Limiting: 7.867105706918873
2019-12-03 14:43:39.052472:   [RESULT]   type_token_ratio: 0.4451901565995526
2019-12-03 14:43:39.052625:   [RESULT]   lexical_density: 0.4451901565995526
2019-12-03 14:43:39.052724:   [RESULT]   char_per_word: 7.015075376884422
2019-12-03 14:43:39.052813:   [RESULT]   Score on CPE Limiting: 7.905455690083526
2019-12-03 14:43:39.052949:   [RESULT]   type_token_ratio: 0.35353535353535354
2019-12-03 14:43:39.053037:   [RESULT]   lexical_density: 0.35353535353535354
2019-12-03 14:43:39.053153:   [RESULT]   char_per_word: 6.095238095238095
2019-12-03 14:43:39.053240:   [RESULT]   Score on CPE Limiting: 6.8023088023088025
2019-12-03 14:43:39.053373:   [RESULT]   type_token_ratio: 0.33623910336239105
2019-12-03 14:43:39.053532:   [RESULT]   lexical_density: 0.33623910336239105
2019-12-03 14:43:39.053639:   [RESULT]   char_per_word: 6.555555555555555
2019-12-03 14:43:39.053722:   [RESULT]   Score on CPE Limiting: 7.228033762280338
2019-12-03 14:43:39.053792:   [DONE  ]   ----------Limiting CPE Done----------
2019-12-03 14:43:39.053902:   [DONE  ]   ----------Traning data Done----------
2019-12-03 14:43:39.053971:   [START ]   Test Start
2019-12-03 14:43:39.054083:   [START ]   Reading Test ./dataset/cambridge/KET/15.txt Start
2019-12-03 14:43:39.054198:   [START ]   Preprocess ./dataset/cambridge/KET/15.txt Start
2019-12-03 14:43:39.065424:   [DONE  ]   ----------Preprocess ./dataset/cambridge/KET/15.txt Done----------
2019-12-03 14:43:39.065703:   [DONE  ]   ----------Reading Test ./dataset/cambridge/KET/15.txt Done----------
2019-12-03 14:43:39.065879:   [START ]   Scoring KET Start
2019-12-03 14:44:29.707767:   [INFO  ]   -----------------------------------------------------------------------------
2019-12-03 14:44:29.707945:   [INFO  ]   Hello world!
2019-12-03 14:44:29.708685:   [INFO  ]   Start main processing!
2019-12-03 14:44:29.708923:   [START ]   Start Prepare Traning data
2019-12-03 14:44:29.709107:   [START ]   Initialize parameters Start
2019-12-03 14:44:29.709276:   [DONE  ]   ----------Initialize parameters Done----------
2019-12-03 14:44:29.709394:   [START ]   Start setup FOLD
2019-12-03 14:44:29.709572:   [START ]   Setup FOLD Done
2019-12-03 14:44:29.709656:   [START ]   Start Training data
2019-12-03 14:44:29.709737:   [START ]   Start Reading KET
2019-12-03 14:44:29.709885:   [START ]   Preprocess ./dataset/cambridge/KET/1.txt Start
2019-12-03 14:44:31.089651:   [DONE  ]   ----------Preprocess ./dataset/cambridge/KET/1.txt Done----------
2019-12-03 14:44:31.089933:   [START ]   Preprocess ./dataset/cambridge/KET/2.txt Start
2019-12-03 14:44:31.112013:   [DONE  ]   ----------Preprocess ./dataset/cambridge/KET/2.txt Done----------
2019-12-03 14:44:31.112225:   [START ]   Preprocess ./dataset/cambridge/KET/3.txt Start
2019-12-03 14:44:31.126116:   [DONE  ]   ----------Preprocess ./dataset/cambridge/KET/3.txt Done----------
2019-12-03 14:44:31.126353:   [START ]   Preprocess ./dataset/cambridge/KET/4.txt Start
2019-12-03 14:44:31.148754:   [DONE  ]   ----------Preprocess ./dataset/cambridge/KET/4.txt Done----------
2019-12-03 14:44:31.148965:   [START ]   Preprocess ./dataset/cambridge/KET/5.txt Start
2019-12-03 14:44:31.162551:   [DONE  ]   ----------Preprocess ./dataset/cambridge/KET/5.txt Done----------
2019-12-03 14:44:31.162703:   [START ]   Preprocess ./dataset/cambridge/KET/6.txt Start
2019-12-03 14:44:31.175582:   [DONE  ]   ----------Preprocess ./dataset/cambridge/KET/6.txt Done----------
2019-12-03 14:44:31.175725:   [START ]   Preprocess ./dataset/cambridge/KET/9.txt Start
2019-12-03 14:44:31.188676:   [DONE  ]   ----------Preprocess ./dataset/cambridge/KET/9.txt Done----------
2019-12-03 14:44:31.188819:   [START ]   Preprocess ./dataset/cambridge/KET/10.txt Start
2019-12-03 14:44:31.212467:   [DONE  ]   ----------Preprocess ./dataset/cambridge/KET/10.txt Done----------
2019-12-03 14:44:31.212612:   [START ]   Preprocess ./dataset/cambridge/KET/11.txt Start
2019-12-03 14:44:31.226632:   [DONE  ]   ----------Preprocess ./dataset/cambridge/KET/11.txt Done----------
2019-12-03 14:44:31.226820:   [START ]   Preprocess ./dataset/cambridge/KET/12.txt Start
2019-12-03 14:44:31.239404:   [DONE  ]   ----------Preprocess ./dataset/cambridge/KET/12.txt Done----------
2019-12-03 14:44:31.239559:   [START ]   Preprocess ./dataset/cambridge/KET/13.txt Start
2019-12-03 14:44:31.262788:   [DONE  ]   ----------Preprocess ./dataset/cambridge/KET/13.txt Done----------
2019-12-03 14:44:31.262983:   [START ]   Preprocess ./dataset/cambridge/KET/15.txt Start
2019-12-03 14:44:31.273787:   [DONE  ]   ----------Preprocess ./dataset/cambridge/KET/15.txt Done----------
2019-12-03 14:44:31.273873:   [DONE  ]   ----------Read KET Done----------
2019-12-03 14:44:31.274183:   [START ]   Limiting KET Start
2019-12-03 14:44:31.274369:   [RESULT]   type_token_ratio: 0.4
2019-12-03 14:44:31.274577:   [RESULT]   lexical_density: 0.4
2019-12-03 14:44:31.274679:   [RESULT]   char_per_word: 5.355263157894737
2019-12-03 14:44:31.274967:   [RESULT]   Score on KET Limiting: 6.155263157894738
2019-12-03 14:44:31.275083:   [RESULT]   type_token_ratio: 0.35294117647058826
2019-12-03 14:44:31.275236:   [RESULT]   lexical_density: 0.35294117647058826
2019-12-03 14:44:31.275338:   [RESULT]   char_per_word: 4.880952380952381
2019-12-03 14:44:31.275453:   [RESULT]   Score on KET Limiting: 5.586834733893557
2019-12-03 14:44:31.275762:   [RESULT]   type_token_ratio: 0.5204081632653061
2019-12-03 14:44:31.275923:   [RESULT]   lexical_density: 0.5204081632653061
2019-12-03 14:44:31.276007:   [RESULT]   char_per_word: 4.882352941176471
2019-12-03 14:44:31.276195:   [RESULT]   Score on KET Limiting: 5.923169267707083
2019-12-03 14:44:31.276298:   [RESULT]   type_token_ratio: 0.3193717277486911
2019-12-03 14:44:31.276379:   [RESULT]   lexical_density: 0.3193717277486911
2019-12-03 14:44:31.276461:   [RESULT]   char_per_word: 5.262295081967213
2019-12-03 14:44:31.276582:   [RESULT]   Score on KET Limiting: 5.901038537464595
2019-12-03 14:44:31.276666:   [RESULT]   type_token_ratio: 0.42016806722689076
2019-12-03 14:44:31.276859:   [RESULT]   lexical_density: 0.42016806722689076
2019-12-03 14:44:31.276935:   [RESULT]   char_per_word: 5.18
2019-12-03 14:44:31.277029:   [RESULT]   Score on KET Limiting: 6.020336134453782
2019-12-03 14:44:31.277110:   [RESULT]   type_token_ratio: 0.38596491228070173
2019-12-03 14:44:31.277183:   [RESULT]   lexical_density: 0.38596491228070173
2019-12-03 14:44:31.277259:   [RESULT]   char_per_word: 4.931818181818182
2019-12-03 14:44:31.277430:   [RESULT]   Score on KET Limiting: 5.703748006379586
2019-12-03 14:44:31.277511:   [RESULT]   type_token_ratio: 0.4074074074074074
2019-12-03 14:44:31.277698:   [RESULT]   lexical_density: 0.4074074074074074
2019-12-03 14:44:31.277774:   [RESULT]   char_per_word: 5.2727272727272725
2019-12-03 14:44:31.277909:   [RESULT]   Score on KET Limiting: 6.087542087542087
2019-12-03 14:44:31.277997:   [RESULT]   type_token_ratio: 0.35175879396984927
2019-12-03 14:44:31.278124:   [RESULT]   lexical_density: 0.35175879396984927
2019-12-03 14:44:31.278203:   [RESULT]   char_per_word: 5.271428571428571
2019-12-03 14:44:31.278277:   [RESULT]   Score on KET Limiting: 5.97494615936827
2019-12-03 14:44:31.278421:   [RESULT]   type_token_ratio: 0.31932773109243695
2019-12-03 14:44:31.278566:   [RESULT]   lexical_density: 0.31932773109243695
2019-12-03 14:44:31.278641:   [RESULT]   char_per_word: 5.526315789473684
2019-12-03 14:44:31.278821:   [RESULT]   Score on KET Limiting: 6.1649712516585575
2019-12-03 14:44:31.278964:   [RESULT]   type_token_ratio: 0.3125
2019-12-03 14:44:31.279038:   [RESULT]   lexical_density: 0.3125
2019-12-03 14:44:31.279149:   [RESULT]   char_per_word: 4.685714285714286
2019-12-03 14:44:31.279224:   [RESULT]   Score on KET Limiting: 5.310714285714286
2019-12-03 14:44:31.279344:   [RESULT]   type_token_ratio: 0.2914572864321608
2019-12-03 14:44:31.279493:   [RESULT]   lexical_density: 0.2914572864321608
2019-12-03 14:44:31.279573:   [RESULT]   char_per_word: 5.155172413793103
2019-12-03 14:44:31.279648:   [RESULT]   Score on KET Limiting: 5.738086986657425
2019-12-03 14:44:31.279761:   [RESULT]   type_token_ratio: 0.3473684210526316
2019-12-03 14:44:31.279836:   [RESULT]   lexical_density: 0.3473684210526316
2019-12-03 14:44:31.279946:   [RESULT]   char_per_word: 5.151515151515151
2019-12-03 14:44:31.280021:   [RESULT]   Score on KET Limiting: 5.846251993620414
2019-12-03 14:44:31.280123:   [DONE  ]   ----------Limiting KET Done----------
2019-12-03 14:44:31.280269:   [START ]   Start Reading PET
2019-12-03 14:44:31.280380:   [START ]   Preprocess ./dataset/cambridge/PET/1.txt Start
2019-12-03 14:44:31.318436:   [DONE  ]   ----------Preprocess ./dataset/cambridge/PET/1.txt Done----------
2019-12-03 14:44:31.318623:   [START ]   Preprocess ./dataset/cambridge/PET/2.txt Start
2019-12-03 14:44:31.342768:   [DONE  ]   ----------Preprocess ./dataset/cambridge/PET/2.txt Done----------
2019-12-03 14:44:31.342966:   [START ]   Preprocess ./dataset/cambridge/PET/3.txt Start
2019-12-03 14:44:31.360444:   [DONE  ]   ----------Preprocess ./dataset/cambridge/PET/3.txt Done----------
2019-12-03 14:44:31.360627:   [START ]   Preprocess ./dataset/cambridge/PET/4.txt Start
2019-12-03 14:44:31.404694:   [DONE  ]   ----------Preprocess ./dataset/cambridge/PET/4.txt Done----------
2019-12-03 14:44:31.404845:   [START ]   Preprocess ./dataset/cambridge/PET/6.txt Start
2019-12-03 14:44:31.421516:   [DONE  ]   ----------Preprocess ./dataset/cambridge/PET/6.txt Done----------
2019-12-03 14:44:31.421653:   [START ]   Preprocess ./dataset/cambridge/PET/7.txt Start
2019-12-03 14:44:31.473824:   [DONE  ]   ----------Preprocess ./dataset/cambridge/PET/7.txt Done----------
2019-12-03 14:44:31.474009:   [START ]   Preprocess ./dataset/cambridge/PET/8.txt Start
2019-12-03 14:44:31.500168:   [DONE  ]   ----------Preprocess ./dataset/cambridge/PET/8.txt Done----------
2019-12-03 14:44:31.500350:   [START ]   Preprocess ./dataset/cambridge/PET/9.txt Start
2019-12-03 14:44:31.518285:   [DONE  ]   ----------Preprocess ./dataset/cambridge/PET/9.txt Done----------
2019-12-03 14:44:31.518459:   [START ]   Preprocess ./dataset/cambridge/PET/11.txt Start
2019-12-03 14:44:31.539479:   [DONE  ]   ----------Preprocess ./dataset/cambridge/PET/11.txt Done----------
2019-12-03 14:44:31.539654:   [START ]   Preprocess ./dataset/cambridge/PET/12.txt Start
2019-12-03 14:44:31.555511:   [DONE  ]   ----------Preprocess ./dataset/cambridge/PET/12.txt Done----------
2019-12-03 14:44:31.555671:   [START ]   Preprocess ./dataset/cambridge/PET/14.txt Start
2019-12-03 14:44:31.579416:   [DONE  ]   ----------Preprocess ./dataset/cambridge/PET/14.txt Done----------
2019-12-03 14:44:31.579603:   [START ]   Preprocess ./dataset/cambridge/PET/15.txt Start
2019-12-03 14:44:31.596850:   [DONE  ]   ----------Preprocess ./dataset/cambridge/PET/15.txt Done----------
2019-12-03 14:44:31.596937:   [DONE  ]   ----------Read PET Done----------
2019-12-03 14:44:31.597355:   [START ]   Limiting PET Start
2019-12-03 14:44:31.597470:   [RESULT]   type_token_ratio: 0.40878378378378377
2019-12-03 14:44:31.597607:   [RESULT]   lexical_density: 0.40878378378378377
2019-12-03 14:44:31.597841:   [RESULT]   char_per_word: 6.231404958677686
2019-12-03 14:44:31.598051:   [RESULT]   Score on PET Limiting: 7.0489725262452545
2019-12-03 14:44:31.598282:   [RESULT]   type_token_ratio: 0.40096618357487923
2019-12-03 14:44:31.598552:   [RESULT]   lexical_density: 0.40096618357487923
2019-12-03 14:44:31.598766:   [RESULT]   char_per_word: 5.674698795180723
2019-12-03 14:44:31.598966:   [RESULT]   Score on PET Limiting: 6.476631162330482
2019-12-03 14:44:31.599183:   [RESULT]   type_token_ratio: 0.423841059602649
2019-12-03 14:44:31.599378:   [RESULT]   lexical_density: 0.423841059602649
2019-12-03 14:44:31.599584:   [RESULT]   char_per_word: 6.375
2019-12-03 14:44:31.599777:   [RESULT]   Score on PET Limiting: 7.222682119205299
2019-12-03 14:44:31.600035:   [RESULT]   type_token_ratio: 0.34413965087281795
2019-12-03 14:44:31.600315:   [RESULT]   lexical_density: 0.34413965087281795
2019-12-03 14:44:31.600533:   [RESULT]   char_per_word: 5.826086956521739
2019-12-03 14:44:31.600717:   [RESULT]   Score on PET Limiting: 6.514366258267375
2019-12-03 14:44:31.600923:   [RESULT]   type_token_ratio: 0.46621621621621623
2019-12-03 14:44:31.601039:   [RESULT]   lexical_density: 0.46621621621621623
2019-12-03 14:44:31.601140:   [RESULT]   char_per_word: 5.6521739130434785
2019-12-03 14:44:31.601290:   [RESULT]   Score on PET Limiting: 6.58460634547591
2019-12-03 14:44:31.601440:   [RESULT]   type_token_ratio: 0.3906633906633907
2019-12-03 14:44:31.601577:   [RESULT]   lexical_density: 0.3906633906633907
2019-12-03 14:44:31.601697:   [RESULT]   char_per_word: 6.062893081761007
2019-12-03 14:44:31.601880:   [RESULT]   Score on PET Limiting: 6.844219863087788
2019-12-03 14:44:31.601991:   [RESULT]   type_token_ratio: 0.3177570093457944
2019-12-03 14:44:31.602134:   [RESULT]   lexical_density: 0.3177570093457944
2019-12-03 14:44:31.602237:   [RESULT]   char_per_word: 5.573529411764706
2019-12-03 14:44:31.602331:   [RESULT]   Score on PET Limiting: 6.209043430456295
2019-12-03 14:44:31.602449:   [RESULT]   type_token_ratio: 0.4090909090909091
2019-12-03 14:44:31.602591:   [RESULT]   lexical_density: 0.4090909090909091
2019-12-03 14:44:31.602692:   [RESULT]   char_per_word: 5.4603174603174605
2019-12-03 14:44:31.602786:   [RESULT]   Score on PET Limiting: 6.278499278499279
2019-12-03 14:44:31.602895:   [RESULT]   type_token_ratio: 0.4782608695652174
2019-12-03 14:44:31.602991:   [RESULT]   lexical_density: 0.4782608695652174
2019-12-03 14:44:31.603096:   [RESULT]   char_per_word: 6.113636363636363
2019-12-03 14:44:31.603261:   [RESULT]   Score on PET Limiting: 7.070158102766799
2019-12-03 14:44:31.603402:   [RESULT]   type_token_ratio: 0.39552238805970147
2019-12-03 14:44:31.603514:   [RESULT]   lexical_density: 0.39552238805970147
2019-12-03 14:44:31.603620:   [RESULT]   char_per_word: 5.90566037735849
2019-12-03 14:44:31.603734:   [RESULT]   Score on PET Limiting: 6.696705153477893
2019-12-03 14:44:31.603847:   [RESULT]   type_token_ratio: 0.39215686274509803
2019-12-03 14:44:31.603943:   [RESULT]   lexical_density: 0.39215686274509803
2019-12-03 14:44:31.604048:   [RESULT]   char_per_word: 5.9625
2019-12-03 14:44:31.604164:   [RESULT]   Score on PET Limiting: 6.746813725490197
2019-12-03 14:44:31.604281:   [RESULT]   type_token_ratio: 0.37748344370860926
2019-12-03 14:44:31.604456:   [RESULT]   lexical_density: 0.37748344370860926
2019-12-03 14:44:31.604560:   [RESULT]   char_per_word: 5.9298245614035086
2019-12-03 14:44:31.604672:   [RESULT]   Score on PET Limiting: 6.684791448820727
2019-12-03 14:44:31.604763:   [DONE  ]   ----------Limiting PET Done----------
2019-12-03 14:44:31.604935:   [START ]   Start Reading FCE
2019-12-03 14:44:31.605099:   [START ]   Preprocess ./dataset/cambridge/FCE/1.txt Start
2019-12-03 14:44:31.686764:   [DONE  ]   ----------Preprocess ./dataset/cambridge/FCE/1.txt Done----------
2019-12-03 14:44:31.686969:   [START ]   Preprocess ./dataset/cambridge/FCE/2.txt Start
2019-12-03 14:44:31.759705:   [DONE  ]   ----------Preprocess ./dataset/cambridge/FCE/2.txt Done----------
2019-12-03 14:44:31.759932:   [START ]   Preprocess ./dataset/cambridge/FCE/3.txt Start
2019-12-03 14:44:31.835977:   [DONE  ]   ----------Preprocess ./dataset/cambridge/FCE/3.txt Done----------
2019-12-03 14:44:31.836248:   [START ]   Preprocess ./dataset/cambridge/FCE/4.txt Start
2019-12-03 14:44:31.913510:   [DONE  ]   ----------Preprocess ./dataset/cambridge/FCE/4.txt Done----------
2019-12-03 14:44:31.913718:   [START ]   Preprocess ./dataset/cambridge/FCE/5.txt Start
2019-12-03 14:44:31.986173:   [DONE  ]   ----------Preprocess ./dataset/cambridge/FCE/5.txt Done----------
2019-12-03 14:44:31.986395:   [START ]   Preprocess ./dataset/cambridge/FCE/6.txt Start
2019-12-03 14:44:32.068973:   [DONE  ]   ----------Preprocess ./dataset/cambridge/FCE/6.txt Done----------
2019-12-03 14:44:32.069190:   [START ]   Preprocess ./dataset/cambridge/FCE/7.txt Start
2019-12-03 14:44:32.156384:   [DONE  ]   ----------Preprocess ./dataset/cambridge/FCE/7.txt Done----------
2019-12-03 14:44:32.156614:   [START ]   Preprocess ./dataset/cambridge/FCE/8.txt Start
2019-12-03 14:44:32.243476:   [DONE  ]   ----------Preprocess ./dataset/cambridge/FCE/8.txt Done----------
2019-12-03 14:44:32.243701:   [START ]   Preprocess ./dataset/cambridge/FCE/9.txt Start
2019-12-03 14:44:32.311572:   [DONE  ]   ----------Preprocess ./dataset/cambridge/FCE/9.txt Done----------
2019-12-03 14:44:32.311779:   [START ]   Preprocess ./dataset/cambridge/FCE/10.txt Start
2019-12-03 14:44:32.384266:   [DONE  ]   ----------Preprocess ./dataset/cambridge/FCE/10.txt Done----------
2019-12-03 14:44:32.384475:   [START ]   Preprocess ./dataset/cambridge/FCE/13.txt Start
2019-12-03 14:44:32.458639:   [DONE  ]   ----------Preprocess ./dataset/cambridge/FCE/13.txt Done----------
2019-12-03 14:44:32.458860:   [START ]   Preprocess ./dataset/cambridge/FCE/14.txt Start
2019-12-03 14:44:32.526607:   [DONE  ]   ----------Preprocess ./dataset/cambridge/FCE/14.txt Done----------
2019-12-03 14:44:32.526741:   [DONE  ]   ----------Read FCE Done----------
2019-12-03 14:44:32.527787:   [START ]   Limiting FCE Start
2019-12-03 14:44:32.528033:   [RESULT]   type_token_ratio: 0.34206896551724136
2019-12-03 14:44:32.528215:   [RESULT]   lexical_density: 0.34206896551724136
2019-12-03 14:44:32.528392:   [RESULT]   char_per_word: 5.919354838709677
2019-12-03 14:44:32.528531:   [RESULT]   Score on FCE Limiting: 6.60349276974416
2019-12-03 14:44:32.528713:   [RESULT]   type_token_ratio: 0.3769911504424779
2019-12-03 14:44:32.528824:   [RESULT]   lexical_density: 0.3769911504424779
2019-12-03 14:44:32.528956:   [RESULT]   char_per_word: 5.962441314553991
2019-12-03 14:44:32.529064:   [RESULT]   Score on FCE Limiting: 6.716423615438947
2019-12-03 14:44:32.529217:   [RESULT]   type_token_ratio: 0.36482084690553745
2019-12-03 14:44:32.529330:   [RESULT]   lexical_density: 0.36482084690553745
2019-12-03 14:44:32.529461:   [RESULT]   char_per_word: 6.401785714285714
2019-12-03 14:44:32.529566:   [RESULT]   Score on FCE Limiting: 7.13142740809679
2019-12-03 14:44:32.529721:   [RESULT]   type_token_ratio: 0.3472429210134128
2019-12-03 14:44:32.529843:   [RESULT]   lexical_density: 0.3472429210134128
2019-12-03 14:44:32.529959:   [RESULT]   char_per_word: 5.896995708154506
2019-12-03 14:44:32.530049:   [RESULT]   Score on FCE Limiting: 6.5914815501813315
2019-12-03 14:44:32.530321:   [RESULT]   type_token_ratio: 0.3643533123028391
2019-12-03 14:44:32.530476:   [RESULT]   lexical_density: 0.3643533123028391
2019-12-03 14:44:32.530594:   [RESULT]   char_per_word: 6.008658008658009
2019-12-03 14:44:32.530688:   [RESULT]   Score on FCE Limiting: 6.737364633263687
2019-12-03 14:44:32.530829:   [RESULT]   type_token_ratio: 0.31420765027322406
2019-12-03 14:44:32.530922:   [RESULT]   lexical_density: 0.31420765027322406
2019-12-03 14:44:32.531073:   [RESULT]   char_per_word: 5.773913043478261
2019-12-03 14:44:32.531163:   [RESULT]   Score on FCE Limiting: 6.40232834402471
2019-12-03 14:44:32.531295:   [RESULT]   type_token_ratio: 0.29839883551673946
2019-12-03 14:44:32.531387:   [RESULT]   lexical_density: 0.29839883551673946
2019-12-03 14:44:32.531496:   [RESULT]   char_per_word: 5.590243902439024
2019-12-03 14:44:32.531586:   [RESULT]   Score on FCE Limiting: 6.187041573472502
2019-12-03 14:44:32.531736:   [RESULT]   type_token_ratio: 0.3258064516129032
2019-12-03 14:44:32.531826:   [RESULT]   lexical_density: 0.3258064516129032
2019-12-03 14:44:32.531954:   [RESULT]   char_per_word: 6.01980198019802
2019-12-03 14:44:32.532043:   [RESULT]   Score on FCE Limiting: 6.671414883423827
2019-12-03 14:44:32.532188:   [RESULT]   type_token_ratio: 0.38860971524288107
2019-12-03 14:44:32.532351:   [RESULT]   lexical_density: 0.38860971524288107
2019-12-03 14:44:32.532462:   [RESULT]   char_per_word: 6.120689655172414
2019-12-03 14:44:32.532551:   [RESULT]   Score on FCE Limiting: 6.897909085658175
2019-12-03 14:44:32.532676:   [RESULT]   type_token_ratio: 0.3584
2019-12-03 14:44:32.532765:   [RESULT]   lexical_density: 0.3584
2019-12-03 14:44:32.532944:   [RESULT]   char_per_word: 5.834821428571429
2019-12-03 14:44:32.533040:   [RESULT]   Score on FCE Limiting: 6.551621428571428
2019-12-03 14:44:32.533180:   [RESULT]   type_token_ratio: 0.36316695352839934
2019-12-03 14:44:32.533343:   [RESULT]   lexical_density: 0.36316695352839934
2019-12-03 14:44:32.533451:   [RESULT]   char_per_word: 6.312796208530806
2019-12-03 14:44:32.533538:   [RESULT]   Score on FCE Limiting: 7.039130115587604
2019-12-03 14:44:32.533655:   [RESULT]   type_token_ratio: 0.32685512367491165
2019-12-03 14:44:32.533743:   [RESULT]   lexical_density: 0.32685512367491165
2019-12-03 14:44:32.533847:   [RESULT]   char_per_word: 6.005405405405406
2019-12-03 14:44:32.533934:   [RESULT]   Score on FCE Limiting: 6.659115652755228
2019-12-03 14:44:32.534013:   [DONE  ]   ----------Limiting FCE Done----------
2019-12-03 14:44:32.534129:   [START ]   Start Reading CAE
2019-12-03 14:44:32.534259:   [START ]   Preprocess ./dataset/cambridge/CAE/2.txt Start
2019-12-03 14:44:32.590324:   [DONE  ]   ----------Preprocess ./dataset/cambridge/CAE/2.txt Done----------
2019-12-03 14:44:32.590549:   [START ]   Preprocess ./dataset/cambridge/CAE/3.txt Start
2019-12-03 14:44:32.703350:   [DONE  ]   ----------Preprocess ./dataset/cambridge/CAE/3.txt Done----------
2019-12-03 14:44:32.703569:   [START ]   Preprocess ./dataset/cambridge/CAE/4.txt Start
2019-12-03 14:44:32.842130:   [DONE  ]   ----------Preprocess ./dataset/cambridge/CAE/4.txt Done----------
2019-12-03 14:44:32.842350:   [START ]   Preprocess ./dataset/cambridge/CAE/5.txt Start
2019-12-03 14:44:32.917783:   [DONE  ]   ----------Preprocess ./dataset/cambridge/CAE/5.txt Done----------
2019-12-03 14:44:32.918087:   [START ]   Preprocess ./dataset/cambridge/CAE/7.txt Start
2019-12-03 14:44:33.013561:   [DONE  ]   ----------Preprocess ./dataset/cambridge/CAE/7.txt Done----------
2019-12-03 14:44:33.013771:   [START ]   Preprocess ./dataset/cambridge/CAE/8.txt Start
2019-12-03 14:44:33.124763:   [DONE  ]   ----------Preprocess ./dataset/cambridge/CAE/8.txt Done----------
2019-12-03 14:44:33.124985:   [START ]   Preprocess ./dataset/cambridge/CAE/10.txt Start
2019-12-03 14:44:33.261832:   [DONE  ]   ----------Preprocess ./dataset/cambridge/CAE/10.txt Done----------
2019-12-03 14:44:33.262055:   [START ]   Preprocess ./dataset/cambridge/CAE/11.txt Start
2019-12-03 14:44:33.330380:   [DONE  ]   ----------Preprocess ./dataset/cambridge/CAE/11.txt Done----------
2019-12-03 14:44:33.330596:   [START ]   Preprocess ./dataset/cambridge/CAE/12.txt Start
2019-12-03 14:44:33.436561:   [DONE  ]   ----------Preprocess ./dataset/cambridge/CAE/12.txt Done----------
2019-12-03 14:44:33.436781:   [START ]   Preprocess ./dataset/cambridge/CAE/13.txt Start
2019-12-03 14:44:33.547391:   [DONE  ]   ----------Preprocess ./dataset/cambridge/CAE/13.txt Done----------
2019-12-03 14:44:33.547601:   [START ]   Preprocess ./dataset/cambridge/CAE/14.txt Start
2019-12-03 14:44:33.617074:   [DONE  ]   ----------Preprocess ./dataset/cambridge/CAE/14.txt Done----------
2019-12-03 14:44:33.617285:   [START ]   Preprocess ./dataset/cambridge/CAE/15.txt Start
2019-12-03 14:44:33.685777:   [DONE  ]   ----------Preprocess ./dataset/cambridge/CAE/15.txt Done----------
2019-12-03 14:44:33.685919:   [DONE  ]   ----------Read CAE Done----------
2019-12-03 14:44:33.687362:   [START ]   Limiting CAE Start
2019-12-03 14:44:33.687708:   [RESULT]   type_token_ratio: 0.40040241448692154
2019-12-03 14:44:33.687807:   [RESULT]   lexical_density: 0.40040241448692154
2019-12-03 14:44:33.688113:   [RESULT]   char_per_word: 6.582914572864322
2019-12-03 14:44:33.688209:   [RESULT]   Score on CAE Limiting: 7.3837194018381656
2019-12-03 14:44:33.688364:   [RESULT]   type_token_ratio: 0.3070362473347548
2019-12-03 14:44:33.688468:   [RESULT]   lexical_density: 0.3070362473347548
2019-12-03 14:44:33.688592:   [RESULT]   char_per_word: 5.90625
2019-12-03 14:44:33.688687:   [RESULT]   Score on CAE Limiting: 6.520322494669509
2019-12-03 14:44:33.688884:   [RESULT]   type_token_ratio: 0.38316151202749144
2019-12-03 14:44:33.689008:   [RESULT]   lexical_density: 0.38316151202749144
2019-12-03 14:44:33.689286:   [RESULT]   char_per_word: 6.692825112107624
2019-12-03 14:44:33.689542:   [RESULT]   Score on CAE Limiting: 7.459148136162607
2019-12-03 14:44:33.689821:   [RESULT]   type_token_ratio: 0.3851963746223565
2019-12-03 14:44:33.690018:   [RESULT]   lexical_density: 0.3851963746223565
2019-12-03 14:44:33.690144:   [RESULT]   char_per_word: 7.070588235294117
2019-12-03 14:44:33.690239:   [RESULT]   Score on CAE Limiting: 7.840980984538831
2019-12-03 14:44:33.690408:   [RESULT]   type_token_ratio: 0.28782707622298065
2019-12-03 14:44:33.690505:   [RESULT]   lexical_density: 0.28782707622298065
2019-12-03 14:44:33.690694:   [RESULT]   char_per_word: 6.529644268774703
2019-12-03 14:44:33.690806:   [RESULT]   Score on CAE Limiting: 7.105298421220665
2019-12-03 14:44:33.690995:   [RESULT]   type_token_ratio: 0.39934711643090315
2019-12-03 14:44:33.691186:   [RESULT]   lexical_density: 0.39934711643090315
2019-12-03 14:44:33.691319:   [RESULT]   char_per_word: 6.4959128065395095
2019-12-03 14:44:33.691431:   [RESULT]   Score on CAE Limiting: 7.294607039401315
2019-12-03 14:44:33.691598:   [RESULT]   type_token_ratio: 0.36108821104699096
2019-12-03 14:44:33.691700:   [RESULT]   lexical_density: 0.36108821104699096
2019-12-03 14:44:33.691880:   [RESULT]   char_per_word: 5.970319634703197
2019-12-03 14:44:33.691978:   [RESULT]   Score on CAE Limiting: 6.692496056797179
2019-12-03 14:44:33.692118:   [RESULT]   type_token_ratio: 0.43853820598006643
2019-12-03 14:44:33.692281:   [RESULT]   lexical_density: 0.43853820598006643
2019-12-03 14:44:33.692397:   [RESULT]   char_per_word: 6.371212121212121
2019-12-03 14:44:33.692487:   [RESULT]   Score on CAE Limiting: 7.2482885331722535
2019-12-03 14:44:33.692638:   [RESULT]   type_token_ratio: 0.3727175080558539
2019-12-03 14:44:33.692737:   [RESULT]   lexical_density: 0.3727175080558539
2019-12-03 14:44:33.692877:   [RESULT]   char_per_word: 6.296829971181556
2019-12-03 14:44:33.692972:   [RESULT]   Score on CAE Limiting: 7.042264987293264
2019-12-03 14:44:33.693123:   [RESULT]   type_token_ratio: 0.3079625292740047
2019-12-03 14:44:33.693287:   [RESULT]   lexical_density: 0.3079625292740047
2019-12-03 14:44:33.693402:   [RESULT]   char_per_word: 6.064638783269962
2019-12-03 14:44:33.693492:   [RESULT]   Score on CAE Limiting: 6.680563841817972
2019-12-03 14:44:33.693608:   [RESULT]   type_token_ratio: 0.36879432624113473
2019-12-03 14:44:33.693695:   [RESULT]   lexical_density: 0.36879432624113473
2019-12-03 14:44:33.693803:   [RESULT]   char_per_word: 6.586538461538462
2019-12-03 14:44:33.693890:   [RESULT]   Score on CAE Limiting: 7.324127114020731
2019-12-03 14:44:33.694007:   [RESULT]   type_token_ratio: 0.36278195488721804
2019-12-03 14:44:33.694098:   [RESULT]   lexical_density: 0.36278195488721804
2019-12-03 14:44:33.694204:   [RESULT]   char_per_word: 6.27979274611399
2019-12-03 14:44:33.694290:   [RESULT]   Score on CAE Limiting: 7.005356655888426
2019-12-03 14:44:33.694367:   [DONE  ]   ----------Limiting CAE Done----------
2019-12-03 14:44:33.694519:   [START ]   Start Reading CPE
2019-12-03 14:44:33.694668:   [START ]   Preprocess ./dataset/cambridge/CPE/2.txt Start
2019-12-03 14:44:33.789995:   [DONE  ]   ----------Preprocess ./dataset/cambridge/CPE/2.txt Done----------
2019-12-03 14:44:33.790216:   [START ]   Preprocess ./dataset/cambridge/CPE/3.txt Start
2019-12-03 14:44:33.918782:   [DONE  ]   ----------Preprocess ./dataset/cambridge/CPE/3.txt Done----------
2019-12-03 14:44:33.918998:   [START ]   Preprocess ./dataset/cambridge/CPE/4.txt Start
2019-12-03 14:44:33.986630:   [DONE  ]   ----------Preprocess ./dataset/cambridge/CPE/4.txt Done----------
2019-12-03 14:44:33.986842:   [START ]   Preprocess ./dataset/cambridge/CPE/5.txt Start
2019-12-03 14:44:34.045633:   [DONE  ]   ----------Preprocess ./dataset/cambridge/CPE/5.txt Done----------
2019-12-03 14:44:34.045836:   [START ]   Preprocess ./dataset/cambridge/CPE/6.txt Start
2019-12-03 14:44:34.169046:   [DONE  ]   ----------Preprocess ./dataset/cambridge/CPE/6.txt Done----------
2019-12-03 14:44:34.169438:   [START ]   Preprocess ./dataset/cambridge/CPE/9.txt Start
2019-12-03 14:44:34.223800:   [DONE  ]   ----------Preprocess ./dataset/cambridge/CPE/9.txt Done----------
2019-12-03 14:44:34.224021:   [START ]   Preprocess ./dataset/cambridge/CPE/10.txt Start
2019-12-03 14:44:34.323407:   [DONE  ]   ----------Preprocess ./dataset/cambridge/CPE/10.txt Done----------
2019-12-03 14:44:34.323622:   [START ]   Preprocess ./dataset/cambridge/CPE/11.txt Start
2019-12-03 14:44:34.433678:   [DONE  ]   ----------Preprocess ./dataset/cambridge/CPE/11.txt Done----------
2019-12-03 14:44:34.433893:   [START ]   Preprocess ./dataset/cambridge/CPE/12.txt Start
2019-12-03 14:44:34.542820:   [DONE  ]   ----------Preprocess ./dataset/cambridge/CPE/12.txt Done----------
2019-12-03 14:44:34.543107:   [START ]   Preprocess ./dataset/cambridge/CPE/13.txt Start
2019-12-03 14:44:34.605211:   [DONE  ]   ----------Preprocess ./dataset/cambridge/CPE/13.txt Done----------
2019-12-03 14:44:34.605455:   [START ]   Preprocess ./dataset/cambridge/CPE/14.txt Start
2019-12-03 14:44:34.720699:   [DONE  ]   ----------Preprocess ./dataset/cambridge/CPE/14.txt Done----------
2019-12-03 14:44:34.720921:   [START ]   Preprocess ./dataset/cambridge/CPE/15.txt Start
2019-12-03 14:44:34.822708:   [DONE  ]   ----------Preprocess ./dataset/cambridge/CPE/15.txt Done----------
2019-12-03 14:44:34.822881:   [DONE  ]   ----------Read CPE Done----------
2019-12-03 14:44:34.823825:   [START ]   Limiting CPE Start
2019-12-03 14:44:34.824113:   [RESULT]   type_token_ratio: 0.4033018867924528
2019-12-03 14:44:34.824233:   [RESULT]   lexical_density: 0.4033018867924528
2019-12-03 14:44:34.824401:   [RESULT]   char_per_word: 6.637426900584796
2019-12-03 14:44:34.824506:   [RESULT]   Score on CPE Limiting: 7.444030674169701
2019-12-03 14:44:34.824678:   [RESULT]   type_token_ratio: 0.3888888888888889
2019-12-03 14:44:34.824779:   [RESULT]   lexical_density: 0.3888888888888889
2019-12-03 14:44:34.824932:   [RESULT]   char_per_word: 6.466165413533835
2019-12-03 14:44:34.825031:   [RESULT]   Score on CPE Limiting: 7.2439431913116135
2019-12-03 14:44:34.825180:   [RESULT]   type_token_ratio: 0.39679715302491103
2019-12-03 14:44:34.825272:   [RESULT]   lexical_density: 0.39679715302491103
2019-12-03 14:44:34.825498:   [RESULT]   char_per_word: 7.044843049327354
2019-12-03 14:44:34.825602:   [RESULT]   Score on CPE Limiting: 7.8384373553771765
2019-12-03 14:44:34.825726:   [RESULT]   type_token_ratio: 0.4330357142857143
2019-12-03 14:44:34.825818:   [RESULT]   lexical_density: 0.4330357142857143
2019-12-03 14:44:34.826032:   [RESULT]   char_per_word: 6.283505154639175
2019-12-03 14:44:34.826131:   [RESULT]   Score on CPE Limiting: 7.149576583210604
2019-12-03 14:44:34.826295:   [RESULT]   type_token_ratio: 0.36129032258064514
2019-12-03 14:44:34.826397:   [RESULT]   lexical_density: 0.36129032258064514
2019-12-03 14:44:34.826584:   [RESULT]   char_per_word: 6.854166666666667
2019-12-03 14:44:34.826681:   [RESULT]   Score on CPE Limiting: 7.576747311827957
2019-12-03 14:44:34.826825:   [RESULT]   type_token_ratio: 0.4012875536480687
2019-12-03 14:44:34.826914:   [RESULT]   lexical_density: 0.4012875536480687
2019-12-03 14:44:34.827020:   [RESULT]   char_per_word: 6.106951871657754
2019-12-03 14:44:34.827165:   [RESULT]   Score on CPE Limiting: 6.909526978953892
2019-12-03 14:44:34.827407:   [RESULT]   type_token_ratio: 0.43367935409457903
2019-12-03 14:44:34.827512:   [RESULT]   lexical_density: 0.43367935409457903
2019-12-03 14:44:34.827654:   [RESULT]   char_per_word: 6.473404255319149
2019-12-03 14:44:34.827754:   [RESULT]   Score on CPE Limiting: 7.340762963508308
2019-12-03 14:44:34.827914:   [RESULT]   type_token_ratio: 0.38571428571428573
2019-12-03 14:44:34.828014:   [RESULT]   lexical_density: 0.38571428571428573
2019-12-03 14:44:34.828177:   [RESULT]   char_per_word: 6.64957264957265
2019-12-03 14:44:34.828268:   [RESULT]   Score on CPE Limiting: 7.421001221001222
2019-12-03 14:44:34.828523:   [RESULT]   type_token_ratio: 0.35527809307604996
2019-12-03 14:44:34.828617:   [RESULT]   lexical_density: 0.35527809307604996
2019-12-03 14:44:34.828749:   [RESULT]   char_per_word: 7.156549520766773
2019-12-03 14:44:34.828838:   [RESULT]   Score on CPE Limiting: 7.867105706918873
2019-12-03 14:44:34.828967:   [RESULT]   type_token_ratio: 0.4451901565995526
2019-12-03 14:44:34.829050:   [RESULT]   lexical_density: 0.4451901565995526
2019-12-03 14:44:34.829192:   [RESULT]   char_per_word: 7.015075376884422
2019-12-03 14:44:34.829288:   [RESULT]   Score on CPE Limiting: 7.905455690083526
2019-12-03 14:44:34.829619:   [RESULT]   type_token_ratio: 0.35353535353535354
2019-12-03 14:44:34.829804:   [RESULT]   lexical_density: 0.35353535353535354
2019-12-03 14:44:34.829944:   [RESULT]   char_per_word: 6.095238095238095
2019-12-03 14:44:34.830045:   [RESULT]   Score on CPE Limiting: 6.8023088023088025
2019-12-03 14:44:34.830226:   [RESULT]   type_token_ratio: 0.33623910336239105
2019-12-03 14:44:34.830331:   [RESULT]   lexical_density: 0.33623910336239105
2019-12-03 14:44:34.830461:   [RESULT]   char_per_word: 6.555555555555555
2019-12-03 14:44:34.830551:   [RESULT]   Score on CPE Limiting: 7.228033762280338
2019-12-03 14:44:34.830672:   [DONE  ]   ----------Limiting CPE Done----------
2019-12-03 14:44:34.830755:   [DONE  ]   ----------Traning data Done----------
2019-12-03 14:44:34.830856:   [START ]   Test Start
2019-12-03 14:44:34.830934:   [START ]   Reading Test ./dataset/cambridge/KET/8.txt Start
2019-12-03 14:44:34.831079:   [START ]   Preprocess ./dataset/cambridge/KET/8.txt Start
2019-12-03 14:44:34.847059:   [DONE  ]   ----------Preprocess ./dataset/cambridge/KET/8.txt Done----------
2019-12-03 14:44:34.847188:   [DONE  ]   ----------Reading Test ./dataset/cambridge/KET/8.txt Done----------
2019-12-03 14:44:34.847492:   [START ]   Scoring KET Start
2019-12-03 14:45:36.158376:   [INFO  ]   -----------------------------------------------------------------------------
2019-12-03 14:45:36.158622:   [INFO  ]   Hello world!
2019-12-03 14:45:36.159307:   [INFO  ]   Start main processing!
2019-12-03 14:45:36.159581:   [START ]   Start Prepare Traning data
2019-12-03 14:45:36.159669:   [START ]   Initialize parameters Start
2019-12-03 14:45:36.159991:   [DONE  ]   Initialize parameters Done
2019-12-03 14:45:36.160117:   [START ]   Start setup FOLD
2019-12-03 14:45:36.160280:   [START ]   Setup FOLD Done
2019-12-03 14:45:36.160381:   [START ]   Start Training data
2019-12-03 14:45:36.160710:   [START ]   Start Reading KET
2019-12-03 14:45:36.160874:   [START ]   Preprocess ./dataset/cambridge/KET/1.txt Start
2019-12-03 14:45:37.562101:   [DONE  ]   Preprocess ./dataset/cambridge/KET/1.txt Done
2019-12-03 14:45:37.562390:   [START ]   Preprocess ./dataset/cambridge/KET/2.txt Start
2019-12-03 14:45:37.582748:   [DONE  ]   Preprocess ./dataset/cambridge/KET/2.txt Done
2019-12-03 14:45:37.583092:   [START ]   Preprocess ./dataset/cambridge/KET/3.txt Start
2019-12-03 14:45:37.597301:   [DONE  ]   Preprocess ./dataset/cambridge/KET/3.txt Done
2019-12-03 14:45:37.597505:   [START ]   Preprocess ./dataset/cambridge/KET/4.txt Start
2019-12-03 14:45:37.619266:   [DONE  ]   Preprocess ./dataset/cambridge/KET/4.txt Done
2019-12-03 14:45:37.619441:   [START ]   Preprocess ./dataset/cambridge/KET/6.txt Start
2019-12-03 14:45:37.632029:   [DONE  ]   Preprocess ./dataset/cambridge/KET/6.txt Done
2019-12-03 14:45:37.632179:   [START ]   Preprocess ./dataset/cambridge/KET/7.txt Start
2019-12-03 14:45:37.654440:   [DONE  ]   Preprocess ./dataset/cambridge/KET/7.txt Done
2019-12-03 14:45:37.654699:   [START ]   Preprocess ./dataset/cambridge/KET/8.txt Start
2019-12-03 14:45:37.669218:   [DONE  ]   Preprocess ./dataset/cambridge/KET/8.txt Done
2019-12-03 14:45:37.669385:   [START ]   Preprocess ./dataset/cambridge/KET/10.txt Start
2019-12-03 14:45:37.692073:   [DONE  ]   Preprocess ./dataset/cambridge/KET/10.txt Done
2019-12-03 14:45:37.692228:   [START ]   Preprocess ./dataset/cambridge/KET/12.txt Start
2019-12-03 14:45:37.705029:   [DONE  ]   Preprocess ./dataset/cambridge/KET/12.txt Done
2019-12-03 14:45:37.705175:   [START ]   Preprocess ./dataset/cambridge/KET/13.txt Start
2019-12-03 14:45:37.728827:   [DONE  ]   Preprocess ./dataset/cambridge/KET/13.txt Done
2019-12-03 14:45:37.729338:   [START ]   Preprocess ./dataset/cambridge/KET/14.txt Start
2019-12-03 14:45:37.743555:   [DONE  ]   Preprocess ./dataset/cambridge/KET/14.txt Done
2019-12-03 14:45:37.743787:   [START ]   Preprocess ./dataset/cambridge/KET/15.txt Start
2019-12-03 14:45:37.754555:   [DONE  ]   Preprocess ./dataset/cambridge/KET/15.txt Done
2019-12-03 14:45:37.754638:   [DONE  ]   Read KET Done
2019-12-03 14:45:37.754905:   [START ]   Limiting KET Start
2019-12-03 14:45:37.755013:   [RESULT]   ----------type_token_ratio: 0.4----------
2019-12-03 14:45:37.755098:   [RESULT]   ----------lexical_density: 0.4----------
2019-12-03 14:45:37.755184:   [RESULT]   ----------char_per_word: 5.355263157894737----------
2019-12-03 14:45:37.755261:   [RESULT]   ----------Score on KET Limiting: 6.155263157894738----------
2019-12-03 14:45:37.755488:   [RESULT]   ----------type_token_ratio: 0.35294117647058826----------
2019-12-03 14:45:37.755576:   [RESULT]   ----------lexical_density: 0.35294117647058826----------
2019-12-03 14:45:37.755862:   [RESULT]   ----------char_per_word: 4.880952380952381----------
2019-12-03 14:45:37.755970:   [RESULT]   ----------Score on KET Limiting: 5.586834733893557----------
2019-12-03 14:45:37.756098:   [RESULT]   ----------type_token_ratio: 0.5204081632653061----------
2019-12-03 14:45:37.756215:   [RESULT]   ----------lexical_density: 0.5204081632653061----------
2019-12-03 14:45:37.756337:   [RESULT]   ----------char_per_word: 4.882352941176471----------
2019-12-03 14:45:37.756416:   [RESULT]   ----------Score on KET Limiting: 5.923169267707083----------
2019-12-03 14:45:37.756557:   [RESULT]   ----------type_token_ratio: 0.3193717277486911----------
2019-12-03 14:45:37.756636:   [RESULT]   ----------lexical_density: 0.3193717277486911----------
2019-12-03 14:45:37.756764:   [RESULT]   ----------char_per_word: 5.262295081967213----------
2019-12-03 14:45:37.756851:   [RESULT]   ----------Score on KET Limiting: 5.901038537464595----------
2019-12-03 14:45:37.756940:   [RESULT]   ----------type_token_ratio: 0.38596491228070173----------
2019-12-03 14:45:37.757016:   [RESULT]   ----------lexical_density: 0.38596491228070173----------
2019-12-03 14:45:37.757207:   [RESULT]   ----------char_per_word: 4.931818181818182----------
2019-12-03 14:45:37.757297:   [RESULT]   ----------Score on KET Limiting: 5.703748006379586----------
2019-12-03 14:45:37.757492:   [RESULT]   ----------type_token_ratio: 0.38860103626943004----------
2019-12-03 14:45:37.757579:   [RESULT]   ----------lexical_density: 0.38860103626943004----------
2019-12-03 14:45:37.757665:   [RESULT]   ----------char_per_word: 5.56----------
2019-12-03 14:45:37.757793:   [RESULT]   ----------Score on KET Limiting: 6.337202072538859----------
2019-12-03 14:45:37.757878:   [RESULT]   ----------type_token_ratio: 0.4186046511627907----------
2019-12-03 14:45:37.758009:   [RESULT]   ----------lexical_density: 0.4186046511627907----------
2019-12-03 14:45:37.758162:   [RESULT]   ----------char_per_word: 4.796296296296297----------
2019-12-03 14:45:37.758237:   [RESULT]   ----------Score on KET Limiting: 5.633505598621878----------
2019-12-03 14:45:37.758383:   [RESULT]   ----------type_token_ratio: 0.35175879396984927----------
2019-12-03 14:45:37.758459:   [RESULT]   ----------lexical_density: 0.35175879396984927----------
2019-12-03 14:45:37.758538:   [RESULT]   ----------char_per_word: 5.271428571428571----------
2019-12-03 14:45:37.758668:   [RESULT]   ----------Score on KET Limiting: 5.97494615936827----------
2019-12-03 14:45:37.758803:   [RESULT]   ----------type_token_ratio: 0.3125----------
2019-12-03 14:45:37.758890:   [RESULT]   ----------lexical_density: 0.3125----------
2019-12-03 14:45:37.759168:   [RESULT]   ----------char_per_word: 4.685714285714286----------
2019-12-03 14:45:37.759242:   [RESULT]   ----------Score on KET Limiting: 5.310714285714286----------
2019-12-03 14:45:37.759424:   [RESULT]   ----------type_token_ratio: 0.2914572864321608----------
2019-12-03 14:45:37.759500:   [RESULT]   ----------lexical_density: 0.2914572864321608----------
2019-12-03 14:45:37.759641:   [RESULT]   ----------char_per_word: 5.155172413793103----------
2019-12-03 14:45:37.759716:   [RESULT]   ----------Score on KET Limiting: 5.738086986657425----------
2019-12-03 14:45:37.759891:   [RESULT]   ----------type_token_ratio: 0.41228070175438597----------
2019-12-03 14:45:37.759965:   [RESULT]   ----------lexical_density: 0.41228070175438597----------
2019-12-03 14:45:37.760077:   [RESULT]   ----------char_per_word: 4.957446808510638----------
2019-12-03 14:45:37.760153:   [RESULT]   ----------Score on KET Limiting: 5.78200821201941----------
2019-12-03 14:45:37.760314:   [RESULT]   ----------type_token_ratio: 0.3473684210526316----------
2019-12-03 14:45:37.760415:   [RESULT]   ----------lexical_density: 0.3473684210526316----------
2019-12-03 14:45:37.760493:   [RESULT]   ----------char_per_word: 5.151515151515151----------
2019-12-03 14:45:37.760599:   [RESULT]   ----------Score on KET Limiting: 5.846251993620414----------
2019-12-03 14:45:37.760670:   [DONE  ]   Limiting KET Done
2019-12-03 14:45:37.760778:   [START ]   Start Reading PET
2019-12-03 14:45:37.760961:   [START ]   Preprocess ./dataset/cambridge/PET/1.txt Start
2019-12-03 14:45:37.800376:   [DONE  ]   Preprocess ./dataset/cambridge/PET/1.txt Done
2019-12-03 14:45:37.800576:   [START ]   Preprocess ./dataset/cambridge/PET/2.txt Start
2019-12-03 14:45:37.826793:   [DONE  ]   Preprocess ./dataset/cambridge/PET/2.txt Done
2019-12-03 14:45:37.827014:   [START ]   Preprocess ./dataset/cambridge/PET/3.txt Start
2019-12-03 14:45:37.846001:   [DONE  ]   Preprocess ./dataset/cambridge/PET/3.txt Done
2019-12-03 14:45:37.846192:   [START ]   Preprocess ./dataset/cambridge/PET/4.txt Start
2019-12-03 14:45:37.893587:   [DONE  ]   Preprocess ./dataset/cambridge/PET/4.txt Done
2019-12-03 14:45:37.893795:   [START ]   Preprocess ./dataset/cambridge/PET/5.txt Start
2019-12-03 14:45:37.919408:   [DONE  ]   Preprocess ./dataset/cambridge/PET/5.txt Done
2019-12-03 14:45:37.919567:   [START ]   Preprocess ./dataset/cambridge/PET/6.txt Start
2019-12-03 14:45:37.937722:   [DONE  ]   Preprocess ./dataset/cambridge/PET/6.txt Done
2019-12-03 14:45:37.937987:   [START ]   Preprocess ./dataset/cambridge/PET/8.txt Start
2019-12-03 14:45:37.962849:   [DONE  ]   Preprocess ./dataset/cambridge/PET/8.txt Done
2019-12-03 14:45:37.963028:   [START ]   Preprocess ./dataset/cambridge/PET/9.txt Start
2019-12-03 14:45:37.980996:   [DONE  ]   Preprocess ./dataset/cambridge/PET/9.txt Done
2019-12-03 14:45:37.981154:   [START ]   Preprocess ./dataset/cambridge/PET/10.txt Start
2019-12-03 14:45:38.033772:   [DONE  ]   Preprocess ./dataset/cambridge/PET/10.txt Done
2019-12-03 14:45:38.034011:   [START ]   Preprocess ./dataset/cambridge/PET/11.txt Start
2019-12-03 14:45:38.055516:   [DONE  ]   Preprocess ./dataset/cambridge/PET/11.txt Done
2019-12-03 14:45:38.055746:   [START ]   Preprocess ./dataset/cambridge/PET/14.txt Start
2019-12-03 14:45:38.079602:   [DONE  ]   Preprocess ./dataset/cambridge/PET/14.txt Done
2019-12-03 14:45:38.079783:   [START ]   Preprocess ./dataset/cambridge/PET/15.txt Start
2019-12-03 14:45:38.097112:   [DONE  ]   Preprocess ./dataset/cambridge/PET/15.txt Done
2019-12-03 14:45:38.097201:   [DONE  ]   Read PET Done
2019-12-03 14:45:38.097564:   [START ]   Limiting PET Start
2019-12-03 14:45:38.097744:   [RESULT]   ----------type_token_ratio: 0.40878378378378377----------
2019-12-03 14:45:38.097900:   [RESULT]   ----------lexical_density: 0.40878378378378377----------
2019-12-03 14:45:38.098046:   [RESULT]   ----------char_per_word: 6.231404958677686----------
2019-12-03 14:45:38.098175:   [RESULT]   ----------Score on PET Limiting: 7.0489725262452545----------
2019-12-03 14:45:38.098315:   [RESULT]   ----------type_token_ratio: 0.40096618357487923----------
2019-12-03 14:45:38.098433:   [RESULT]   ----------lexical_density: 0.40096618357487923----------
2019-12-03 14:45:38.098585:   [RESULT]   ----------char_per_word: 5.674698795180723----------
2019-12-03 14:45:38.098723:   [RESULT]   ----------Score on PET Limiting: 6.476631162330482----------
2019-12-03 14:45:38.098821:   [RESULT]   ----------type_token_ratio: 0.423841059602649----------
2019-12-03 14:45:38.098967:   [RESULT]   ----------lexical_density: 0.423841059602649----------
2019-12-03 14:45:38.099095:   [RESULT]   ----------char_per_word: 6.375----------
2019-12-03 14:45:38.099267:   [RESULT]   ----------Score on PET Limiting: 7.222682119205299----------
2019-12-03 14:45:38.099395:   [RESULT]   ----------type_token_ratio: 0.34413965087281795----------
2019-12-03 14:45:38.099538:   [RESULT]   ----------lexical_density: 0.34413965087281795----------
2019-12-03 14:45:38.099651:   [RESULT]   ----------char_per_word: 5.826086956521739----------
2019-12-03 14:45:38.099744:   [RESULT]   ----------Score on PET Limiting: 6.514366258267375----------
2019-12-03 14:45:38.099847:   [RESULT]   ----------type_token_ratio: 0.39545454545454545----------
2019-12-03 14:45:38.100009:   [RESULT]   ----------lexical_density: 0.39545454545454545----------
2019-12-03 14:45:38.100103:   [RESULT]   ----------char_per_word: 5.310344827586207----------
2019-12-03 14:45:38.100188:   [RESULT]   ----------Score on PET Limiting: 6.101253918495298----------
2019-12-03 14:45:38.100281:   [RESULT]   ----------type_token_ratio: 0.46621621621621623----------
2019-12-03 14:45:38.100390:   [RESULT]   ----------lexical_density: 0.46621621621621623----------
2019-12-03 14:45:38.100493:   [RESULT]   ----------char_per_word: 5.6521739130434785----------
2019-12-03 14:45:38.100577:   [RESULT]   ----------Score on PET Limiting: 6.58460634547591----------
2019-12-03 14:45:38.100673:   [RESULT]   ----------type_token_ratio: 0.3177570093457944----------
2019-12-03 14:45:38.100772:   [RESULT]   ----------lexical_density: 0.3177570093457944----------
2019-12-03 14:45:38.100861:   [RESULT]   ----------char_per_word: 5.573529411764706----------
2019-12-03 14:45:38.100965:   [RESULT]   ----------Score on PET Limiting: 6.209043430456295----------
2019-12-03 14:45:38.101059:   [RESULT]   ----------type_token_ratio: 0.4090909090909091----------
2019-12-03 14:45:38.101155:   [RESULT]   ----------lexical_density: 0.4090909090909091----------
2019-12-03 14:45:38.101243:   [RESULT]   ----------char_per_word: 5.4603174603174605----------
2019-12-03 14:45:38.101348:   [RESULT]   ----------Score on PET Limiting: 6.278499278499279----------
2019-12-03 14:45:38.101461:   [RESULT]   ----------type_token_ratio: 0.38913043478260867----------
2019-12-03 14:45:38.101552:   [RESULT]   ----------lexical_density: 0.38913043478260867----------
2019-12-03 14:45:38.101657:   [RESULT]   ----------char_per_word: 6.150837988826815----------
2019-12-03 14:45:38.101744:   [RESULT]   ----------Score on PET Limiting: 6.929098858392033----------
2019-12-03 14:45:38.101839:   [RESULT]   ----------type_token_ratio: 0.4782608695652174----------
2019-12-03 14:45:38.101937:   [RESULT]   ----------lexical_density: 0.4782608695652174----------
2019-12-03 14:45:38.102029:   [RESULT]   ----------char_per_word: 6.113636363636363----------
2019-12-03 14:45:38.102130:   [RESULT]   ----------Score on PET Limiting: 7.070158102766799----------
2019-12-03 14:45:38.102228:   [RESULT]   ----------type_token_ratio: 0.39215686274509803----------
2019-12-03 14:45:38.102314:   [RESULT]   ----------lexical_density: 0.39215686274509803----------
2019-12-03 14:45:38.102404:   [RESULT]   ----------char_per_word: 5.9625----------
2019-12-03 14:45:38.102508:   [RESULT]   ----------Score on PET Limiting: 6.746813725490197----------
2019-12-03 14:45:38.102600:   [RESULT]   ----------type_token_ratio: 0.37748344370860926----------
2019-12-03 14:45:38.102698:   [RESULT]   ----------lexical_density: 0.37748344370860926----------
2019-12-03 14:45:38.102786:   [RESULT]   ----------char_per_word: 5.9298245614035086----------
2019-12-03 14:45:38.102890:   [RESULT]   ----------Score on PET Limiting: 6.684791448820727----------
2019-12-03 14:45:38.102966:   [DONE  ]   Limiting PET Done
2019-12-03 14:45:38.103079:   [START ]   Start Reading FCE
2019-12-03 14:45:38.103201:   [START ]   Preprocess ./dataset/cambridge/FCE/1.txt Start
2019-12-03 14:45:38.187411:   [DONE  ]   Preprocess ./dataset/cambridge/FCE/1.txt Done
2019-12-03 14:45:38.187625:   [START ]   Preprocess ./dataset/cambridge/FCE/2.txt Start
2019-12-03 14:45:38.255826:   [DONE  ]   Preprocess ./dataset/cambridge/FCE/2.txt Done
2019-12-03 14:45:38.256043:   [START ]   Preprocess ./dataset/cambridge/FCE/3.txt Start
2019-12-03 14:45:38.335911:   [DONE  ]   Preprocess ./dataset/cambridge/FCE/3.txt Done
2019-12-03 14:45:38.336136:   [START ]   Preprocess ./dataset/cambridge/FCE/5.txt Start
2019-12-03 14:45:38.410725:   [DONE  ]   Preprocess ./dataset/cambridge/FCE/5.txt Done
2019-12-03 14:45:38.410947:   [START ]   Preprocess ./dataset/cambridge/FCE/6.txt Start
2019-12-03 14:45:38.496490:   [DONE  ]   Preprocess ./dataset/cambridge/FCE/6.txt Done
2019-12-03 14:45:38.496701:   [START ]   Preprocess ./dataset/cambridge/FCE/7.txt Start
2019-12-03 14:45:38.583294:   [DONE  ]   Preprocess ./dataset/cambridge/FCE/7.txt Done
2019-12-03 14:45:38.583581:   [START ]   Preprocess ./dataset/cambridge/FCE/8.txt Start
2019-12-03 14:45:38.655622:   [DONE  ]   Preprocess ./dataset/cambridge/FCE/8.txt Done
2019-12-03 14:45:38.655844:   [START ]   Preprocess ./dataset/cambridge/FCE/10.txt Start
2019-12-03 14:45:38.727527:   [DONE  ]   Preprocess ./dataset/cambridge/FCE/10.txt Done
2019-12-03 14:45:38.727727:   [START ]   Preprocess ./dataset/cambridge/FCE/11.txt Start
2019-12-03 14:45:38.800872:   [DONE  ]   Preprocess ./dataset/cambridge/FCE/11.txt Done
2019-12-03 14:45:38.801084:   [START ]   Preprocess ./dataset/cambridge/FCE/13.txt Start
2019-12-03 14:45:38.878195:   [DONE  ]   Preprocess ./dataset/cambridge/FCE/13.txt Done
2019-12-03 14:45:38.878411:   [START ]   Preprocess ./dataset/cambridge/FCE/14.txt Start
2019-12-03 14:45:38.946340:   [DONE  ]   Preprocess ./dataset/cambridge/FCE/14.txt Done
2019-12-03 14:45:38.946568:   [START ]   Preprocess ./dataset/cambridge/FCE/15.txt Start
2019-12-03 14:45:39.025811:   [DONE  ]   Preprocess ./dataset/cambridge/FCE/15.txt Done
2019-12-03 14:45:39.025951:   [DONE  ]   Read FCE Done
2019-12-03 14:45:39.027204:   [START ]   Limiting FCE Start
2019-12-03 14:45:39.027631:   [RESULT]   ----------type_token_ratio: 0.34206896551724136----------
2019-12-03 14:45:39.027908:   [RESULT]   ----------lexical_density: 0.34206896551724136----------
2019-12-03 14:45:39.028030:   [RESULT]   ----------char_per_word: 5.919354838709677----------
2019-12-03 14:45:39.028256:   [RESULT]   ----------Score on FCE Limiting: 6.60349276974416----------
2019-12-03 14:45:39.028565:   [RESULT]   ----------type_token_ratio: 0.3769911504424779----------
2019-12-03 14:45:39.028858:   [RESULT]   ----------lexical_density: 0.3769911504424779----------
2019-12-03 14:45:39.029125:   [RESULT]   ----------char_per_word: 5.962441314553991----------
2019-12-03 14:45:39.029345:   [RESULT]   ----------Score on FCE Limiting: 6.716423615438947----------
2019-12-03 14:45:39.029665:   [RESULT]   ----------type_token_ratio: 0.36482084690553745----------
2019-12-03 14:45:39.029871:   [RESULT]   ----------lexical_density: 0.36482084690553745----------
2019-12-03 14:45:39.030124:   [RESULT]   ----------char_per_word: 6.401785714285714----------
2019-12-03 14:45:39.030328:   [RESULT]   ----------Score on FCE Limiting: 7.13142740809679----------
2019-12-03 14:45:39.030611:   [RESULT]   ----------type_token_ratio: 0.3643533123028391----------
2019-12-03 14:45:39.030892:   [RESULT]   ----------lexical_density: 0.3643533123028391----------
2019-12-03 14:45:39.031145:   [RESULT]   ----------char_per_word: 6.008658008658009----------
2019-12-03 14:45:39.031350:   [RESULT]   ----------Score on FCE Limiting: 6.737364633263687----------
2019-12-03 14:45:39.031642:   [RESULT]   ----------type_token_ratio: 0.31420765027322406----------
2019-12-03 14:45:39.031848:   [RESULT]   ----------lexical_density: 0.31420765027322406----------
2019-12-03 14:45:39.032100:   [RESULT]   ----------char_per_word: 5.773913043478261----------
2019-12-03 14:45:39.032305:   [RESULT]   ----------Score on FCE Limiting: 6.40232834402471----------
2019-12-03 14:45:39.032597:   [RESULT]   ----------type_token_ratio: 0.29839883551673946----------
2019-12-03 14:45:39.032828:   [RESULT]   ----------lexical_density: 0.29839883551673946----------
2019-12-03 14:45:39.033017:   [RESULT]   ----------char_per_word: 5.590243902439024----------
2019-12-03 14:45:39.033174:   [RESULT]   ----------Score on FCE Limiting: 6.187041573472502----------
2019-12-03 14:45:39.033392:   [RESULT]   ----------type_token_ratio: 0.3258064516129032----------
2019-12-03 14:45:39.033555:   [RESULT]   ----------lexical_density: 0.3258064516129032----------
2019-12-03 14:45:39.033743:   [RESULT]   ----------char_per_word: 6.01980198019802----------
2019-12-03 14:45:39.033898:   [RESULT]   ----------Score on FCE Limiting: 6.671414883423827----------
2019-12-03 14:45:39.034114:   [RESULT]   ----------type_token_ratio: 0.3584----------
2019-12-03 14:45:39.034342:   [RESULT]   ----------lexical_density: 0.3584----------
2019-12-03 14:45:39.034535:   [RESULT]   ----------char_per_word: 5.834821428571429----------
2019-12-03 14:45:39.034692:   [RESULT]   ----------Score on FCE Limiting: 6.551621428571428----------
2019-12-03 14:45:39.034907:   [RESULT]   ----------type_token_ratio: 0.3603174603174603----------
2019-12-03 14:45:39.035062:   [RESULT]   ----------lexical_density: 0.3603174603174603----------
2019-12-03 14:45:39.035253:   [RESULT]   ----------char_per_word: 5.872246696035242----------
2019-12-03 14:45:39.035409:   [RESULT]   ----------Score on FCE Limiting: 6.592881616670162----------
2019-12-03 14:45:39.035613:   [RESULT]   ----------type_token_ratio: 0.36316695352839934----------
2019-12-03 14:45:39.035840:   [RESULT]   ----------lexical_density: 0.36316695352839934----------
2019-12-03 14:45:39.036107:   [RESULT]   ----------char_per_word: 6.312796208530806----------
2019-12-03 14:45:39.036279:   [RESULT]   ----------Score on FCE Limiting: 7.039130115587604----------
2019-12-03 14:45:39.036469:   [RESULT]   ----------type_token_ratio: 0.32685512367491165----------
2019-12-03 14:45:39.036600:   [RESULT]   ----------lexical_density: 0.32685512367491165----------
2019-12-03 14:45:39.036752:   [RESULT]   ----------char_per_word: 6.005405405405406----------
2019-12-03 14:45:39.036875:   [RESULT]   ----------Score on FCE Limiting: 6.659115652755228----------
2019-12-03 14:45:39.037200:   [RESULT]   ----------type_token_ratio: 0.3848396501457726----------
2019-12-03 14:45:39.037403:   [RESULT]   ----------lexical_density: 0.3848396501457726----------
2019-12-03 14:45:39.037581:   [RESULT]   ----------char_per_word: 6.318181818181818----------
2019-12-03 14:45:39.037721:   [RESULT]   ----------Score on FCE Limiting: 7.087861118473363----------
2019-12-03 14:45:39.037864:   [DONE  ]   Limiting FCE Done
2019-12-03 14:45:39.038029:   [START ]   Start Reading CAE
2019-12-03 14:45:39.038232:   [START ]   Preprocess ./dataset/cambridge/CAE/2.txt Start
2019-12-03 14:45:39.098405:   [DONE  ]   Preprocess ./dataset/cambridge/CAE/2.txt Done
2019-12-03 14:45:39.098622:   [START ]   Preprocess ./dataset/cambridge/CAE/3.txt Start
2019-12-03 14:45:39.206900:   [DONE  ]   Preprocess ./dataset/cambridge/CAE/3.txt Done
2019-12-03 14:45:39.207115:   [START ]   Preprocess ./dataset/cambridge/CAE/4.txt Start
2019-12-03 14:45:39.342167:   [DONE  ]   Preprocess ./dataset/cambridge/CAE/4.txt Done
2019-12-03 14:45:39.342391:   [START ]   Preprocess ./dataset/cambridge/CAE/5.txt Start
2019-12-03 14:45:39.424491:   [DONE  ]   Preprocess ./dataset/cambridge/CAE/5.txt Done
2019-12-03 14:45:39.424698:   [START ]   Preprocess ./dataset/cambridge/CAE/6.txt Start
2019-12-03 14:45:39.522023:   [DONE  ]   Preprocess ./dataset/cambridge/CAE/6.txt Done
2019-12-03 14:45:39.522234:   [START ]   Preprocess ./dataset/cambridge/CAE/7.txt Start
2019-12-03 14:45:39.621753:   [DONE  ]   Preprocess ./dataset/cambridge/CAE/7.txt Done
2019-12-03 14:45:39.622050:   [START ]   Preprocess ./dataset/cambridge/CAE/8.txt Start
2019-12-03 14:45:39.740856:   [DONE  ]   Preprocess ./dataset/cambridge/CAE/8.txt Done
2019-12-03 14:45:39.741072:   [START ]   Preprocess ./dataset/cambridge/CAE/9.txt Start
2019-12-03 14:45:39.861280:   [DONE  ]   Preprocess ./dataset/cambridge/CAE/9.txt Done
2019-12-03 14:45:39.861499:   [START ]   Preprocess ./dataset/cambridge/CAE/10.txt Start
2019-12-03 14:45:40.002489:   [DONE  ]   Preprocess ./dataset/cambridge/CAE/10.txt Done
2019-12-03 14:45:40.002709:   [START ]   Preprocess ./dataset/cambridge/CAE/13.txt Start
2019-12-03 14:45:40.110329:   [DONE  ]   Preprocess ./dataset/cambridge/CAE/13.txt Done
2019-12-03 14:45:40.110547:   [START ]   Preprocess ./dataset/cambridge/CAE/14.txt Start
2019-12-03 14:45:40.177231:   [DONE  ]   Preprocess ./dataset/cambridge/CAE/14.txt Done
2019-12-03 14:45:40.177466:   [START ]   Preprocess ./dataset/cambridge/CAE/15.txt Start
2019-12-03 14:45:40.238643:   [DONE  ]   Preprocess ./dataset/cambridge/CAE/15.txt Done
2019-12-03 14:45:40.238769:   [DONE  ]   Read CAE Done
2019-12-03 14:45:40.240125:   [START ]   Limiting CAE Start
2019-12-03 14:45:40.240365:   [RESULT]   ----------type_token_ratio: 0.40040241448692154----------
2019-12-03 14:45:40.240512:   [RESULT]   ----------lexical_density: 0.40040241448692154----------
2019-12-03 14:45:40.240645:   [RESULT]   ----------char_per_word: 6.582914572864322----------
2019-12-03 14:45:40.240737:   [RESULT]   ----------Score on CAE Limiting: 7.3837194018381656----------
2019-12-03 14:45:40.240917:   [RESULT]   ----------type_token_ratio: 0.3070362473347548----------
2019-12-03 14:45:40.241014:   [RESULT]   ----------lexical_density: 0.3070362473347548----------
2019-12-03 14:45:40.241277:   [RESULT]   ----------char_per_word: 5.90625----------
2019-12-03 14:45:40.241385:   [RESULT]   ----------Score on CAE Limiting: 6.520322494669509----------
2019-12-03 14:45:40.241604:   [RESULT]   ----------type_token_ratio: 0.38316151202749144----------
2019-12-03 14:45:40.241810:   [RESULT]   ----------lexical_density: 0.38316151202749144----------
2019-12-03 14:45:40.241960:   [RESULT]   ----------char_per_word: 6.692825112107624----------
2019-12-03 14:45:40.242065:   [RESULT]   ----------Score on CAE Limiting: 7.459148136162607----------
2019-12-03 14:45:40.242204:   [RESULT]   ----------type_token_ratio: 0.3851963746223565----------
2019-12-03 14:45:40.242302:   [RESULT]   ----------lexical_density: 0.3851963746223565----------
2019-12-03 14:45:40.242422:   [RESULT]   ----------char_per_word: 7.070588235294117----------
2019-12-03 14:45:40.242517:   [RESULT]   ----------Score on CAE Limiting: 7.840980984538831----------
2019-12-03 14:45:40.242658:   [RESULT]   ----------type_token_ratio: 0.3471502590673575----------
2019-12-03 14:45:40.242827:   [RESULT]   ----------lexical_density: 0.3471502590673575----------
2019-12-03 14:45:40.242946:   [RESULT]   ----------char_per_word: 6.634328358208955----------
2019-12-03 14:45:40.243040:   [RESULT]   ----------Score on CAE Limiting: 7.328628876343671----------
2019-12-03 14:45:40.243191:   [RESULT]   ----------type_token_ratio: 0.28782707622298065----------
2019-12-03 14:45:40.243285:   [RESULT]   ----------lexical_density: 0.28782707622298065----------
2019-12-03 14:45:40.243462:   [RESULT]   ----------char_per_word: 6.529644268774703----------
2019-12-03 14:45:40.243566:   [RESULT]   ----------Score on CAE Limiting: 7.105298421220665----------
2019-12-03 14:45:40.243839:   [RESULT]   ----------type_token_ratio: 0.39934711643090315----------
2019-12-03 14:45:40.244011:   [RESULT]   ----------lexical_density: 0.39934711643090315----------
2019-12-03 14:45:40.244152:   [RESULT]   ----------char_per_word: 6.4959128065395095----------
2019-12-03 14:45:40.244257:   [RESULT]   ----------Score on CAE Limiting: 7.294607039401315----------
2019-12-03 14:45:40.244413:   [RESULT]   ----------type_token_ratio: 0.3162743091095189----------
2019-12-03 14:45:40.244514:   [RESULT]   ----------lexical_density: 0.3162743091095189----------
2019-12-03 14:45:40.244690:   [RESULT]   ----------char_per_word: 6.090614886731392----------
2019-12-03 14:45:40.244788:   [RESULT]   ----------Score on CAE Limiting: 6.72316350495043----------
2019-12-03 14:45:40.245044:   [RESULT]   ----------type_token_ratio: 0.36108821104699096----------
2019-12-03 14:45:40.245250:   [RESULT]   ----------lexical_density: 0.36108821104699096----------
2019-12-03 14:45:40.245398:   [RESULT]   ----------char_per_word: 5.970319634703197----------
2019-12-03 14:45:40.245509:   [RESULT]   ----------Score on CAE Limiting: 6.692496056797179----------
2019-12-03 14:45:40.245650:   [RESULT]   ----------type_token_ratio: 0.3079625292740047----------
2019-12-03 14:45:40.245746:   [RESULT]   ----------lexical_density: 0.3079625292740047----------
2019-12-03 14:45:40.245863:   [RESULT]   ----------char_per_word: 6.064638783269962----------
2019-12-03 14:45:40.245954:   [RESULT]   ----------Score on CAE Limiting: 6.680563841817972----------
2019-12-03 14:45:40.246073:   [RESULT]   ----------type_token_ratio: 0.36879432624113473----------
2019-12-03 14:45:40.246192:   [RESULT]   ----------lexical_density: 0.36879432624113473----------
2019-12-03 14:45:40.246301:   [RESULT]   ----------char_per_word: 6.586538461538462----------
2019-12-03 14:45:40.246446:   [RESULT]   ----------Score on CAE Limiting: 7.324127114020731----------
2019-12-03 14:45:40.246564:   [RESULT]   ----------type_token_ratio: 0.36278195488721804----------
2019-12-03 14:45:40.246655:   [RESULT]   ----------lexical_density: 0.36278195488721804----------
2019-12-03 14:45:40.246762:   [RESULT]   ----------char_per_word: 6.27979274611399----------
2019-12-03 14:45:40.246849:   [RESULT]   ----------Score on CAE Limiting: 7.005356655888426----------
2019-12-03 14:45:40.246925:   [DONE  ]   Limiting CAE Done
2019-12-03 14:45:40.247040:   [START ]   Start Reading CPE
2019-12-03 14:45:40.247168:   [START ]   Preprocess ./dataset/cambridge/CPE/1.txt Start
2019-12-03 14:45:40.314541:   [DONE  ]   Preprocess ./dataset/cambridge/CPE/1.txt Done
2019-12-03 14:45:40.314837:   [START ]   Preprocess ./dataset/cambridge/CPE/2.txt Start
2019-12-03 14:45:40.426757:   [DONE  ]   Preprocess ./dataset/cambridge/CPE/2.txt Done
2019-12-03 14:45:40.426964:   [START ]   Preprocess ./dataset/cambridge/CPE/3.txt Start
2019-12-03 14:45:40.550808:   [DONE  ]   Preprocess ./dataset/cambridge/CPE/3.txt Done
2019-12-03 14:45:40.551036:   [START ]   Preprocess ./dataset/cambridge/CPE/4.txt Start
2019-12-03 14:45:40.625339:   [DONE  ]   Preprocess ./dataset/cambridge/CPE/4.txt Done
2019-12-03 14:45:40.625557:   [START ]   Preprocess ./dataset/cambridge/CPE/7.txt Start
2019-12-03 14:45:40.734975:   [DONE  ]   Preprocess ./dataset/cambridge/CPE/7.txt Done
2019-12-03 14:45:40.735184:   [START ]   Preprocess ./dataset/cambridge/CPE/8.txt Start
2019-12-03 14:45:40.833368:   [DONE  ]   Preprocess ./dataset/cambridge/CPE/8.txt Done
2019-12-03 14:45:40.833581:   [START ]   Preprocess ./dataset/cambridge/CPE/9.txt Start
2019-12-03 14:45:40.896841:   [DONE  ]   Preprocess ./dataset/cambridge/CPE/9.txt Done
2019-12-03 14:45:40.897064:   [START ]   Preprocess ./dataset/cambridge/CPE/10.txt Start
2019-12-03 14:45:40.997630:   [DONE  ]   Preprocess ./dataset/cambridge/CPE/10.txt Done
2019-12-03 14:45:40.997856:   [START ]   Preprocess ./dataset/cambridge/CPE/11.txt Start
2019-12-03 14:45:41.111920:   [DONE  ]   Preprocess ./dataset/cambridge/CPE/11.txt Done
2019-12-03 14:45:41.112141:   [START ]   Preprocess ./dataset/cambridge/CPE/13.txt Start
2019-12-03 14:45:41.172293:   [DONE  ]   Preprocess ./dataset/cambridge/CPE/13.txt Done
2019-12-03 14:45:41.172494:   [START ]   Preprocess ./dataset/cambridge/CPE/14.txt Start
2019-12-03 14:45:41.276632:   [DONE  ]   Preprocess ./dataset/cambridge/CPE/14.txt Done
2019-12-03 14:45:41.276857:   [START ]   Preprocess ./dataset/cambridge/CPE/15.txt Start
2019-12-03 14:45:41.375481:   [DONE  ]   Preprocess ./dataset/cambridge/CPE/15.txt Done
2019-12-03 14:45:41.375629:   [DONE  ]   Read CPE Done
2019-12-03 14:45:41.376904:   [START ]   Limiting CPE Start
2019-12-03 14:45:41.377156:   [RESULT]   ----------type_token_ratio: 0.42857142857142855----------
2019-12-03 14:45:41.377294:   [RESULT]   ----------lexical_density: 0.42857142857142855----------
2019-12-03 14:45:41.377430:   [RESULT]   ----------char_per_word: 6.173913043478261----------
2019-12-03 14:45:41.377541:   [RESULT]   ----------Score on CPE Limiting: 7.031055900621118----------
2019-12-03 14:45:41.377717:   [RESULT]   ----------type_token_ratio: 0.4033018867924528----------
2019-12-03 14:45:41.377889:   [RESULT]   ----------lexical_density: 0.4033018867924528----------
2019-12-03 14:45:41.378042:   [RESULT]   ----------char_per_word: 6.637426900584796----------
2019-12-03 14:45:41.378159:   [RESULT]   ----------Score on CPE Limiting: 7.444030674169701----------
2019-12-03 14:45:41.378365:   [RESULT]   ----------type_token_ratio: 0.3888888888888889----------
2019-12-03 14:45:41.378482:   [RESULT]   ----------lexical_density: 0.3888888888888889----------
2019-12-03 14:45:41.378661:   [RESULT]   ----------char_per_word: 6.466165413533835----------
2019-12-03 14:45:41.378781:   [RESULT]   ----------Score on CPE Limiting: 7.2439431913116135----------
2019-12-03 14:45:41.378928:   [RESULT]   ----------type_token_ratio: 0.39679715302491103----------
2019-12-03 14:45:41.379109:   [RESULT]   ----------lexical_density: 0.39679715302491103----------
2019-12-03 14:45:41.379242:   [RESULT]   ----------char_per_word: 7.044843049327354----------
2019-12-03 14:45:41.379345:   [RESULT]   ----------Score on CPE Limiting: 7.8384373553771765----------
2019-12-03 14:45:41.379586:   [RESULT]   ----------type_token_ratio: 0.3824152542372881----------
2019-12-03 14:45:41.379739:   [RESULT]   ----------lexical_density: 0.3824152542372881----------
2019-12-03 14:45:41.379907:   [RESULT]   ----------char_per_word: 5.966759002770083----------
2019-12-03 14:45:41.380006:   [RESULT]   ----------Score on CPE Limiting: 6.7315895112446595----------
2019-12-03 14:45:41.380239:   [RESULT]   ----------type_token_ratio: 0.27471116816431324----------
2019-12-03 14:45:41.380336:   [RESULT]   ----------lexical_density: 0.27471116816431324----------
2019-12-03 14:45:41.380446:   [RESULT]   ----------char_per_word: 6.242990654205608----------
2019-12-03 14:45:41.380537:   [RESULT]   ----------Score on CPE Limiting: 6.792412990534235----------
2019-12-03 14:45:41.380653:   [RESULT]   ----------type_token_ratio: 0.4012875536480687----------
2019-12-03 14:45:41.380740:   [RESULT]   ----------lexical_density: 0.4012875536480687----------
2019-12-03 14:45:41.380846:   [RESULT]   ----------char_per_word: 6.106951871657754----------
2019-12-03 14:45:41.380953:   [RESULT]   ----------Score on CPE Limiting: 6.909526978953892----------
2019-12-03 14:45:41.381106:   [RESULT]   ----------type_token_ratio: 0.43367935409457903----------
2019-12-03 14:45:41.381226:   [RESULT]   ----------lexical_density: 0.43367935409457903----------
2019-12-03 14:45:41.381359:   [RESULT]   ----------char_per_word: 6.473404255319149----------
2019-12-03 14:45:41.381513:   [RESULT]   ----------Score on CPE Limiting: 7.340762963508308----------
2019-12-03 14:45:41.381680:   [RESULT]   ----------type_token_ratio: 0.38571428571428573----------
2019-12-03 14:45:41.381787:   [RESULT]   ----------lexical_density: 0.38571428571428573----------
2019-12-03 14:45:41.381926:   [RESULT]   ----------char_per_word: 6.64957264957265----------
2019-12-03 14:45:41.382027:   [RESULT]   ----------Score on CPE Limiting: 7.421001221001222----------
2019-12-03 14:45:41.382203:   [RESULT]   ----------type_token_ratio: 0.4451901565995526----------
2019-12-03 14:45:41.382379:   [RESULT]   ----------lexical_density: 0.4451901565995526----------
2019-12-03 14:45:41.382494:   [RESULT]   ----------char_per_word: 7.015075376884422----------
2019-12-03 14:45:41.382587:   [RESULT]   ----------Score on CPE Limiting: 7.905455690083526----------
2019-12-03 14:45:41.382739:   [RESULT]   ----------type_token_ratio: 0.35353535353535354----------
2019-12-03 14:45:41.382841:   [RESULT]   ----------lexical_density: 0.35353535353535354----------
2019-12-03 14:45:41.382968:   [RESULT]   ----------char_per_word: 6.095238095238095----------
2019-12-03 14:45:41.383066:   [RESULT]   ----------Score on CPE Limiting: 6.8023088023088025----------
2019-12-03 14:45:41.383210:   [RESULT]   ----------type_token_ratio: 0.33623910336239105----------
2019-12-03 14:45:41.383379:   [RESULT]   ----------lexical_density: 0.33623910336239105----------
2019-12-03 14:45:41.383496:   [RESULT]   ----------char_per_word: 6.555555555555555----------
2019-12-03 14:45:41.383587:   [RESULT]   ----------Score on CPE Limiting: 7.228033762280338----------
2019-12-03 14:45:41.383664:   [DONE  ]   Limiting CPE Done
2019-12-03 14:45:41.383893:   [DONE  ]   Traning data Done
2019-12-03 14:45:41.383967:   [START ]   Test Start
2019-12-03 14:45:41.384054:   [START ]   Reading Test ./dataset/cambridge/KET/9.txt Start
2019-12-03 14:45:41.384213:   [START ]   Preprocess ./dataset/cambridge/KET/9.txt Start
2019-12-03 14:45:41.397936:   [DONE  ]   Preprocess ./dataset/cambridge/KET/9.txt Done
2019-12-03 14:45:41.398070:   [DONE  ]   Reading Test ./dataset/cambridge/KET/9.txt Done
2019-12-03 14:45:41.398481:   [START ]   Scoring KET Start
2019-12-03 14:59:15.978122:   [INFO  ]   -----------------------------------------------------------------------------
2019-12-03 14:59:15.978729:   [INFO  ]   Hello world!
2019-12-03 14:59:15.978889:   [INFO  ]   Start main processing!
2019-12-03 14:59:15.978987:   [START ]   Start Prepare Traning data
2019-12-03 14:59:15.979068:   [START ]   Initialize parameters Start
2019-12-03 14:59:15.979228:   [DONE  ]   Initialize parameters Done
2019-12-03 14:59:15.979393:   [START ]   Start setup FOLD
2019-12-03 14:59:15.979509:   [START ]   Setup FOLD Done
2019-12-03 14:59:15.979589:   [START ]   Start Training data
2019-12-03 14:59:15.979668:   [START ]   Start Reading KET
2019-12-03 14:59:15.980550:   [START ]   Preprocess ./dataset/cambridge/KET/1.txt Start
2019-12-03 14:59:17.398953:   [DONE  ]   Preprocess ./dataset/cambridge/KET/1.txt Done
2019-12-03 14:59:17.401227:   [START ]   Preprocess ./dataset/cambridge/KET/2.txt Start
2019-12-03 14:59:17.419523:   [DONE  ]   Preprocess ./dataset/cambridge/KET/2.txt Done
2019-12-03 14:59:17.420729:   [START ]   Preprocess ./dataset/cambridge/KET/3.txt Start
2019-12-03 14:59:17.433554:   [DONE  ]   Preprocess ./dataset/cambridge/KET/3.txt Done
2019-12-03 14:59:17.434360:   [START ]   Preprocess ./dataset/cambridge/KET/4.txt Start
2019-12-03 14:59:17.455471:   [DONE  ]   Preprocess ./dataset/cambridge/KET/4.txt Done
2019-12-03 14:59:17.456404:   [START ]   Preprocess ./dataset/cambridge/KET/5.txt Start
2019-12-03 14:59:17.476528:   [DONE  ]   Preprocess ./dataset/cambridge/KET/5.txt Done
2019-12-03 14:59:17.478303:   [START ]   Preprocess ./dataset/cambridge/KET/6.txt Start
2019-12-03 14:59:17.497774:   [DONE  ]   Preprocess ./dataset/cambridge/KET/6.txt Done
2019-12-03 14:59:17.499022:   [START ]   Preprocess ./dataset/cambridge/KET/7.txt Start
2019-12-03 14:59:17.522226:   [DONE  ]   Preprocess ./dataset/cambridge/KET/7.txt Done
2019-12-03 14:59:17.523027:   [START ]   Preprocess ./dataset/cambridge/KET/8.txt Start
2019-12-03 14:59:17.537817:   [DONE  ]   Preprocess ./dataset/cambridge/KET/8.txt Done
2019-12-03 14:59:17.538485:   [START ]   Preprocess ./dataset/cambridge/KET/11.txt Start
2019-12-03 14:59:17.559716:   [DONE  ]   Preprocess ./dataset/cambridge/KET/11.txt Done
2019-12-03 14:59:17.560412:   [START ]   Preprocess ./dataset/cambridge/KET/12.txt Start
2019-12-03 14:59:17.574197:   [DONE  ]   Preprocess ./dataset/cambridge/KET/12.txt Done
2019-12-03 14:59:17.574497:   [START ]   Preprocess ./dataset/cambridge/KET/13.txt Start
2019-12-03 14:59:17.597278:   [DONE  ]   Preprocess ./dataset/cambridge/KET/13.txt Done
2019-12-03 14:59:17.597653:   [START ]   Preprocess ./dataset/cambridge/KET/14.txt Start
2019-12-03 14:59:17.617771:   [DONE  ]   Preprocess ./dataset/cambridge/KET/14.txt Done
2019-12-03 14:59:17.617968:   [DONE  ]   Read KET Done
2019-12-03 14:59:17.618079:   [START ]   Limiting KET Start
2019-12-03 14:59:17.618679:   [RESULT]   ----------type_token_ratio: 0.4----------
2019-12-03 14:59:17.618789:   [RESULT]   ----------lexical_density: 0.4----------
2019-12-03 14:59:17.618943:   [RESULT]   ----------char_per_word: 5.355263157894737----------
2019-12-03 14:59:17.619042:   [RESULT]   ----------Score on KET Limiting: 6.155263157894738----------
2019-12-03 14:59:17.619256:   [RESULT]   ----------type_token_ratio: 0.35294117647058826----------
2019-12-03 14:59:17.619366:   [RESULT]   ----------lexical_density: 0.35294117647058826----------
2019-12-03 14:59:17.619485:   [RESULT]   ----------char_per_word: 4.880952380952381----------
2019-12-03 14:59:17.619680:   [RESULT]   ----------Score on KET Limiting: 5.586834733893557----------
2019-12-03 14:59:17.620220:   [RESULT]   ----------type_token_ratio: 0.5204081632653061----------
2019-12-03 14:59:17.620328:   [RESULT]   ----------lexical_density: 0.5204081632653061----------
2019-12-03 14:59:17.620473:   [RESULT]   ----------char_per_word: 4.882352941176471----------
2019-12-03 14:59:17.620565:   [RESULT]   ----------Score on KET Limiting: 5.923169267707083----------
2019-12-03 14:59:17.631735:   [RESULT]   ----------type_token_ratio: 0.3193717277486911----------
2019-12-03 14:59:17.631934:   [RESULT]   ----------lexical_density: 0.3193717277486911----------
2019-12-03 14:59:17.632230:   [RESULT]   ----------char_per_word: 5.262295081967213----------
2019-12-03 14:59:17.632320:   [RESULT]   ----------Score on KET Limiting: 5.901038537464595----------
2019-12-03 14:59:17.632789:   [RESULT]   ----------type_token_ratio: 0.42016806722689076----------
2019-12-03 14:59:17.632960:   [RESULT]   ----------lexical_density: 0.42016806722689076----------
2019-12-03 14:59:17.633256:   [RESULT]   ----------char_per_word: 5.18----------
2019-12-03 14:59:17.633388:   [RESULT]   ----------Score on KET Limiting: 6.020336134453782----------
2019-12-03 14:59:17.633728:   [RESULT]   ----------type_token_ratio: 0.38596491228070173----------
2019-12-03 14:59:17.634373:   [RESULT]   ----------lexical_density: 0.38596491228070173----------
2019-12-03 14:59:17.635233:   [RESULT]   ----------char_per_word: 4.931818181818182----------
2019-12-03 14:59:17.635896:   [RESULT]   ----------Score on KET Limiting: 5.703748006379586----------
2019-12-03 14:59:17.637490:   [RESULT]   ----------type_token_ratio: 0.38860103626943004----------
2019-12-03 14:59:17.637753:   [RESULT]   ----------lexical_density: 0.38860103626943004----------
2019-12-03 14:59:17.637881:   [RESULT]   ----------char_per_word: 5.56----------
2019-12-03 14:59:17.638143:   [RESULT]   ----------Score on KET Limiting: 6.337202072538859----------
2019-12-03 14:59:17.638776:   [RESULT]   ----------type_token_ratio: 0.4186046511627907----------
2019-12-03 14:59:17.638990:   [RESULT]   ----------lexical_density: 0.4186046511627907----------
2019-12-03 14:59:17.639269:   [RESULT]   ----------char_per_word: 4.796296296296297----------
2019-12-03 14:59:17.639373:   [RESULT]   ----------Score on KET Limiting: 5.633505598621878----------
2019-12-03 14:59:17.639829:   [RESULT]   ----------type_token_ratio: 0.31932773109243695----------
2019-12-03 14:59:17.639946:   [RESULT]   ----------lexical_density: 0.31932773109243695----------
2019-12-03 14:59:17.640109:   [RESULT]   ----------char_per_word: 5.526315789473684----------
2019-12-03 14:59:17.640201:   [RESULT]   ----------Score on KET Limiting: 6.1649712516585575----------
2019-12-03 14:59:17.640520:   [RESULT]   ----------type_token_ratio: 0.3125----------
2019-12-03 14:59:17.640618:   [RESULT]   ----------lexical_density: 0.3125----------
2019-12-03 14:59:17.640865:   [RESULT]   ----------char_per_word: 4.685714285714286----------
2019-12-03 14:59:17.640951:   [RESULT]   ----------Score on KET Limiting: 5.310714285714286----------
2019-12-03 14:59:17.641335:   [RESULT]   ----------type_token_ratio: 0.2914572864321608----------
2019-12-03 14:59:17.641433:   [RESULT]   ----------lexical_density: 0.2914572864321608----------
2019-12-03 14:59:17.641616:   [RESULT]   ----------char_per_word: 5.155172413793103----------
2019-12-03 14:59:17.641707:   [RESULT]   ----------Score on KET Limiting: 5.738086986657425----------
2019-12-03 14:59:17.641962:   [RESULT]   ----------type_token_ratio: 0.41228070175438597----------
2019-12-03 14:59:17.642095:   [RESULT]   ----------lexical_density: 0.41228070175438597----------
2019-12-03 14:59:17.642293:   [RESULT]   ----------char_per_word: 4.957446808510638----------
2019-12-03 14:59:17.642370:   [RESULT]   ----------Score on KET Limiting: 5.78200821201941----------
2019-12-03 14:59:17.642528:   [DONE  ]   Limiting KET Done
2019-12-03 14:59:17.642603:   [START ]   Start Reading PET
2019-12-03 14:59:17.643293:   [START ]   Preprocess ./dataset/cambridge/PET/2.txt Start
2019-12-03 14:59:17.679075:   [DONE  ]   Preprocess ./dataset/cambridge/PET/2.txt Done
2019-12-03 14:59:17.680284:   [START ]   Preprocess ./dataset/cambridge/PET/3.txt Start
2019-12-03 14:59:17.697983:   [DONE  ]   Preprocess ./dataset/cambridge/PET/3.txt Done
2019-12-03 14:59:17.699425:   [START ]   Preprocess ./dataset/cambridge/PET/4.txt Start
2019-12-03 14:59:17.752378:   [DONE  ]   Preprocess ./dataset/cambridge/PET/4.txt Done
2019-12-03 14:59:17.753977:   [START ]   Preprocess ./dataset/cambridge/PET/5.txt Start
2019-12-03 14:59:17.780667:   [DONE  ]   Preprocess ./dataset/cambridge/PET/5.txt Done
2019-12-03 14:59:17.781511:   [START ]   Preprocess ./dataset/cambridge/PET/6.txt Start
2019-12-03 14:59:17.798463:   [DONE  ]   Preprocess ./dataset/cambridge/PET/6.txt Done
2019-12-03 14:59:17.799867:   [START ]   Preprocess ./dataset/cambridge/PET/8.txt Start
2019-12-03 14:59:17.831333:   [DONE  ]   Preprocess ./dataset/cambridge/PET/8.txt Done
2019-12-03 14:59:17.831878:   [START ]   Preprocess ./dataset/cambridge/PET/9.txt Start
2019-12-03 14:59:17.857617:   [DONE  ]   Preprocess ./dataset/cambridge/PET/9.txt Done
2019-12-03 14:59:17.858273:   [START ]   Preprocess ./dataset/cambridge/PET/10.txt Start
2019-12-03 14:59:17.915979:   [DONE  ]   Preprocess ./dataset/cambridge/PET/10.txt Done
2019-12-03 14:59:17.916989:   [START ]   Preprocess ./dataset/cambridge/PET/11.txt Start
2019-12-03 14:59:17.938247:   [DONE  ]   Preprocess ./dataset/cambridge/PET/11.txt Done
2019-12-03 14:59:17.938589:   [START ]   Preprocess ./dataset/cambridge/PET/13.txt Start
2019-12-03 14:59:17.982284:   [DONE  ]   Preprocess ./dataset/cambridge/PET/13.txt Done
2019-12-03 14:59:17.982641:   [START ]   Preprocess ./dataset/cambridge/PET/14.txt Start
2019-12-03 14:59:18.005797:   [DONE  ]   Preprocess ./dataset/cambridge/PET/14.txt Done
2019-12-03 14:59:18.006209:   [START ]   Preprocess ./dataset/cambridge/PET/15.txt Start
2019-12-03 14:59:18.024248:   [DONE  ]   Preprocess ./dataset/cambridge/PET/15.txt Done
2019-12-03 14:59:18.024345:   [DONE  ]   Read PET Done
2019-12-03 14:59:18.024871:   [START ]   Limiting PET Start
2019-12-03 14:59:18.025659:   [RESULT]   ----------type_token_ratio: 0.40096618357487923----------
2019-12-03 14:59:18.025797:   [RESULT]   ----------lexical_density: 0.40096618357487923----------
2019-12-03 14:59:18.025953:   [RESULT]   ----------char_per_word: 5.674698795180723----------
2019-12-03 14:59:18.026082:   [RESULT]   ----------Score on PET Limiting: 6.476631162330482----------
2019-12-03 14:59:18.026480:   [RESULT]   ----------type_token_ratio: 0.423841059602649----------
2019-12-03 14:59:18.026577:   [RESULT]   ----------lexical_density: 0.423841059602649----------
2019-12-03 14:59:18.026866:   [RESULT]   ----------char_per_word: 6.375----------
2019-12-03 14:59:18.027029:   [RESULT]   ----------Score on PET Limiting: 7.222682119205299----------
2019-12-03 14:59:18.038320:   [RESULT]   ----------type_token_ratio: 0.34413965087281795----------
2019-12-03 14:59:18.038563:   [RESULT]   ----------lexical_density: 0.34413965087281795----------
2019-12-03 14:59:18.038830:   [RESULT]   ----------char_per_word: 5.826086956521739----------
2019-12-03 14:59:18.039271:   [RESULT]   ----------Score on PET Limiting: 6.514366258267375----------
2019-12-03 14:59:18.040539:   [RESULT]   ----------type_token_ratio: 0.39545454545454545----------
2019-12-03 14:59:18.041404:   [RESULT]   ----------lexical_density: 0.39545454545454545----------
2019-12-03 14:59:18.042131:   [RESULT]   ----------char_per_word: 5.310344827586207----------
2019-12-03 14:59:18.042787:   [RESULT]   ----------Score on PET Limiting: 6.101253918495298----------
2019-12-03 14:59:18.043783:   [RESULT]   ----------type_token_ratio: 0.46621621621621623----------
2019-12-03 14:59:18.044041:   [RESULT]   ----------lexical_density: 0.46621621621621623----------
2019-12-03 14:59:18.044245:   [RESULT]   ----------char_per_word: 5.6521739130434785----------
2019-12-03 14:59:18.044484:   [RESULT]   ----------Score on PET Limiting: 6.58460634547591----------
2019-12-03 14:59:18.058029:   [RESULT]   ----------type_token_ratio: 0.3177570093457944----------
2019-12-03 14:59:18.058998:   [RESULT]   ----------lexical_density: 0.3177570093457944----------
2019-12-03 14:59:18.059195:   [RESULT]   ----------char_per_word: 5.573529411764706----------
2019-12-03 14:59:18.059536:   [RESULT]   ----------Score on PET Limiting: 6.209043430456295----------
2019-12-03 14:59:18.069245:   [RESULT]   ----------type_token_ratio: 0.4090909090909091----------
2019-12-03 14:59:18.069555:   [RESULT]   ----------lexical_density: 0.4090909090909091----------
2019-12-03 14:59:18.069755:   [RESULT]   ----------char_per_word: 5.4603174603174605----------
2019-12-03 14:59:18.070085:   [RESULT]   ----------Score on PET Limiting: 6.278499278499279----------
2019-12-03 14:59:18.089384:   [RESULT]   ----------type_token_ratio: 0.38913043478260867----------
2019-12-03 14:59:18.089695:   [RESULT]   ----------lexical_density: 0.38913043478260867----------
2019-12-03 14:59:18.089867:   [RESULT]   ----------char_per_word: 6.150837988826815----------
2019-12-03 14:59:18.089999:   [RESULT]   ----------Score on PET Limiting: 6.929098858392033----------
2019-12-03 14:59:18.107516:   [RESULT]   ----------type_token_ratio: 0.4782608695652174----------
2019-12-03 14:59:18.108598:   [RESULT]   ----------lexical_density: 0.4782608695652174----------
2019-12-03 14:59:18.109556:   [RESULT]   ----------char_per_word: 6.113636363636363----------
2019-12-03 14:59:18.110302:   [RESULT]   ----------Score on PET Limiting: 7.070158102766799----------
2019-12-03 14:59:18.123509:   [RESULT]   ----------type_token_ratio: 0.3917808219178082----------
2019-12-03 14:59:18.124244:   [RESULT]   ----------lexical_density: 0.3917808219178082----------
2019-12-03 14:59:18.125275:   [RESULT]   ----------char_per_word: 5.916083916083916----------
2019-12-03 14:59:18.125734:   [RESULT]   ----------Score on PET Limiting: 6.699645559919532----------
2019-12-03 14:59:18.126715:   [RESULT]   ----------type_token_ratio: 0.39215686274509803----------
2019-12-03 14:59:18.126920:   [RESULT]   ----------lexical_density: 0.39215686274509803----------
2019-12-03 14:59:18.127063:   [RESULT]   ----------char_per_word: 5.9625----------
2019-12-03 14:59:18.127175:   [RESULT]   ----------Score on PET Limiting: 6.746813725490197----------
2019-12-03 14:59:18.137688:   [RESULT]   ----------type_token_ratio: 0.37748344370860926----------
2019-12-03 14:59:18.137846:   [RESULT]   ----------lexical_density: 0.37748344370860926----------
2019-12-03 14:59:18.138035:   [RESULT]   ----------char_per_word: 5.9298245614035086----------
2019-12-03 14:59:18.138141:   [RESULT]   ----------Score on PET Limiting: 6.684791448820727----------
2019-12-03 14:59:18.138317:   [DONE  ]   Limiting PET Done
2019-12-03 14:59:18.138415:   [START ]   Start Reading FCE
2019-12-03 14:59:18.139125:   [START ]   Preprocess ./dataset/cambridge/FCE/1.txt Start
2019-12-03 14:59:18.225773:   [DONE  ]   Preprocess ./dataset/cambridge/FCE/1.txt Done
2019-12-03 14:59:18.227229:   [START ]   Preprocess ./dataset/cambridge/FCE/2.txt Start
2019-12-03 14:59:18.299189:   [DONE  ]   Preprocess ./dataset/cambridge/FCE/2.txt Done
2019-12-03 14:59:18.300823:   [START ]   Preprocess ./dataset/cambridge/FCE/3.txt Start
2019-12-03 14:59:18.382366:   [DONE  ]   Preprocess ./dataset/cambridge/FCE/3.txt Done
2019-12-03 14:59:18.383917:   [START ]   Preprocess ./dataset/cambridge/FCE/4.txt Start
2019-12-03 14:59:18.463222:   [DONE  ]   Preprocess ./dataset/cambridge/FCE/4.txt Done
2019-12-03 14:59:18.464968:   [START ]   Preprocess ./dataset/cambridge/FCE/5.txt Start
2019-12-03 14:59:18.542678:   [DONE  ]   Preprocess ./dataset/cambridge/FCE/5.txt Done
2019-12-03 14:59:18.544225:   [START ]   Preprocess ./dataset/cambridge/FCE/6.txt Start
2019-12-03 14:59:18.627783:   [DONE  ]   Preprocess ./dataset/cambridge/FCE/6.txt Done
2019-12-03 14:59:18.629027:   [START ]   Preprocess ./dataset/cambridge/FCE/7.txt Start
2019-12-03 14:59:18.708372:   [DONE  ]   Preprocess ./dataset/cambridge/FCE/7.txt Done
2019-12-03 14:59:18.709479:   [START ]   Preprocess ./dataset/cambridge/FCE/8.txt Start
2019-12-03 14:59:18.789151:   [DONE  ]   Preprocess ./dataset/cambridge/FCE/8.txt Done
2019-12-03 14:59:18.790033:   [START ]   Preprocess ./dataset/cambridge/FCE/9.txt Start
2019-12-03 14:59:18.857671:   [DONE  ]   Preprocess ./dataset/cambridge/FCE/9.txt Done
2019-12-03 14:59:18.858736:   [START ]   Preprocess ./dataset/cambridge/FCE/13.txt Start
2019-12-03 14:59:18.967540:   [DONE  ]   Preprocess ./dataset/cambridge/FCE/13.txt Done
2019-12-03 14:59:18.968067:   [START ]   Preprocess ./dataset/cambridge/FCE/14.txt Start
2019-12-03 14:59:19.040594:   [DONE  ]   Preprocess ./dataset/cambridge/FCE/14.txt Done
2019-12-03 14:59:19.041957:   [START ]   Preprocess ./dataset/cambridge/FCE/15.txt Start
2019-12-03 14:59:19.121213:   [DONE  ]   Preprocess ./dataset/cambridge/FCE/15.txt Done
2019-12-03 14:59:19.121357:   [DONE  ]   Read FCE Done
2019-12-03 14:59:19.122598:   [START ]   Limiting FCE Start
2019-12-03 14:59:19.124446:   [RESULT]   ----------type_token_ratio: 0.34206896551724136----------
2019-12-03 14:59:19.124728:   [RESULT]   ----------lexical_density: 0.34206896551724136----------
2019-12-03 14:59:19.125009:   [RESULT]   ----------char_per_word: 5.919354838709677----------
2019-12-03 14:59:19.125098:   [RESULT]   ----------Score on FCE Limiting: 6.60349276974416----------
2019-12-03 14:59:19.139070:   [RESULT]   ----------type_token_ratio: 0.3769911504424779----------
2019-12-03 14:59:19.139318:   [RESULT]   ----------lexical_density: 0.3769911504424779----------
2019-12-03 14:59:19.139536:   [RESULT]   ----------char_per_word: 5.962441314553991----------
2019-12-03 14:59:19.139725:   [RESULT]   ----------Score on FCE Limiting: 6.716423615438947----------
2019-12-03 14:59:19.141859:   [RESULT]   ----------type_token_ratio: 0.36482084690553745----------
2019-12-03 14:59:19.143443:   [RESULT]   ----------lexical_density: 0.36482084690553745----------
2019-12-03 14:59:19.144242:   [RESULT]   ----------char_per_word: 6.401785714285714----------
2019-12-03 14:59:19.144586:   [RESULT]   ----------Score on FCE Limiting: 7.13142740809679----------
2019-12-03 14:59:19.159851:   [RESULT]   ----------type_token_ratio: 0.3472429210134128----------
2019-12-03 14:59:19.161041:   [RESULT]   ----------lexical_density: 0.3472429210134128----------
2019-12-03 14:59:19.161810:   [RESULT]   ----------char_per_word: 5.896995708154506----------
2019-12-03 14:59:19.161980:   [RESULT]   ----------Score on FCE Limiting: 6.5914815501813315----------
2019-12-03 14:59:19.197271:   [RESULT]   ----------type_token_ratio: 0.3643533123028391----------
2019-12-03 14:59:19.197501:   [RESULT]   ----------lexical_density: 0.3643533123028391----------
2019-12-03 14:59:19.197741:   [RESULT]   ----------char_per_word: 6.008658008658009----------
2019-12-03 14:59:19.197850:   [RESULT]   ----------Score on FCE Limiting: 6.737364633263687----------
2019-12-03 14:59:19.199997:   [RESULT]   ----------type_token_ratio: 0.31420765027322406----------
2019-12-03 14:59:19.200148:   [RESULT]   ----------lexical_density: 0.31420765027322406----------
2019-12-03 14:59:19.200279:   [RESULT]   ----------char_per_word: 5.773913043478261----------
2019-12-03 14:59:19.200380:   [RESULT]   ----------Score on FCE Limiting: 6.40232834402471----------
2019-12-03 14:59:19.201674:   [RESULT]   ----------type_token_ratio: 0.29839883551673946----------
2019-12-03 14:59:19.201808:   [RESULT]   ----------lexical_density: 0.29839883551673946----------
2019-12-03 14:59:19.201939:   [RESULT]   ----------char_per_word: 5.590243902439024----------
2019-12-03 14:59:19.202040:   [RESULT]   ----------Score on FCE Limiting: 6.187041573472502----------
2019-12-03 14:59:19.204242:   [RESULT]   ----------type_token_ratio: 0.3258064516129032----------
2019-12-03 14:59:19.204391:   [RESULT]   ----------lexical_density: 0.3258064516129032----------
2019-12-03 14:59:19.204519:   [RESULT]   ----------char_per_word: 6.01980198019802----------
2019-12-03 14:59:19.204620:   [RESULT]   ----------Score on FCE Limiting: 6.671414883423827----------
2019-12-03 14:59:19.206104:   [RESULT]   ----------type_token_ratio: 0.38860971524288107----------
2019-12-03 14:59:19.206221:   [RESULT]   ----------lexical_density: 0.38860971524288107----------
2019-12-03 14:59:19.206360:   [RESULT]   ----------char_per_word: 6.120689655172414----------
2019-12-03 14:59:19.206458:   [RESULT]   ----------Score on FCE Limiting: 6.897909085658175----------
2019-12-03 14:59:19.207951:   [RESULT]   ----------type_token_ratio: 0.36316695352839934----------
2019-12-03 14:59:19.208065:   [RESULT]   ----------lexical_density: 0.36316695352839934----------
2019-12-03 14:59:19.208191:   [RESULT]   ----------char_per_word: 6.312796208530806----------
2019-12-03 14:59:19.208280:   [RESULT]   ----------Score on FCE Limiting: 7.039130115587604----------
2019-12-03 14:59:19.210013:   [RESULT]   ----------type_token_ratio: 0.32685512367491165----------
2019-12-03 14:59:19.210130:   [RESULT]   ----------lexical_density: 0.32685512367491165----------
2019-12-03 14:59:19.210257:   [RESULT]   ----------char_per_word: 6.005405405405406----------
2019-12-03 14:59:19.210347:   [RESULT]   ----------Score on FCE Limiting: 6.659115652755228----------
2019-12-03 14:59:19.225636:   [RESULT]   ----------type_token_ratio: 0.3848396501457726----------
2019-12-03 14:59:19.225885:   [RESULT]   ----------lexical_density: 0.3848396501457726----------
2019-12-03 14:59:19.226019:   [RESULT]   ----------char_per_word: 6.318181818181818----------
2019-12-03 14:59:19.226161:   [RESULT]   ----------Score on FCE Limiting: 7.087861118473363----------
2019-12-03 14:59:19.226286:   [DONE  ]   Limiting FCE Done
2019-12-03 14:59:19.226362:   [START ]   Start Reading CAE
2019-12-03 14:59:19.227252:   [START ]   Preprocess ./dataset/cambridge/CAE/1.txt Start
2019-12-03 14:59:19.298472:   [DONE  ]   Preprocess ./dataset/cambridge/CAE/1.txt Done
2019-12-03 14:59:19.299516:   [START ]   Preprocess ./dataset/cambridge/CAE/2.txt Start
2019-12-03 14:59:19.356238:   [DONE  ]   Preprocess ./dataset/cambridge/CAE/2.txt Done
2019-12-03 14:59:19.357929:   [START ]   Preprocess ./dataset/cambridge/CAE/3.txt Start
2019-12-03 14:59:19.475354:   [DONE  ]   Preprocess ./dataset/cambridge/CAE/3.txt Done
2019-12-03 14:59:19.477129:   [START ]   Preprocess ./dataset/cambridge/CAE/4.txt Start
2019-12-03 14:59:19.613178:   [DONE  ]   Preprocess ./dataset/cambridge/CAE/4.txt Done
2019-12-03 14:59:19.614675:   [START ]   Preprocess ./dataset/cambridge/CAE/5.txt Start
2019-12-03 14:59:19.692195:   [DONE  ]   Preprocess ./dataset/cambridge/CAE/5.txt Done
2019-12-03 14:59:19.693810:   [START ]   Preprocess ./dataset/cambridge/CAE/6.txt Start
2019-12-03 14:59:19.789831:   [DONE  ]   Preprocess ./dataset/cambridge/CAE/6.txt Done
2019-12-03 14:59:19.790933:   [START ]   Preprocess ./dataset/cambridge/CAE/7.txt Start
2019-12-03 14:59:19.896450:   [DONE  ]   Preprocess ./dataset/cambridge/CAE/7.txt Done
2019-12-03 14:59:19.897291:   [START ]   Preprocess ./dataset/cambridge/CAE/9.txt Start
2019-12-03 14:59:20.006025:   [DONE  ]   Preprocess ./dataset/cambridge/CAE/9.txt Done
2019-12-03 14:59:20.007128:   [START ]   Preprocess ./dataset/cambridge/CAE/10.txt Start
2019-12-03 14:59:20.153469:   [DONE  ]   Preprocess ./dataset/cambridge/CAE/10.txt Done
2019-12-03 14:59:20.154451:   [START ]   Preprocess ./dataset/cambridge/CAE/11.txt Start
2019-12-03 14:59:20.223293:   [DONE  ]   Preprocess ./dataset/cambridge/CAE/11.txt Done
2019-12-03 14:59:20.223853:   [START ]   Preprocess ./dataset/cambridge/CAE/12.txt Start
2019-12-03 14:59:20.329269:   [DONE  ]   Preprocess ./dataset/cambridge/CAE/12.txt Done
2019-12-03 14:59:20.330247:   [START ]   Preprocess ./dataset/cambridge/CAE/13.txt Start
2019-12-03 14:59:20.429293:   [DONE  ]   Preprocess ./dataset/cambridge/CAE/13.txt Done
2019-12-03 14:59:20.429508:   [DONE  ]   Read CAE Done
2019-12-03 14:59:20.430364:   [START ]   Limiting CAE Start
2019-12-03 14:59:20.431977:   [RESULT]   ----------type_token_ratio: 0.47285464098073554----------
2019-12-03 14:59:20.432210:   [RESULT]   ----------lexical_density: 0.47285464098073554----------
2019-12-03 14:59:20.432451:   [RESULT]   ----------char_per_word: 6.233333333333333----------
2019-12-03 14:59:20.432539:   [RESULT]   ----------Score on CAE Limiting: 7.179042615294805----------
2019-12-03 14:59:20.443846:   [RESULT]   ----------type_token_ratio: 0.40040241448692154----------
2019-12-03 14:59:20.444016:   [RESULT]   ----------lexical_density: 0.40040241448692154----------
2019-12-03 14:59:20.444371:   [RESULT]   ----------char_per_word: 6.582914572864322----------
2019-12-03 14:59:20.444476:   [RESULT]   ----------Score on CAE Limiting: 7.3837194018381656----------
2019-12-03 14:59:20.450639:   [RESULT]   ----------type_token_ratio: 0.3070362473347548----------
2019-12-03 14:59:20.450819:   [RESULT]   ----------lexical_density: 0.3070362473347548----------
2019-12-03 14:59:20.450943:   [RESULT]   ----------char_per_word: 5.90625----------
2019-12-03 14:59:20.451037:   [RESULT]   ----------Score on CAE Limiting: 6.520322494669509----------
2019-12-03 14:59:20.454380:   [RESULT]   ----------type_token_ratio: 0.38316151202749144----------
2019-12-03 14:59:20.454561:   [RESULT]   ----------lexical_density: 0.38316151202749144----------
2019-12-03 14:59:20.454936:   [RESULT]   ----------char_per_word: 6.692825112107624----------
2019-12-03 14:59:20.455116:   [RESULT]   ----------Score on CAE Limiting: 7.459148136162607----------
2019-12-03 14:59:20.455964:   [RESULT]   ----------type_token_ratio: 0.3851963746223565----------
2019-12-03 14:59:20.456085:   [RESULT]   ----------lexical_density: 0.3851963746223565----------
2019-12-03 14:59:20.456238:   [RESULT]   ----------char_per_word: 7.070588235294117----------
2019-12-03 14:59:20.456341:   [RESULT]   ----------Score on CAE Limiting: 7.840980984538831----------
2019-12-03 14:59:20.458612:   [RESULT]   ----------type_token_ratio: 0.3471502590673575----------
2019-12-03 14:59:20.458750:   [RESULT]   ----------lexical_density: 0.3471502590673575----------
2019-12-03 14:59:20.458889:   [RESULT]   ----------char_per_word: 6.634328358208955----------
2019-12-03 14:59:20.458984:   [RESULT]   ----------Score on CAE Limiting: 7.328628876343671----------
2019-12-03 14:59:20.475007:   [RESULT]   ----------type_token_ratio: 0.28782707622298065----------
2019-12-03 14:59:20.475217:   [RESULT]   ----------lexical_density: 0.28782707622298065----------
2019-12-03 14:59:20.475403:   [RESULT]   ----------char_per_word: 6.529644268774703----------
2019-12-03 14:59:20.475584:   [RESULT]   ----------Score on CAE Limiting: 7.105298421220665----------
2019-12-03 14:59:20.495091:   [RESULT]   ----------type_token_ratio: 0.3162743091095189----------
2019-12-03 14:59:20.495314:   [RESULT]   ----------lexical_density: 0.3162743091095189----------
2019-12-03 14:59:20.495462:   [RESULT]   ----------char_per_word: 6.090614886731392----------
2019-12-03 14:59:20.495571:   [RESULT]   ----------Score on CAE Limiting: 6.72316350495043----------
2019-12-03 14:59:20.497378:   [RESULT]   ----------type_token_ratio: 0.36108821104699096----------
2019-12-03 14:59:20.497521:   [RESULT]   ----------lexical_density: 0.36108821104699096----------
2019-12-03 14:59:20.497679:   [RESULT]   ----------char_per_word: 5.970319634703197----------
2019-12-03 14:59:20.497787:   [RESULT]   ----------Score on CAE Limiting: 6.692496056797179----------
2019-12-03 14:59:20.498799:   [RESULT]   ----------type_token_ratio: 0.43853820598006643----------
2019-12-03 14:59:20.498915:   [RESULT]   ----------lexical_density: 0.43853820598006643----------
2019-12-03 14:59:20.499036:   [RESULT]   ----------char_per_word: 6.371212121212121----------
2019-12-03 14:59:20.499127:   [RESULT]   ----------Score on CAE Limiting: 7.2482885331722535----------
2019-12-03 14:59:20.500703:   [RESULT]   ----------type_token_ratio: 0.3727175080558539----------
2019-12-03 14:59:20.500813:   [RESULT]   ----------lexical_density: 0.3727175080558539----------
2019-12-03 14:59:20.500964:   [RESULT]   ----------char_per_word: 6.296829971181556----------
2019-12-03 14:59:20.501061:   [RESULT]   ----------Score on CAE Limiting: 7.042264987293264----------
2019-12-03 14:59:20.502344:   [RESULT]   ----------type_token_ratio: 0.3079625292740047----------
2019-12-03 14:59:20.502451:   [RESULT]   ----------lexical_density: 0.3079625292740047----------
2019-12-03 14:59:20.502620:   [RESULT]   ----------char_per_word: 6.064638783269962----------
2019-12-03 14:59:20.502711:   [RESULT]   ----------Score on CAE Limiting: 6.680563841817972----------
2019-12-03 14:59:20.502857:   [DONE  ]   Limiting CAE Done
2019-12-03 14:59:20.503035:   [START ]   Start Reading CPE
2019-12-03 14:59:20.504100:   [START ]   Preprocess ./dataset/cambridge/CPE/2.txt Start
2019-12-03 14:59:20.605625:   [DONE  ]   Preprocess ./dataset/cambridge/CPE/2.txt Done
2019-12-03 14:59:20.606759:   [START ]   Preprocess ./dataset/cambridge/CPE/3.txt Start
2019-12-03 14:59:20.722555:   [DONE  ]   Preprocess ./dataset/cambridge/CPE/3.txt Done
2019-12-03 14:59:20.724285:   [START ]   Preprocess ./dataset/cambridge/CPE/5.txt Start
2019-12-03 14:59:20.775311:   [DONE  ]   Preprocess ./dataset/cambridge/CPE/5.txt Done
2019-12-03 14:59:20.778491:   [START ]   Preprocess ./dataset/cambridge/CPE/6.txt Start
2019-12-03 14:59:20.889168:   [DONE  ]   Preprocess ./dataset/cambridge/CPE/6.txt Done
2019-12-03 14:59:20.890745:   [START ]   Preprocess ./dataset/cambridge/CPE/7.txt Start
2019-12-03 14:59:21.000473:   [DONE  ]   Preprocess ./dataset/cambridge/CPE/7.txt Done
2019-12-03 14:59:21.002252:   [START ]   Preprocess ./dataset/cambridge/CPE/8.txt Start
2019-12-03 14:59:21.099460:   [DONE  ]   Preprocess ./dataset/cambridge/CPE/8.txt Done
2019-12-03 14:59:21.100474:   [START ]   Preprocess ./dataset/cambridge/CPE/9.txt Start
2019-12-03 14:59:21.155619:   [DONE  ]   Preprocess ./dataset/cambridge/CPE/9.txt Done
2019-12-03 14:59:21.156738:   [START ]   Preprocess ./dataset/cambridge/CPE/10.txt Start
2019-12-03 14:59:21.263743:   [DONE  ]   Preprocess ./dataset/cambridge/CPE/10.txt Done
2019-12-03 14:59:21.264585:   [START ]   Preprocess ./dataset/cambridge/CPE/12.txt Start
2019-12-03 14:59:21.372459:   [DONE  ]   Preprocess ./dataset/cambridge/CPE/12.txt Done
2019-12-03 14:59:21.373197:   [START ]   Preprocess ./dataset/cambridge/CPE/13.txt Start
2019-12-03 14:59:21.424571:   [DONE  ]   Preprocess ./dataset/cambridge/CPE/13.txt Done
2019-12-03 14:59:21.427088:   [START ]   Preprocess ./dataset/cambridge/CPE/14.txt Start
2019-12-03 14:59:21.547639:   [DONE  ]   Preprocess ./dataset/cambridge/CPE/14.txt Done
2019-12-03 14:59:21.548807:   [START ]   Preprocess ./dataset/cambridge/CPE/15.txt Start
2019-12-03 14:59:21.639192:   [DONE  ]   Preprocess ./dataset/cambridge/CPE/15.txt Done
2019-12-03 14:59:21.639463:   [DONE  ]   Read CPE Done
2019-12-03 14:59:21.640321:   [START ]   Limiting CPE Start
2019-12-03 14:59:21.655819:   [RESULT]   ----------type_token_ratio: 0.4033018867924528----------
2019-12-03 14:59:21.656038:   [RESULT]   ----------lexical_density: 0.4033018867924528----------
2019-12-03 14:59:21.656185:   [RESULT]   ----------char_per_word: 6.637426900584796----------
2019-12-03 14:59:21.656285:   [RESULT]   ----------Score on CPE Limiting: 7.444030674169701----------
2019-12-03 14:59:21.661428:   [RESULT]   ----------type_token_ratio: 0.3888888888888889----------
2019-12-03 14:59:21.661703:   [RESULT]   ----------lexical_density: 0.3888888888888889----------
2019-12-03 14:59:21.661946:   [RESULT]   ----------char_per_word: 6.466165413533835----------
2019-12-03 14:59:21.662081:   [RESULT]   ----------Score on CPE Limiting: 7.2439431913116135----------
2019-12-03 14:59:21.662891:   [RESULT]   ----------type_token_ratio: 0.4330357142857143----------
2019-12-03 14:59:21.663000:   [RESULT]   ----------lexical_density: 0.4330357142857143----------
2019-12-03 14:59:21.663230:   [RESULT]   ----------char_per_word: 6.283505154639175----------
2019-12-03 14:59:21.663342:   [RESULT]   ----------Score on CPE Limiting: 7.149576583210604----------
2019-12-03 14:59:21.664830:   [RESULT]   ----------type_token_ratio: 0.36129032258064514----------
2019-12-03 14:59:21.664952:   [RESULT]   ----------lexical_density: 0.36129032258064514----------
2019-12-03 14:59:21.665090:   [RESULT]   ----------char_per_word: 6.854166666666667----------
2019-12-03 14:59:21.665185:   [RESULT]   ----------Score on CPE Limiting: 7.576747311827957----------
2019-12-03 14:59:21.666834:   [RESULT]   ----------type_token_ratio: 0.3824152542372881----------
2019-12-03 14:59:21.666974:   [RESULT]   ----------lexical_density: 0.3824152542372881----------
2019-12-03 14:59:21.667108:   [RESULT]   ----------char_per_word: 5.966759002770083----------
2019-12-03 14:59:21.667206:   [RESULT]   ----------Score on CPE Limiting: 6.7315895112446595----------
2019-12-03 14:59:21.668144:   [RESULT]   ----------type_token_ratio: 0.27471116816431324----------
2019-12-03 14:59:21.668239:   [RESULT]   ----------lexical_density: 0.27471116816431324----------
2019-12-03 14:59:21.668423:   [RESULT]   ----------char_per_word: 6.242990654205608----------
2019-12-03 14:59:21.668510:   [RESULT]   ----------Score on CPE Limiting: 6.792412990534235----------
2019-12-03 14:59:21.669462:   [RESULT]   ----------type_token_ratio: 0.4012875536480687----------
2019-12-03 14:59:21.669558:   [RESULT]   ----------lexical_density: 0.4012875536480687----------
2019-12-03 14:59:21.669694:   [RESULT]   ----------char_per_word: 6.106951871657754----------
2019-12-03 14:59:21.669783:   [RESULT]   ----------Score on CPE Limiting: 6.909526978953892----------
2019-12-03 14:59:21.700077:   [RESULT]   ----------type_token_ratio: 0.43367935409457903----------
2019-12-03 14:59:21.701133:   [RESULT]   ----------lexical_density: 0.43367935409457903----------
2019-12-03 14:59:21.702103:   [RESULT]   ----------char_per_word: 6.473404255319149----------
2019-12-03 14:59:21.702333:   [RESULT]   ----------Score on CPE Limiting: 7.340762963508308----------
2019-12-03 14:59:21.717783:   [RESULT]   ----------type_token_ratio: 0.35527809307604996----------
2019-12-03 14:59:21.717974:   [RESULT]   ----------lexical_density: 0.35527809307604996----------
2019-12-03 14:59:21.718103:   [RESULT]   ----------char_per_word: 7.156549520766773----------
2019-12-03 14:59:21.718295:   [RESULT]   ----------Score on CPE Limiting: 7.867105706918873----------
2019-12-03 14:59:21.719636:   [RESULT]   ----------type_token_ratio: 0.4451901565995526----------
2019-12-03 14:59:21.719786:   [RESULT]   ----------lexical_density: 0.4451901565995526----------
2019-12-03 14:59:21.719977:   [RESULT]   ----------char_per_word: 7.015075376884422----------
2019-12-03 14:59:21.720099:   [RESULT]   ----------Score on CPE Limiting: 7.905455690083526----------
2019-12-03 14:59:21.731703:   [RESULT]   ----------type_token_ratio: 0.35353535353535354----------
2019-12-03 14:59:21.732035:   [RESULT]   ----------lexical_density: 0.35353535353535354----------
2019-12-03 14:59:21.732287:   [RESULT]   ----------char_per_word: 6.095238095238095----------
2019-12-03 14:59:21.732439:   [RESULT]   ----------Score on CPE Limiting: 6.8023088023088025----------
2019-12-03 14:59:21.733449:   [RESULT]   ----------type_token_ratio: 0.33623910336239105----------
2019-12-03 14:59:21.733566:   [RESULT]   ----------lexical_density: 0.33623910336239105----------
2019-12-03 14:59:21.733699:   [RESULT]   ----------char_per_word: 6.555555555555555----------
2019-12-03 14:59:21.733792:   [RESULT]   ----------Score on CPE Limiting: 7.228033762280338----------
2019-12-03 14:59:21.733925:   [DONE  ]   Limiting CPE Done
2019-12-03 14:59:21.734002:   [DONE  ]   Traning data Done
2019-12-03 14:59:21.734156:   [START ]   Test Start
2019-12-03 14:59:21.734250:   [START ]   Reading Test ./dataset/cambridge/KET/9.txt Start
2019-12-03 14:59:21.734930:   [START ]   Preprocess ./dataset/cambridge/KET/9.txt Start
2019-12-03 14:59:21.750329:   [DONE  ]   Preprocess ./dataset/cambridge/KET/9.txt Done
2019-12-03 14:59:21.750464:   [DONE  ]   Reading Test ./dataset/cambridge/KET/9.txt Done
2019-12-03 14:59:21.750597:   [START ]   Scoring KET Start
